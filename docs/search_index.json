[["index.html", "Quantitative Environmental Methods: LRES 546 Chapter 1 Introduction 1.1 Course overview and objectives 1.2 Course Description 1.3 Structure 1.4 Philosophical approach and resources 1.5 Tentative schedule, subject to change", " Quantitative Environmental Methods: LRES 546 Tim Covino &amp; Lauren Kremer Chapter 1 Introduction This book provides the materials that we will use in Quantitative Environmental Methods (LRES 546). In this class we will be learning the fundamentals of environmental data analysis and simulation in R. This class can be taken online or in-person. The online will be asynchronous. As a function of demand, in 2024, this class will be taught online. Instructor: Dr. Tim Covino Class times (depending on demand): T 13:40 – 14:55; Th 13:40 – 14:55 - For 2024 the class will be online, asynchronous. Office hours: By appointment Email: timothy.covino@montana.edu TA: Lauren Kremer Email: lauren.kremer@montana.edu 1.1 Course overview and objectives provide theoretical understanding and practical experience with common analysis and modeling techniques relevant to watershed hydrology/environmental science. provide training in analyzing, simulating, and presenting scientific data in written and oral formats. 1.2 Course Description This course will focus on development of quantitative analysis and modeling skills in watershed and environmental science. Students will develop skills necessary to perform quantitative analyses, describe and evaluate model structure, evaluate the merit of different models of varying type and complexity, and use quantitative analyses to address problems in environmental/watershed science. Students will apply computer programming in R to analyze and simulate watershed and/or environmental dynamics spanning simple to complex processes, analyses, and simulations. Technical skills and conceptual understanding will be built through lectures, readings, and hands-on quantitative projects. Note: If you aren’t familiar with R, or don’t have coding experience, don’t worry. We will walk you through all of the coding. I hope that by the end of this class each student will be a strong quantitative scientist with equally strong coding skills. 1.3 Structure This class will utilize hands-on/active learning, and students will be conducting watershed analyses and modeling exercises. We will be doing our analysis and modeling in R and will do some R coding in each class/work session. Programming is best learned by doing it often. Each week there will be recorded videos and/or readings, where we will talk about and work through various types of hydrological analyses. We will then put the content from the recorded lectures to work in a lab where students will complete a variety of hydrological analyses in R. Students can work through material on their schedule, but Dr. Covino and Lauren Kremer (TA) will be available during lab (Thursday 13:40 - 14:55) to help with technical coding problems or to answer other questions on weekly labs. 1.4 Philosophical approach and resources This course will use all online and open-source resources and will follow FAIR (findability, accessibility, interoperability, and reusability) data principles and promote sharing of hydrological education and research materials. Our computing will utilize open source R and RStudio software. Books/resources, we may not use all of these, but they are good references: - R for Data Science - Statistical Methods in Water Resources - Tidy modeling with R - ggplot2: Elegant Graphics for Data Analysis - Advanced R - R Packages - Environmental Data Science Additional readings will be made available on this bookdown page as needed. 1.5 Tentative schedule, subject to change Week 1: - Introduction, overview, and technical skills. - If you need a refresher for R please see Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). Week 2: - Hydrograph separation. Week 3: Week 4: - Frequency analysis Week 5: - Rational and curve number (CN) methods Week 6: - Transfer function lumped hydrological model Week 7: - Monte Carlo and sensitivity analysis Week 8: - Term project proposal Week 9: Spring Break Week 10: - Snowmelt models Week 11: - Gridded and flow data retrieval Week 12: - DEM processing and analysis in Whitebox Week 13: - Evapotranspiration (ET) models Week 14: - HBV model – lumped to semi-distributed modeling OR Precipitation interpolation Week 15: - Term project work Week 16: - Final presentations "],["hydrograph-separation-learning-module.html", "Chapter 2 Hydrograph separation: learning module 2.1 Summary 2.2 Overall Learning Objectives 2.3 Lecture 2.4 Assessment 2.5 Interactive application and questions", " Chapter 2 Hydrograph separation: learning module This module was developed by Ana Bergrstrom and Matt Ross. 2.1 Summary Streamflow can come from a range of water sources. When it is not raining, streams are fed by the slow drainage of groundwater. When a rainstorm occurs, streamflow increases and water enters the stream more quickly. The rate at which water reaches the stream and the partitioning between groundwater and faster flow pathways is variable across watersheds. It is important to understand how water is partitioned between fast and slow pathways (baseflow and stormflow) and what controls this partitioning in order to better predict susceptibility to flooding and if and how groundwater can sustain streamflow in long periods of drought. In this module we will introduce the components of streamflow during a rain event, and how event water moves through a hillslope to reach a stream. We will discuss methods for partitioning a hydrograph between baseflow (groundwater) and storm flow (event water). Finally, we will explore how characteristics of a watershed might lead to more or less water being partitioned into baseflow vs. stormflow. We will test understanding through evaluating data collected from watersheds in West Virginia to determine how mountaintop mining, which fundamentally changed the watershed structure, affects baseflow. Note: While this assessment is written for watersheds in West Virginia, we have designed the course so it is adaptable. Instructors should be able to easily substitute in their own data for the assessment. 2.2 Overall Learning Objectives At the end of this module, students should be able to describe the components of streamflow, the basics of how water moves through a hillslope, and the watershed characteristics that affect partitioning between baseflow and stormflow. 2.3 Lecture 2.3.1 Components of streamflow during a rain event During a rainstorm, precipitation is falling across the watershed: close to the stream, on a hillslope, and up at the watershed divide. This water that falls across the watershed flows downslope toward the stream via a number of flow pathways. Here we define and describe the basic flow pathways during a rain event. The first component is channel interception. This is water that falls directly on the water surface of the stream. The amount of water that falls directly on the channel is a function of stream size, if we have a very small, narrow creek, this will be a very small quantity. However, you can imagine that in a very large, broad river such as the Amazon, this volume of water is much larger. Channel interception is the first component during a rain event that causes streamflow to increase because it is contributing directly to the stream and therefore has no travel time. The second is overland flow, which is water that flows on the land surface to the stream. Overland flow can occur via a number of mechanisms which we will not explore too deeply here, but encourage further study on your own (resources provided). Briefly, overland flow includes water that falls on an impermeable surface such as pavement, water that runs downslope due to rain falling faster than the rate at which it can infiltrate the ground surface, and water that flows over the land surface because the ground is completely saturated. Overland flow is typically faster than water that travels through soils and deeper flow pathways and therefore is the next major component that starts to contribute to the increase in streamflow during a rain event. The third component is subsurface flow. This is water that infiltrates the land surface and flows downslope through shallow groundwater flow pathways. This is the last component that increases streamflow during a storm event, is the slowest of the stormflow components, and can contribute to elevated streamflow for a while after precipitation ends. The final component is baseflow. Baseflow can also be described as groundwater. This component is what sustains streamflow between rain events, but also continues to contribute during a rain event. Of water that infiltrates the ground surface, some moves quickly to the stream as subsurface flow, but some moves deeper and becomes part of deeper groundwater and baseflow. Thus baseflow can increase in times of higher wetness in the watershed, particularly during and right after rainy seasons or spring snowmelt. We can simplify this partitioning into baseflow and stormflow (often called quickflow). Baseflow being groundwater that moves more slowly and sustains streamflow between rain events. Stormflow is water that contributes to streamflow as a result of a rain event. Under this definition we can lump channel interception, overland flow, and subsurface flow into stormflow. 2.3.2 Storm flow through a hillslope When rain falls on the land surface, much of it infiltrates into the soil. Water moves through the soil column until it meets a layer of lower permeability and runs down the hillslope as subsurface flow. This layer of lower permeability allows some water to move through it, contributing to groundwater. Frequently the layer of lower permeability is the interface between soil and rock. Therefore the depth of soil has a large effect on how much water moves through soil vs. how much moves deeper into groundwater, becoming baseflow. 2.3.3 How we quantify baseflow It is impossible to know the amount of water moving as overland, subsurface, and base flow in all parts of a watershed. So in order to quantify how much water in a stream at any given time is storm flow vs. baseflow, we need to use some simplified methods. These frequently involve using the hydrograph (plot of streamflow over time) drawing lines, and calculating the volume of water above and below the line. This can be somewhat arbitrary and there are a variety of methods for delineating the cutoff between baseflow and stormflow. Despite what method you use and how simplified it is, this technique still provides valuable information and allows us to make comparisons across watersheds in order to understand how they function and what their structural properties are. 2.3.4 Baseflow separation methods One of the most basic methods for calculating base flow vs. storm flow is the straight line method. First, find the value of discharge at the point that streamflow begins to rise due to a storm. A straight line is drawn at that value until it intersects with the hydrograph (i.e. streamflow recedes back to the discharge it was at before the rainfall event started. Anything below this line is base flow and anything above it is storm flow. We learned above that some rainfall can move deep into the soil profile and contribute to baseflow. We might expect baseflow to increase over time and thus would want to use a method that can account for this. An addition to the straight line method was posed by Hewlett and Hibbert, 1967. This method, which we’ll call the Hewlett and Hibbert method finds the discharge at the starting point of a storm. Then, rather than a straight line of 0 slope and in the straight line method, we assume a slope of 0.05 cubic feet per second per square mile. The line with this calculated slope is drawn until it intersects with the hydrograph receding at the end of the storm. There are myriad other methods for baseflow separation of a wide range of complexity. We will give an example of one more method: a recursive filter method established by Lyne and Hollick (1976). This method smooths the hydrograph and partitions part of that smoothed signal into baseflow. The equation for this method is: You can see from this equation that a filter parameter, a, must be chosen. This parameter can be decided by the user, takes a value between 0 and 1, and is typically close to 1. Additionally this filtering method must be constrained so that baseflow is not negative or greater than the total streamflow. Output from this method for the Harvey River in Australia is originally published in Lyne and Hollick (1976) below (notice in the caption that the a parameter was set to 0.8): 2.3.5 Watershed controls on baseflow and stormflow The way a watershed is structured has a strong control on how water is partitioned into baseflow and stormflow. Below is a list of key structural properties: Land Use and Land Cover: If a watershed is developed or natural can dictate how much water infiltrates the land surface and how quickly. For example, a watershed with lots of pavement means that much more water will be overland flow with fewer opportunities to recharge baseflow. Furthermore, how a watershed is developed will affect partitioning. For example a residential area with houses on large, grassy lots will allow for more infiltration than a shopping center with large parking lots. Land cover in natural areas will also affect partitioning. Some other variables to consider may be: Land cover in natural areas: a dense forest vs. a recently harvested hillside. Soil type: clayey soils vs. sandy soils Depth to impeding layer: could be the bedrock interface, but could also be a low permeability clay layer in the soil Permeability of the underlying rock: Highly fractured sandstone vs. solid granite Slope: steeper slopes vs. flatter areas The partitioning is a combination of all of these factors. A watershed may have a very low slope, suggesting that it might have less stormflow. But if the soils in this watershed have an impermeable clay layer near the soil surface, a lot more water may end up as stormflow than one would expect. 2.4 Assessment To assess and improve your understanding we’ve developed an interactive application to explore runoff generation and sources. Check out the application and respond to and submit answers to the synthesis questions in a Word .doc. 2.5 Interactive application and questions This interactive web application highlighting how a major disturbance (mountaintop-mining) can change how water moves through a system. The web app is here: https://cuahsi.shinyapps.io/mtm_baseflow/ 2.5.1 Initial data exploration In the geomorphology tab, click on each of the four watersheds in the map on the left to view a 3-D rendering of its topography and read about its characteristics. Compare and contrast the watershed structure of the paired watersheds (both large and small). Describe the topography and the slope and soil characteristics. (1 pts) Click on the baseflow tab and make sure you have “Flow separation at each site” selected in the choose baseflow data display drop-down menu. Describe which line is baseflow and how you would estimate total storm flow for an event. (1 pt) Zoom into the storms that occurred between 3 Apr and 19 Apr. Compare the overall shape of the hydrographs. Which watersheds have higher peaks? How do the peaks change over successive storms? (2 pts) Move your cursor over the hydrographs and look at the values displayed in the top right corner of each graph. Determine if the reference or mined watersheds have higher baseflow. Describe how the baseflow changes over time in successive storms. (2 pts) 2.5.2 Synthesis Questions What do you think is happening in the hillslopes to cause the baseflow to change over time in successive storms? (3 pts) Describe how differences in watershed structure might contribute to the differences in baseflow between the mined and unmined watersheds. (4 pts) Look at the specific conductance tab and look at the mined vs. unmined. Describe the differences between the two. How do these data support some of your conclusions above? (4 pts) What would you expect a hydrograph for a watershed in Hawaii (Steep slopes, shallow soils) to look like? Would it have relatively high or low baseflow? What about a watershed in Southern Michigan (low slopes, deep soils)? (3 pts) "],["hydrograph-separation-lab-module-20-pts.html", "Chapter 3 Hydrograph separation: lab module (20 pts) 3.1 Intro 3.2 Reading for this lab 3.3 Repo", " Chapter 3 Hydrograph separation: lab module (20 pts) 3.1 Intro In this lab we will analyze stream flow (Q) and precipitation (P) data from Tenderfoot Creek Experimental Forest (TCEF). TCEF is located in central Montana, north of White Sulphur Springs. See here for information about TCEF. You will do some data analysis on flows, calculate annual runoff ratios, and perform a hydrograph separation. 3.2 Reading for this lab Ladson, A. R., R. Brown, B. Neal and R. Nathan (2013) A standard approach to baseflow separation using the Lyne and Hollick filter. Australian Journal of Water Resources 17(1): 173-18 Ladson et al., 2013 Lynne, V., Hollick, M. (1979) Stochastic time-variable rainfall-runoff modelling. In: pp. 89-93 Institute of Engineers Australia National Conference. Perth. Lyne and Hollick, 1979 3.3 Repo Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd” or “_partial.Rmd”. Always be sure to read the README.md files in the GitHub repo. Sometimes they are useful, sometimes they aren’t, but always have a look. As I mentioned above you will work through the “_blank.Rmd” or “_partial.Rmd”. However, there is also a “_complete.Rmd” in the repo. This has all the code. So you can use it as a cheat sheet, but if you want to learn how to code in R, I encourage you to work through the blank version as much as possible. Also, if you don’t have much R background this lab might seem kind of challenging. But don’t worry. I’m challenging you right now, but I’m going to post videos explaining how I would code this and walk you through everything. So don’t get frustrated if this seems tough right now. Soon you will be rattling off code with ease. Conversely, if you are an experienced coder and have ideas for how to do this in ways other than what I’ve shown here, please share code with your colleagues and help them develop their coding skills! OK. Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. The use of projects is highly recommended and is the practice we will follow in this class. See here for an overview of projects and why you should use them from Jenny Bryan. If you are new to R, or need a refresher, please read Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). In this unit we want to start familiarizing ourselves with R by visualizing some hydrological data. "],["learning-module-2---return-intervals-14-pts.html", "Chapter 4 Learning module 2 - Return intervals (14 pts) 4.1 Background information 4.2 Intro 4.3 Packages 4.4 Precipitation return intervals", " Chapter 4 Learning module 2 - Return intervals (14 pts) In this learning module we will focus on return intervals. We first need to understand return intervals before we can move onto the rational method and curve numbers. The repo for this module can be found here 4.1 Background information Lecture from colleague Joel Sholtes on precipitation frequency analysis. Short lecture on Intensity-Duration-Frequency (IDF) curves Reading on frequency analysis of flow (e.g., floods). You should notice that the frequency analysis is the same whether we apply it to Q (flow) or P (precipitation). So as long as you understand the fundamental principles you will be able to do frequency analysis on either Q or P. USGS reading on flow frequency analysis There are also probability lecture slides on D2L. Titled “probability.pptx”. 4.2 Intro In this lab we will look at some precipitation data to get an idea of return intervals for a given rain event. A return interval is the inverse of the probability. So if a certain rain even has a 10% probability of happening any year it has a 1/p return interval, so: R = 1/0.1 = 10 years. This means on average you can expect that size event about every ten years. From a probability perspective it is actually more correct to state that there is a 10% chance of that size rain event in any year. The reason this is better is that it communicates that you certainly can have a 10% probability event occur in back-to-back years. After computing some return intervals we will then use some of the simpler rainfall-runoff modeling approaches (the rational method and the curve number method) to simulate runoff for a hypothetical basin in our next unit. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 4.3 Packages We have a few new packages today. Those include rnoaa and leaflet. rnooa is a package used to download NOAA climate data. leaflet is a package for interactive mapping. Recall that if you have not installed these packages on your computer you will need to run: install.packages(“rnoaa”) and so on for the others. # library(tidyverse) # library(rnoaa) #this package is being deprecated, unfortunately. But should work for now. # library(plotly) # library(leaflet) # library(lubridate) 4.4 Precipitation return intervals First, let’s start by getting some precipitation (P) data. rnoaa has a function called ghcnd_stations(). This function will download the site information for all stations in the GHCND network. GHCND network - Link # # download all available station metadata # # THIS CAN TAKE A WHILE! I&#39;d suggest to run it only once and then comment it out with #. # station_data_download &lt;- ghcnd_stations() # # # filter to only keep MT stations # station_data &lt;- station_data_download %&gt;% # filter(state == &quot;BLANK&quot;) # # # that&#39;s still a lot of stations. Let thin for &quot;BOZEMAN&quot; # # station_data_bzn &lt;- station_data %&gt;% # filter(str_detect(name, &quot;BLANK&quot;)) %&gt;% # # get rid of duplicate ids to make plotting easier # dplyr::distinct(id, .keep_all = TRUE) # # # Now, let&#39;s create a zoomable map with the stations around Gallatin County/Bozeman. Click around and find the station id for the precip gauge with the LONGEST timeseries. If you click on a station symbol, the first and last data year will appear. Obviously you can get this information from the station_data_gal data frame, but I wanted to show you the mapping capabilities. # # gauge_map &lt;- leaflet() %&gt;% # # add a basemap # addTiles() %&gt;% # # add markers for your station. The parameters are pretty self-explanatory # addAwesomeMarkers(data = station_data_bzn, lat = ~latitude, lng = ~longitude, label = ~id, popup = ~paste(&quot;Start date:&quot;, first_year, &quot;&lt;br&gt;End date:&quot;, last_year)) # # gauge_map Ok, so we want station USC00241044, which runs from 1892 to now. We use the meteo_pull_monitors() function to do that. # climate_data &lt;- meteo_pull_monitors(c(&quot;BLANK&quot;), # # precip, snow, min air temp, max air temp (we really won&#39;t use the temp and snow data, though, this is only to show you what else would be available) # var = c(&quot;PRCP&quot;, &quot;SNOW&quot;, &quot;TMIN&quot;, &quot;TMAX&quot;), # date_min = &quot;1893-10-01&quot;, # # set end date to September 30, 2022 # date_max = &quot;2022-09-30&quot;) Now we have some climate data. Take a minute to look at the climate_data df. First, just looking at the df we see that the data don’t actually start until 1894. It is also always a good idea to just plot some data. Below plot prcp, snow, tmax and tmin. You can just make 4 different plots. This is just for visual inspection. This part of the process is called exploratory data analysis (EDA). This should always be the first step when downloading data whether you download the data from the internet or from a sensor in the field. # climate_data %&gt;% # ggplot(aes(x = date, y = prcp)) + # geom_point() You might have noticed that the values seem to be much too large! Did you notice that? This is a great skill to develop. Have a look at the data and ask “are these numbers reasonable?”. In this case, the answer would be no! One thing to note is that NOAA data comes in tenths of degrees for temp and tenths of millimeters for precip. Type ?meteo_pull_monitors into the console and the help screen will tell you that. So, we need to clean up the df a bit. Let’s do that here. # climate_data_corr &lt;- climate_data %&gt;% # mutate( # # change names # name = recode(id, USC00241044 # = &quot;msu_campus&quot;), # # division by 10 turns it into a normal decimal, 15.6 instead of 156 # tmin = tmin / 10, # tmax = tmax / 10, # prcp = prcp / 10, # snow = snow / 10) %&gt;% # # only take important stuff # select(name, id, everything()) Now that we’ve converted units, it is a good idea to plot your data again for some EDA. Make plots of each of the variables (prcp, snow, tmax, and tmin) over time to inspect. # ggplotly( # climate_data_corr %&gt;% # ggplot(aes(x = date, y = prcp)) + # geom_point() + # geom_line(linetype = &quot;dashed&quot;) # ) # I commented this out just because plotly is slow. # climate_data_corr %&gt;% # ggplot(aes(x = date, y = snow)) + # geom_point() How do the data look? Do they make sense? Do you see any strange values? There is a large snow event in 1951. We can assume that is “real”, so let’s keep it in the analysis. But you should think through how you could exclude it from the analysis. How could you use the filter function to do that? Next, we want to use some skills from the hydrograph sep lab to add a water year (WY) column. Try that here. # climate_data_corr &lt;- climate_data_corr %&gt;% # mutate(month = month(date), # year = year(date), # wy = BLANK(month &gt; 9, year + 1, year)) I like to rearrange the order of columns. Using: # climate_data_corr &lt;- climate_data_corr %&gt;% # select(name, id, date, wy, everything()) Now, create a new df called climate_an where you calculate the total P (i.e., the sum) for each water year. Use group_by and summarize (or better yet, reframe). Also keep in mind that you will need to deal with NA values in the df. How do you do that in summarize? As a note, reframe can be used instead of summarize and is a more general purpose function. You can try each. # climate_an &lt;- climate_data_corr %&gt;% # group_by(wy) %&gt;% # BLANK(tot_p = sum(prcp, BLANK), # mean_max = mean(tmax, BLANK), # mean_min = min(tmin, BLANK)) # I also calculate some temp stats here. Just out of curiosity. We don&#39;t use them in this lab. What happens if you don’t deal with NA values by using something like na.rm = TRUE? Now, plot total anual P on the Y and water year on the x. What do you see? # climate_an %&gt;% # ggplot(aes(x = wy, y = tot_p)) + # geom_point() + # geom_line(linetype = &quot;dashed&quot;, color = &quot;blue&quot;) Now let’s calculate some probabilities. Look up the pnorm() function for this (either type it into the Help window, or type ?pnorm in the console. You only need x, the mean, and standard deviation (sd) for the calculations. 4.4.1 Q1. (2 pts) What is the probability that the annual precipitation in a given year is less than 400 mm? This is the F(A) in the CDF in the probability lecture slides. # p_400 &lt;- pnorm(400, mean(climate_an$tot_p), sd(climate_an$tot_p)) # p_400 Q1 ANSWER: 4.4.2 Q2. (2 pts) What is the probability that the maximum annual precipitation in a given year is GREATER than 500 mm? # p_500 &lt;- pnorm(500, mean(climate_an$tot_p), sd(climate_an$tot_p)) # ex_500 &lt;- 1 - p_500 # ex_500 Q2 ANSWER: 4.4.3 Q3. (2 pts) What is the probability that the annual P is between 400 and 500 mm? # p_500_400 &lt;- p_500 - p_400 # # p_500_400 4.4.4 Q4. (2 pts) What is the return period for a year with AT LEAST 550 mm of precip? The return period, Tr, is calculated as Tr = 1/p, with p being the probability for an event to occur. # p_550 &lt;- pnorm(550, mean(climate_an$tot_p), sd(climate_an$tot_p)) # ex_550 &lt;- 1 - p_550 # # ri &lt;- 1/ex_550 # ri 4.4.5 Q5. (6 pts) Explain why probability analysis of climate data assumes the data are normally distributed and stationary? Below provide a histogram and a density plot of the total annual P data and comment on the visual appearance in terms of normality. Next use google and the links below to test for normality and stationarity. Be quantitative in commenting on the normality and stationarity of the total P data. here here here #shapiro.test(climate_an$tot_p) #lag.length = 25 #Box.test(climate_an$tot_p, lag = lag.length, type = &quot;Ljung-Box&quot;) # test stationary signal # climate_an %&gt;% # ggplot(aes(x = tot_p)) + # theme_bw() + # geom_histogram(binwidth = 20, aes(y = after_stat(density)), colour = &quot;black&quot;, fill = &quot;gray&quot;) + # histogram # geom_density(alpha = 0.2, fill = &quot;red&quot;) # density plot "],["rational-method-and-nrcs-curve-number-20-pts.html", "Chapter 5 Rational method and NRCS curve number (20 pts) 5.1 Rational Method 5.2 Curve Number 5.3 Codework", " Chapter 5 Rational method and NRCS curve number (20 pts) The repo for this module can be found here 5.1 Rational Method 5.1.1 Background information The Rational Method is a type of simple hydrological analysis used to estimate the peak runoff rate from a small watershed during a rainfall event. It is particularly useful for estimating the amount of water that will flow through a particular area during a storm, like a drainage system or culvert. Some helpful terminology: runoff coefficient - represents how much rainfall actually becomes runoff time of concentration - the time it takes for some mass of precipitation to travel from the most remote point in a watershed to the outlet or point of interest. e.g., how long it takes a drop of rain to reach a culvert after it falls to the ground. Here is a 5-minute video that describes the equation: 5.1.1.1 Reading - Rational Method. Read at least sections 2 and 3 Link 5.2 Curve Number 5.2.1 Background information The NRCS (Natural Resources Conservation Science) curve number (CN) is a tool used to estimate the total runoff volume of water that will run off an ungaged watershed during a storm event. The curve number is based on soil type, land use and antecedent moisture conditions. You may also see SCS CN in texts. NRCS was previously known as Soil Conservation Service, they are the same. It was designed as a simple tool to describe typical watershed response from infrequent rainfall anywhere in the US for watersheds with the same soil type, land use, and surface runoff conditions. The CN method is a single event model to estimate of runoff volume from rainfall events (not peak discharge or a hydrograph). To understand the function and derivation of the CN number, let’s start with the NRCS runoff equation: \\[ Q = \\frac{{(P - I_{a})^2}}{{P - I_{a} + S}} \\] Where Q = runoff(in) P = rainfall (in) S = potential maximum retention after runoff begins Ia = initial abstraction (initial amount of rainfall that is intercepted by the watershed surface and does not immediately contribute to runoff) Ia is assumed to reduce to 0.2S based on empirical observations by NRCS. If: \\[ S = \\frac{{1000}}{{CN}} - 10 \\] the runoff equation therefore reduces to: \\[ Q = \\frac{{[P - 0.02\\left(\\frac{{1000}}{{CN}} - 10\\right)]^2}}{{P + 0.8\\left(\\frac{{1000}}{{CN}} - 10\\right)}} \\] 5.2.1.1 Reading - Curve numbers Curve Number selection tables are available from the USDA. Slides on selecting curve number start around slide 8. - Link 5.2.1.2 Reading - Supporting material Time of concentration. Up to “other considerations”, pages 15-1 to 15-9. - Link We will use the Kirpich method to calculate the time of concentration. Here is the citation for your reference. Kirpich, Z.P. (1940). “Time of concentration of small agricultural watersheds”. Civil Engineering. 10 (6): 362. 5.3 Codework In this module, you will apply the Rational Method and the SCS curve number (CN) method to estimate peak flows and effective rainfall/runoff volumes. This lab also includes two new coding techniques, namely writing ‘for loops’ and writing functions. We will start with the Rational Method and functions. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 5.3.1 Packages 5.3.2 Part I - Rational Method The goal is to calculate peak runoff (cfs) for a small 280-acre rangeland watershed near Bozeman for multiple events with different return periods. You will first have to calculate the time of concentration and then look up the the rainfall values for the different return intervals. The longest flowpath in the watershed is 6300 ft long, average watershed slope is 1.95%. Look at this table to select C. 5.3.2.1 Time of Concentration Time of concentration is the time it takes water to travel along the longest flowpath in the watershed and exit the watershed. # A &lt;- BLANK # Area in acres # L &lt;- BLANK # Longest flowpatch length (ft) # S &lt;- 0.0195 # Slope (ft/ft) # # tc &lt;- 0.0078 * L^0.77 * S^-0.385 # Kirpich concentration time # tc 5.3.2.2 Storm Depths Now that you have the time of concentration, you need to find the corresponding 1, 2, 5, 10, 25, 50, and 100 year storm depths for a duration that works for the Rational Method in that particular watershed (that is, the duration that is closest to tc). Create a df called “storms” that has a column for the return period, Tr, the storm depth, Pin, and the average storm intensity over ONE HOUR, Pin_hr. Typically hourly depths may be determined from a rainfall analysis. For the sake of the assignment, approximate daily depths corresponding to appropriate frequencies are provided here for Bozeman, MT. To obtain 1 hour depths by dividing daily depth by 24. # storms &lt;- tibble( # Tr = c(BLANK), # column for return intervals # Pday = c(1.0, 1.18, 1.75, 2.10, 2.50, 2.81), # column for storm depth # Pin_hr = Pday / 24) # storm depth converted to an intensity of inches per hour 5.3.2.3 Example for-loop Now that we have the rainfall intensities, we need to set up a way that calculates Qp for each of those intensities, without us having to go back and manually enter them each time. We do this with a for-loop. Let’s first look at how for loops work in R. There is one thing we need to take care of first, though. We have to preallocate the vector y, that is, we are creating an empty vector of the desired length. # x &lt;- seq(0, 10, 2) # create vector from 0 to 10 in increments of 2 # y &lt;- vector(mode = &quot;double&quot;, length(x)) # preallocate y vector with length of x # for (i in 1:length(x)) { # loop through i # y[i] &lt;- x[i]^2 # calculation # } # y Please note that we wouldn’t need a for-loop for this operation, this is just for demonstration. We have created a vector x from 0 to 10 in increments of 2. The for-loop takes each element of that vector and squares it. The “i” is called an index and runs from 1 through 6 (the length of x). During the first iteration i is 1, during the second iteration i is 2, and so on. We are then writing the results from the calculation into a new vector, y. When i is 1, we are squaring the first value in vector x, which is 0. The first value in y will be zero as well. When i is 2, the second value in x gets squared, which is 2^2. The second value in y is going to be 4. 5.3.2.4 Calculate Qp with for-loop Let’s set up the for-loop for the Rational Method. We need to set up a for-loop that does the same calculation 7 times, the number of precip values in “storms” (remember that ‘length()’ for a dataframe returns the number of columns, so you want to use ‘nrow()’. The calculated peak discharges should go into a new column of our “storms” df. However, indexing is slow for dataframes, that is why we will write the new values into a new vector, Qp, and then after the loop insert it into the dataframe. If you are using the complete version of the assignment, do not assume that this C value is correct # # DEFINE C if you haven&#39;t already done so further up (uncomment the line below) # C &lt;- BLANK # runoff coefficient # # # preallocate vector (think about what the length needs to be and how to get it without just typing in the number) # Qp &lt;- vector(mode = &quot;double&quot;, nrow(storms)) # # for (i in 1:nrow(storms)) { # # the actual Rational Method calculation goes here. For the precip, you need to use the column in the storms df that has the one hour precip intensities (make sure to not forget the correct index symbol!) # Qp[i] &lt;- C * A * storms$Pin_hr[i] # } # # # # this adds the new peakflows to the existing &quot;storms&quot; df # storms &lt;- storms %&gt;% # mutate(Qp = Qp) # # # # FULL DISCLAIMER # # We could have accomplished the same with vectorization # # As a matter of fact, you should try to avoid for loops whenever possible. # Qp_vectorized &lt;- C*A*storms$Pin_hr 5.3.2.5 Plot Tr and Qp Now plot the return interval against the storm peakflow. You only need three to four lines for this: 1st sets up the data, 2nd defines the theme that removes the gray background and sets the axes labels to a proper size, 3rd plots the data with geom_points, 4th makes the axes labels with labs. # ggplot(storms, aes(x = Tr, y = Qp)) + # theme_bw(base_size = BLANK) + # geom_point(size = 4, color = BLANK) + # labs(x = &quot;Return Period (Years)&quot;, y = &quot;Peakflow (cfs)&quot;) 1. (2 pt) What does the C in the Rational Method do? ANSWER: 2. (4 pt) What is the the time of concentration and why does it need to be taken into account for the Rational Method? What is a common issue among many tc methods? ANSWER: 5.3.3 Part II - NRCS CN In this exercise we will write a function that takes the necessary inputs for the NRCS CN method and returns a value based on the parameters. This is not fully automated, you will still have to look up the CN yourself for a given land use. The goal is to write a function that requires P, CN, and AMC (antecedent moisture condition) as an input in order to calculate Q. 5.3.3.1 AMC Table 5.3.3.2 Function example Let’s look at a simple example for a function. Assume we want a number with an exponent and we want to be able to choose both the base and the exponent. The function ‘function()’ defines the inputs in (), the actual calculation then follows in {}. #multiply &lt;- function(base, exponent) { # new_number &lt;- base^exponent # return(new_number) #} Run the above function. You will notice that a function was added to the Global Environment all the way at the bottom. Now let’s test the function with a base of 2 and an exponent of 3. This should perform the calculation 2x2x2 = 8 #multiply(2, 3) 5.3.3.3 NRCS CN test Before we write a function, let’s make sure we can set up the correct steps WITHOUT the function. Let’s try it for P = 2.5 inches, CN = 90, and AMC = 3. The biggest problem here is going to be creating a lookup table for the AMC. I will provide the basic structure of the code for this. # P &lt;- 2.5 # 50-year, 24-hour rainfall (in.), from TP40 # CN &lt;- 90 # AMC &lt;- 3 # # ### Adjust for AMC # # use the equations from the lecture to adjust the CN to dry or wet conditions if necessary # # AMC I # if (AMC == 1) { # CN_adj &lt;- (4.2 * CN) / (10 + 0.13 * CN) # } # # # AMC II The curve number remains whatever the input is. # if (AMC == 2) { # CN_adj &lt;- CN # } # # # AMC III # if (AMC == 3) { # CN_adj &lt;- (23 * CN) / (10 + 0.13 * CN) # } # # # # calculate Si # Si &lt;- 1000 / CN_adj - 10 # Storage part # # # estimate initial abstraction # Ia &lt;- 0.2 * Si # # # Check equation from lecture # # calculate Q # Q &lt;- (P - 0.2 * Si)^2 / (P + BLANK * Si) # SCS CN runoff (inches) # Q 3. (4 pt) What is one of the critical assumptions for the SCS CN method? ANSWER: Now write a function called “scs_cn” that takes the inputs P, CN, and AMC (all numeric) and returns Q, Ia, Si, and the RR as a df called “scs_out”. Test the function for P = 2.5 in, CN = 90, AMC = 3. # scs_cn &lt;- function(P, CN, AMC) { # # ### Adjust for AMC # # AMC I # if (AMC == 1) { # CN_adj &lt;- BLANK # } # # # AMC II # if (AMC == 2) { # CN_adj &lt;- CN # } # # # AMC III # if (AMC == 3) { # CN_adj &lt;- (23 * CN) / (10 + 0.13 * CN) # } # # # Check equation from lecture # # calculate S # S &lt;- BLANK / CN_adj - 10 # Storage part # # # estimate initial abstraction # Ia &lt;- 0.2 * S # # Q &lt;- (P - 0.2 * S)^2 / (P + 0.8 * S) # SCS CN runoff (inches) # # # statement that spits out a warning if the initial abstraction is greater than the precipitation amount # if (Ia &gt; P) { # warning(&quot;Ia is greater than P. Resulting Q is 0.&quot;) # } # # scs_out &lt;- tibble(Si, Ia, Q, P, RR = Q / P) # return(scs_out) # } # # # # TEST THE FUNCTION with P = 2.5, CN = 90, and AMC = 3 # SCS_example &lt;- scs_cn(0.01, 90, 3) # SCS_example 4.(6 pts) Describe what factors into the curve number (and most notably what is missing). What does it mean when the CN is 100 or 0? ANSWER: General question: 5. (4 pts) What is the difference between a deterministic and a stochastic model? Describe one example each. ANSWER: "],["transfer-function-rainfall-runoff-models.html", "Chapter 6 Transfer function rainfall-runoff models 6.1 Summary 6.2 Overall Learning Objectives 6.3 Terminology 6.4 The Linear Time-Invariant TF 6.5 Codework - Transfer function rainfall-runoff model", " Chapter 6 Transfer function rainfall-runoff models 6.1 Summary In previous modules, we explored how watershed characteristics influence the flow of input water through or over hillslopes to quickly contribute to stormflow or to be stored for later contribution to baseflow. Therefore, the partitioning of flow into baseflow or stormflow can be determined by the time it spends in the watershed. Furthermore, the residence time of water in various pathways may affect weathering and solute transport within watersheds. To improve our understanding of water movement within a watershed, it can be crucial to incorporate water transit time into hydrological models. This consideration allows for a more realistic representation of how water moves through various storage compartments, such as soil, groundwater, and surface water, accounting for the time it takes for water to traverse these pathways. In this module, we will model the temporal aspects of runoff response to input using a transfer function. First, please read: TRANSEP - a combined tracer and runoff transfer function hydrograph separation model Then this chapter will step through key concepts in the paper to facilitate hands-on exploration of the rainfall-runoff portion of the TRANSEP model in the assessment. Then, we will introduce examples of other transfer functions to demonstrate alternative ways of representing time-induced patterns in hydrological modeling, prompting you to consider response patterns in your study systems. 6.2 Overall Learning Objectives At the end of this module, students should be able to describe several ways to model and identify transit time within hydrological models. They should have a general understanding of how water transit time may influence the timing and composition of runoff. 6.3 Terminology In modeling a flow system, note that consideration of time may vary depending on the questions being asked. Transit time is the average time required for water to travel through the entire flow system, from input (e.g., rainfall on soil surface) to output (e.g., discharge). Residence time is a portion of transit time, describing the amount of time water spends within a specific component of the flow system, like storage (e.g., in soil, groundwater, or a lake). Figure 6.3. Conceptual diagram of the lumped parameter transit time modeling approach (McGuire &amp; McDonnell, 2006) A transfer function (TF) is a mathematical representation of how a system responds to input signals. In a hydrological context, it describes the transformation of inputs (e.g. precipitation) to outputs (e.g. runoff). These models can be valuable tools for understanding the time-varying dynamics of a hydrological system. 6.4 The Linear Time-Invariant TF We’ll begin the discussion in the context of a linear reservoir. Linear reservoirs are simple models designed to simulate the storage and discharge of water in a catchment. These models assume that the catchment can be represented as single storage compartments or as a series of interconnected storage compartments and that the change the amount of water stored in the reservoir (or reservoirs) is directly proportional to the inflows and outflows. In other words, the linear relationship between inflows and outflows means that the rate of water release is proportional to the amount of water stored in the reservoir. 6.4.0.1 The Instantaneous Unit Hydrograph: The Instantaneous Unit Hydrograph (IUH) represents the linear rainfall-runoff model used in the TRANSEP model. It is an approach to hydrograph separation that is useful for analyzing the temporal distribution of runoff in response to a ‘unit’ pulse of rainfall (e.g. uniform one-inch depth over a unit area represented by a unit hydrograph). In other words, it is a hydrograph that results from one unit (e.g. 1 mm) of effective rainfall uniformly distributed over the watershed and occurring in a short duration. Therefore, the following assumptions are made when the IUH is used as a transfer function: 1. the IUH reflects the ensemble of watershed characteristics 2. the shape characteristics of the unit hydrograph are independent of time 3. the output response is linearly proportional to the input Figure 6.4.1.a Conceptual diagram of the linear unit hydrograph Peak discharge of the unit hydrograph, \\(u_p\\); Base time \\(t_b\\)is the total duration of the unit hydrograph; Increase time or time to peak \\(t_p\\) is the time between the start point of the hydrograph and the peak; Concentration time \\(t_c\\) is the time between the end of rainfall and the end of the hydrograph; Lag time \\(t_lag\\) is the time between half rainfall depth and the peak of the hydrograph. IUH as a transfer function allows the calculation of the direct runoff hydrograph for any given rainfall input. IUH is particularly valuable for understanding how the direct runoff is distributed over time in response to a rainfall event. It helps quantify the time distribution of runoff in relation to the rainfall input. Figure 6.4.1.b The discharge of the unit hydrograph(u) from a catchment at time (t) is expressed as \\(h \\cdot u(t)\\) where h is the amount of effective rainfall. In the TRANSEP model, this transfer function is represented as \\(g(\\tau)\\) and thus the rainfall-induced response to runoff. \\[ g(\\tau) = \\frac{\\tau^{\\alpha-1}}{\\mathrm{B}^{\\alpha}\\Gamma(\\alpha)}exp(-\\frac{\\tau}{\\alpha}) \\] The linear portion of the TRANSEP model describes a convolution of the effective precipitation and a runoff transfer function. \\[ Q(t)= \\int_{0}^{t} g(\\tau)p_{\\text{eff}}(t-\\tau)d\\tau \\] Whoa, wait…what? Tau, integrals, and convolution? Don’t worry about the details of the equations. Check out this video to have convolution described using dollars and cookies, then imagine each dollar as a rainfall unit and each cookie as a runoff unit. Review the equations again after the video. knitr::include_url(&quot;https://www.youtube.com/embed/aEGboJxmq-w&quot;) 6.4.0.2 The Loss Function: The loss function represents the linear rainfall-runoff model used in the TRANSEP model. \\[ s(t) = b_{1} p(t + 1 - b_{2}^{-1}) s(t - \\triangle t) \\] \\[ s(t = 0) = b_{3} \\] \\[ p_{\\text{eff}}(t) = p(t) s(t) \\] where \\(p_{\\text{eff}}(t)\\) is the effective precipitation. \\(s(t)\\) is the antecedent precipitation index which is determined by giving more importance to recent precipitation and gradually reducing that importance as we go back in time. The rate at which this importance decreases is controlled by the parameter \\(b_{2}\\). The parameter \\(b_{3}\\) sets the initial antecedent precipitation index at the beginning of the simulated time series. In other words, these equations are used to simulate the flow of water in a hydrological system over time. The first equation represents the change in stored water at each time step, taking into account precipitation, loss to runoff, and the system’s past state. The second equation sets the initial condition for the storage at the beginning of the simulation. The third equation calculates the effective precipitation, considering both precipitation and the current storage state. 6.4.0.3 How do we code this? We will use a skeletal version of TRANSEP, focusing only on the rainfall-runoff piece which includes the loss-function and the gamma transfer function. We will use rainfall and runoff data from TCEF to model annual streamflow at a daily time step. Then we can use this model as a jump-off point to start talking about model calibration and validation in future modules. 6.4.0.4 Final thoughts: If during your modeling experience, you find yourself wading through a bog of complex physics and multiple layers of transfer functions to account for every drop of input into a system, it is time to revisit your objectives. Remember that a model is always ‘wrong’. Like a map, it provides a simplified representation of reality. It may not be entirely accurate, but it serves a valuable purpose. Models help us understand complex systems, make predictions, and gain insights even if they are not an exact replica of the real world. Check out this paper for more: https://agupubs.onlinelibrary.wiley.com/doi/10.1029/93WR00877 6.5 Codework - Transfer function rainfall-runoff model 6.5.1 Download the repo for this lab HERE In this homework/lab, you will write a simple, lumped rainfall-runoff model. The foundation for this model is the TRANSEP model from Weiler et al., 2003. Since TRANSEP (tracer transfer function hydrograph separation model) contains a tracer module that we don’t need, we will only use the loss function (Jakeman and Hornberger) and the gamma transfer function for the water routing. The data for the model is from the Tenderfoot Creek Experimental Forest in central Montana. Load packages with a function that checks and installs packages if needed: #knitr::opts_chunk$set(echo = FALSE) # # # Write your package testing function # pkgTest &lt;- function(x) # { # if (x %in% rownames(installed.packages()) == FALSE) { # install.packages(x, dependencies= TRUE) # } # library(x, character.only = TRUE) # } # # # Make a vector of the packages you need # neededPackages &lt;- c(&#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;tictoc&#39;, &#39;patchwork&#39;) #tools for plot titles # # # For every package in the vector, apply your pkgTest function # for (package in neededPackages){pkgTest(package)} # # # tictoc times the execution of different modeling methods # # patchwork is used for side-by-side plots # # Let&#39;s assume that you know you put your data file in the working directory, but cannot recall its name. Let&#39;s do some working directory exploration with script: # # # Check your working directory: # print(getwd()) # # # Check the datafile name or path by listing files in the working directory. # filepaths &lt;-list.files() # # # Here is an option to list only .csv files in your working directory: # csv_files &lt;- filepaths[grepl(&quot;.csv$&quot;, filepaths, ignore.case = TRUE)] # print(csv_files) # ``` # # #### Read in the data, convert the column that has the date to a (lubridate) date, and add a column that contains the water year # # Identify the path to the desired data. # filepath &lt;- &quot;P_Q_1996_2011.csv&quot; # indata &lt;- read.csv(filepath) # # indata &lt;- indata %&gt;% # mutate(Date = mdy(Date)) %&gt;% # convert &quot;Date&quot; to a date object with mdy() # mutate(wtr_yr = if_else(month(Date) &gt; 9, year(Date) + 1, year(Date))) Define input year We could use every year in the time series, but for starters, we’ll use 2006. Use filter() to extract the 2006 water year. # PQ &lt;- indata %&gt;% # filter(wtr_yr == 2006) # extract the 2006 water year with filter() # # # plot discharge for 2006 # ggplot(PQ, aes(x = Date, y = Discharge_mm)) + # geom_line() # # # make flowtime correction - flowtime is a time-weighted cumulative flow, which aids in understanding the temporal distribution of flow, giving more weight to periods of higher discharge. flow time correction is relevant in hydrology when analyzing Q or time series data, so we can compare hydrological events on a standardized time scale. It can help to identify patterns, assess the duration of high or low flows, and compare behavior of watersheds over time. # # PQ &lt;- PQ %&gt;% # mutate(flowtime = cumsum(Discharge_mm)/mean(Discharge_mm)) %&gt;% # mutate(counter = 1:nrow(PQ)) # # ggplot() + # geom_line(data=PQ, aes(x = flowtime, y = Discharge_mm)) + # geom_line(data=PQ, aes(x = counter, y = Discharge_mm), color=&quot;red&quot;) **1) QUESTION: What does the function cumsum() do?(1 pt) ANSWER: Define the initial inputs This chunk defines the initial inputs for measured precip and measured runoff # tau &lt;- 1:nrow(PQ) # simple timestep counter the same length as PQ # Pobs &lt;- PQ$RainMelt_mm # observed precipitation from PQ df # Qobs &lt;- PQ$Discharge_mm # observed streamflow from PQ df Parameterization We will use these parameters. You can change them if you want, but I’d suggest leaving them like this at least until you get the model to work. Even tiny changes can have a huge effect on the simulated runoff. # # Loss function parameters # b1 &lt;- 0.0018 # volume control parameter (b1 in eq 4a) # b2 &lt;- 50 # backwards weighting parameter (b2 in eq 4a) # # determines how much weight or importance is given to past precipitation events when calculating an antecedent precipitation index. &quot;Exponential weighting backward in time&quot; means that the influence of past precipitation events diminishes as you move further back in time, and this diminishing effect follows an exponential pattern. # # b3 &lt;- 0.135 # initial s(t) value for s(t=0) (b3 in eq 4b) - Initial antecedent precipitation index value. # # # Transfer function parameters # a &lt;- 1.84 # TF shape parameter # b &lt;- 3.29 # TF scale parameter Loss function This is the module for the Jakeman and Hornberger loss function where we turn our measured input precip into effective precipitation (p_eff). This part contains three steps. 1) preallocate a vector p_eff: Initiate an empty vector for effective precipitation that we will fill in with a loop using Peff(t) = p(t)s(t). Effective precipitation is the portion of precipitation that generates streamflow and event water contribution to the stream. It is separated to produce event water and displace pre-event water into the stream. 2) set the initial value for s: s(t) is an antecedent precipitation index. How much does antecedent precipitation affect effective precipitation? 3) generate p_eff inside of a for-loop Please note: The Weiler et al. (2003) paper states that one of the loss function parameters (vol_c) can be determined from the measured input. That is actually not the case. s(t) is the antecedent precipitation index that is calculated by exponentially weighting the precipitation backward in time according to the parameter b2 is a ‘dial’ that places weight on past precipitation events. # # preallocate the p_eff vector # p_eff &lt;- vector(mode = &quot;double&quot;, length(tau)) # # s &lt;- b3 # at this point, s is equal to b3, the start value of s # # # loop with loss function # for (i in 1:length(p_eff)) { # s &lt;- b1 * Pobs[i] + (1 - 1/b2) * s # this is eq 4a from Weiler et al. (2003) # p_eff[i] &lt;- Pobs[i] * s # this is eq 4c from Weiler et al. (2003) # } # # # # #### alternative way to calculate p_eff by populating an s vector # # preallocate s_alt vector # # s_alt &lt;- vector(mode = &quot;double&quot;, length(tau)) # # s_alt[1] &lt;- b3 # # # preallocate p_eff vector # # p_eff_new &lt;- vector(mode = &quot;double&quot;, length(tau)) # # # start loop # # for (i in 2:(length(p_eff))) { # # s_alt[i] &lt;- b1 * Pobs[i] + (1 - 1/b2) * s_alt[i-1] # # p_eff_new[i] &lt;- Pobs[i] * s_alt[i] # # } # #### # # set a starting value for s # # # ## plot observed and effective precipitation against the date # # wide to long with pivot_longer() # precip &lt;- tibble(date = PQ$Date, obs = Pobs, sim = p_eff) %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -date) # # # # a good way to plot P time series is with a step function. ggplot2 has geom_step() for this. # ggplot(precip, aes(x = date, y = value, color = key)) + # geom_step() + # labs(color = &quot;P&quot;) # # # ## plot the ratio of p_eff/Pobs (call that ratio &quot;frac&quot;) # precip_new &lt;- tibble(date = PQ$Date, obs = Pobs, sim = p_eff, frac = p_eff/Pobs) # # ggplot(data = precip_new, aes(x = date, y = frac)) + # geom_line() **2) QUESTION (3 pts): Interpret the two figures. What is the meaning of “frac”? For this answer, think about what the effective precipitation represents. When is frac “high”, when is “frac” low? ANSWER: Extra Credit (2 pt): Combine the two plots into one, with a meaningfully scaled secondary y-axis** Runoff transfer function Routing module - This short chunk sets up the TF used for the water routing. This part contains only two steps: 1) the calculation of the actual TF. 2) normalization of the TF so that the sum of the TF equals 1. tau(0) is the mean residence time # #gTF &lt;- tau # # # this is the &quot;raw&quot; transfer function. This is eq 13 from Weiler 2003. Use the time step for tau. The Gamma function in R is gamma(). NOTE THAT THIS EQ IN WEILER ET AL IS WRONG! It&#39;s exp(-tau / b) and not divided by alpha. # g &lt;- (tau^(a - 1)) / ((b^a * gamma(a))) * exp(-tau / b) # # # normalize the TF, g, by the total sum of g. # gTF &lt;- g / sum(g) # # # plot the TF as a line against tau. You need to put the two vectors tau and gTF into a new df/tibble for this. # tf_plot &lt;- tibble(tau, gTF) # ggplot(tf_plot, aes(tau, gTF)) + # geom_line() 3) QUESTION (2 pt): Why is it important to normalize the transfer function? ANSWER: 4) QUESTION (4 pt): Describe the transfer function. What are the units and what does the shape of the transfer function mean for runoff generation? ANSWER: Convolution This is the heart of the model. Here, we convolute the input with the TF to generate runoff. There is another thing you need to pay attention to: We are only interested in one year (365 days), but since our TF itself is 365 timesteps long, small parts of all inputs except for the first one, will be turned into streamflow AFTER the water year of interest, that is, in the following year. In practice, this means you have two options to handle this. 1) You calculate q_all and then manually cut the matrix/vector to the correct length at the end, or 2) You only populate a vector at each time step and put the generated runoff per iteration in the correct locations within the vector. For this to work, you would need to trim the length of the generated runoff by one during each iteration. This approach is more difficult to code, but saves a lot of memory since you are only calculating/storing one vector of length 365 (or 366 during a leapyear). We will go with option 1). The code for option 2 is shown at the end for reference. Convolution summarized (recall dollars and cookies): Each loop iteration results in a row of the matrix representing the convolution at a specific time step. each time step of p_eff is an iteration, and for each timestep, it multiplies the effective precipitation at that timestep by the entire transfer function. Then each row is summed and stored in the vector q_all. q_all_loop is an intermediate step in the convolution process and can help visualize how the convolution evolves over time. As an example, if we have precip at time step 1, we are interested in how this contributes to runoff at time steps 1,2,3 etc, all the way to 365 (or the end of our period). so the first row of the matrix q_all_loop represents the contribution of precipitation at timestep 1 to runoff at each timestep. the second row represents the contribution of precipitation at time step 2 to runoff at each timestep. Then when we sum up the rows, we get q_all, where each element represents the total runoff at a specific time step. # tic() # # preallocate qsim matrix with the correct dimensions. Remember that p_eff and gTF are the same length. # q_all_loop &lt;- matrix(0, length(p_eff) * 2, length(p_eff)) # set number of rows and columns # # # convolution for-loop # for (i in 1:length(p_eff)) { # loop through length of precipitation timeseries # q_all_loop[(i):(length(p_eff) + i - 1), i] &lt;- p_eff[i] * gTF # populate the q_all matrix (this is the same code as the UH convolution problem with one tiny change because you need to reference gTF and not UH) # } # # # # add up the rows of the matrix to generate the final runoff and replace matrix with final Q # q_all &lt;- apply(q_all_loop, 1, sum, na.rm = TRUE) # # # cut the vector to the appropriate length of one year (otherwise it won&#39;t fit into the original df with the observed data) # q_all &lt;- q_all[1:length(p_eff)] # # # Write the final runoff vector into the PQ df # PQ$Qsim &lt;- q_all # toc() 5) QUESTION (5 pts): We set the TF length to 365. What is the physical meaning of this (i.e., how well does this represent a real system and why)? Could the transfer function be shorter or longer than that? ANSWER: # ## THIS PART SAVES ALL HOURLY Q RESPONSES IN A NEW DF AND PLOTS THEM # Qall &lt;- as_tibble(q_all_loop, .name_repair = NULL) # Qall &lt;- Qall[ -as.numeric(which(apply(Qall, 2, var) == 0))] # toc() # # # Qall[Qall == 0] &lt;- NA # # Qall &lt;- Qall %&gt;% # mutate(Time_hrs = 1:nrow(Qall)) %&gt;% # add the time vector # gather(key, value, -Time_hrs) # # Qall$key &lt;- as.factor(Qall$key) # # # ggplot(Qall, aes(x = Time_hrs, y = value, fill = key)) + # geom_line(alpha = 0.2) + # theme(legend.position = &quot;none&quot;) + # lims(x = c(1, 365)) + # labs(x = &quot;DoY&quot;, y = &quot;Q (mm/day&quot;) Plots Plot the observed and simulated runoff. Include a legend and label the y-axis. # # make long form # PQ_long &lt;- PQ %&gt;% # select(-wtr_yr, -RainMelt_mm) %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # # # plot hydrographs (as lines) and label the y-axis # ggplot(data = PQ_long, aes(x = Date, y = value, color = key)) + # geom_line() + # labs(x = {}, y = &quot;Q (mm/day)&quot;, color = {}) + # lims(x = as.Date(c(&quot;2005-10-01&quot;, &quot;2006-09-30&quot;))) 6) QUESTION (3 pt): Evaluate how good or bad the model performed (i.e., visually compare simulated and observed streamflow, e.g., low flows and peak flows). ANSWER: 7) QUESTION (2 pt): Compare the effective precipitation total with the simulated runoff total and the observed runoff total. What is p_eff and how is it related to q_all? Discuss why there is a (small) mismatch between the sums of p_eff and q_all. ANSWER: # sum(Pobs) # observed P # # sum(p_eff) # effective (modeled) P # # sum(q_all) # modeled Q # # sum(Qobs) # observed Q THIS IS THE CODE FOR CONVOLUTION METHOD 2 This method saves storage requirements since only a vector is generated and not a full matrix that contains all response pulses. The workflow here is to generate a a response vector for the first pulse. This will take up 365 time steps. On the second time step, we generate another response pulse with 364 steps that starts at t=2. We then add that vector to the first one. On the third time step, we generate a response pulse of length 363 that starts at t=3 and then add it to the existing one. And so on and so forth. # # METHOD 2, Option 1 # tic() # # preallocate qsim vector # q_all &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # # start convolution # for (i in 1:length(p_eff)) { # A &lt;- p_eff[i] * gTF # vector with current Q pulse # B &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # reset/preallocate vector with 0s # B[i:length(p_eff)] &lt;- A[1:(length(p_eff) - i + 1)] # iteration one uses the full A, iteration 2 full lengthmodel minus one, etc # q_all &lt;- q_all + B # add new convoluted vector to total runoff # } # toc() # # # # Method 2, Option 2 # tic() # # preallocate qsim vector # q_all_2 &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # B &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # preallocate vector with 0s # # # start convolution # for (i in 1:length(p_eff)) { # A &lt;- p_eff[i] * gTF # vector with current Q pulse # B[i:length(p_eff)] &lt;- A[1:(length(p_eff) - i + 1)] # iteration one uses the full A, iteration 2 full lengthmodel minus one, etc # q_all_2[i:length(p_eff)] &lt;- q_all_2[i:length(p_eff)] + B[i:length(p_eff)] # add new convoluted vector to total runoff at correct locations # } # toc() # # # plot to show the two Q timeseries are the same # # Create two ggplot objects # plot_q1 &lt;- ggplot(data=test_q_all, aes(x=time, y=q1)) + # geom_line() + # labs(title = &quot;Q1 Plot&quot;) # # plot_q2 &lt;- ggplot(data=test_q_all, aes(x=time, y=q2)) + # geom_line(color=&quot;blue&quot;) + # labs(title = &quot;Q2 Plot&quot;) # # # Combine plots side by side # combined_plots &lt;- plot_q1 + plot_q2 + plot_layout(ncol = 2) # # # Display combined plots # combined_plots "],["monte-carlo-simulation-20-pnts.html", "Chapter 7 Monte Carlo Simulation (20 pnts) 7.1 Background: 7.2 How does this apply to hydrological modeling? 7.3 How do we generate a simulation with code? 7.4 Codework", " Chapter 7 Monte Carlo Simulation (20 pnts) 7.1 Background: Monte Carlo Simulation is a method to estimate the probability of the outcomes of an uncertain event. It is based on a law of probability theory that says if we repeat an experiment many times, the average of the results will get closer to the true probability of those outcomes. First check out this video: 7.1.1 Reading Then read this: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2924739/ for a understanding of the fundamentals. 7.2 How does this apply to hydrological modeling? When modeling watershed hydrological processes, we often attempting to quantify watershed inputs e.g., precipitation watershed outputs e.g., evapotranspiration, sublimation, runoff/discharge watershed storage precipitation that is not immediately converted to runoff or ET rather stored as snow or subsurface water. Imagine we are trying to predict the percentage of precipitation stored in a watershed after a storm event. We have learned that there are may factors that affect this prediction, like antecedent conditions, that may be difficult to measure directly. Monte Carlo Simulation can offer a valuable approach to estimate the probability of obtaining certain measurements when those factors can not be directly observed or measured. We can approximate the likelihood of specific measurement by simulating a range of possible scenarios. Monte Carlo Simulation is not only useful for estimating probabilities, but for conducting sensitivity analysis. In any model, there are usually several input parameters. Sensitivity analysis helps us understand how changes in these parameters affect the predicted values. To perform a sensitivity analysis using a Monte Carlo Simulation we can: Define the realistic ranges for each parameter we want to analyze Using Monte Carlo Simulation, randomly sample values from the defined ranges for each parameter Analyze output to understand how different input sample values affect the predicted output 7.2.1 Example Let’s consider all of this in an example model called WECOH - Watershed ECOHydrology. In this study, researchers (Nippgen et al.) were interested in the change in subsurface water storage through time and space on a daily and seasonal basis. Evolution of watershed connectivity The authors directly measured runoff/discharge, collected precipitation data at several points within the watershed, used remote sensing to estimate how much water was lost to evapotranspiration, and used digital elevation models to characterize the watershed topography. As we learned in the hydrograph separation module, topographic characteristics can have a significant impact on storage. Though resources like USDA’s Web Soil Survey can provide a basic understanding underlying geology across a large region, characterizing the heterogeneous nature of soils within a watershed can be logistically unfeasible. To estimate the soil characteristics like storage capacity (how much water the soil can hold) and hydraulic conductivity (how easily water can move through soil) in the study watershed, the authors used available resources to determine the possible range of values for each of their unknown parameters. They then tested thousands of model simulations using randomly selected values with the predetermined range for each of the soil characteristics. They compared the simulated discharge from these simulations to the actual discharge measurements. The simulations that predicted discharge patterns that closely matched reality helped them to estimate the unknown soil properties. Additionally, from the results of these simulations, they could identify which model inputs had the most significant impact on the discharge predictions, and how sensitive the output was to changes in each parameter. In this case, they determined that the model and system were most sensitive to precipitation. This type of sensitivity analysis can help us interpret the relative importance of different parameters and understand the overall sensitivity of the model or system. The study is linked for your reference but a thorough reading is not required. 7.2.2 Reading However, do read this methods paper by Knighton et al. for an example of how Monte Carlo Simulation was used to estimate hydraulic conductivity in an urban system with varied land-cover. 7.3 How do we generate a simulation with code? For a 12-minute example in RStudio, check this out. If you are still learning the basics of R functionality, it may be helpful to code along with this video, pausing as needed. Note that this instruction is coding in an Rscript (after opening RStudio &gt; File &gt; New File &gt; R Script), rather than an Rmarkdown that we use in this class. 7.4 Codework 7.4.1 Download the repo for this lab HERE In this lab/homework, you will use the transfer function model from the previous module for some sensitivity analysis using Monte Carlo simulations. The code is mostly the same as last module with some small adjustments to save the parameters from each Monte Carlo run. As a reminder, in the Monte Carlo analysis, we will run the model x number of times, save the parameters and evaluate the fit after each run. After the completion of the MC runs, we will use the GLUE method to evaluate parameter sensitivity. There are three main objectives for this homework: 1) Set up the Monte Carlo analysis 2) Run the MC simulation for ONE of the years in the study period and perform a GLUE sensitivity analysis 3) Compare the different objective functions. A lot of the code in this homework will be provided. 7.4.2 Setup Import packages, including the “progress” and “tictoc” packages. These will allow us to time our loops and functions Read data - this is the same PQ data we have worked with in previous modules. # rm(list = ls(all = TRUE)) # clear global environment # # indata &lt;- read_csv(&quot;P_Q_1996_2011.csv&quot;) # indata &lt;- indata %&gt;% # mutate(Date = mdy(Date)) %&gt;% # bring the date colum in shape # mutate(wtr_yr = if_else(month(Date) &gt; 9, year(Date) + 1, year(Date))) # create a water year column # # # choose the 2006 water year # PQ &lt;- indata %&gt;% # filter(wtr_yr == 2006) Define variables # tau &lt;- 1:nrow(PQ) # simple timestep counter # Pobs &lt;- PQ$RainMelt_mm # observed precipitation # Qobs &lt;- PQ$Discharge_mm # observed streamflow 7.4.3 Parameter initialization This chunk has two purposes. The first is to set up the number of iterations for the Monte Carlo simulation. The entire model code is essentially wrapped into the main MC for loop. Each iteration of that loop is one full model realization: loss function, TF, convolution, model fit assessment (objective function). For each model run (each MC iteration), we will save the model parameters and respective objective functions in a dataframe. This will be the main source for the GLUE sensitivity analysis at the end. The model parameters are sampled in the main model chunk, this is just the preallocation of the dataframe. You will both run your own MC simulation, to set up the code, but will also receive a .Rdata file with 100,000 runs to give you more behavioral runs for the sensitivity analysis and uncertainty bounds. As a tip, while setting up the code, I would recommend setting the number of MC iterations to something low, for example 10 or maybe even 1. Once you have confirmed that your code works, crank up the number of iterations. Set it to 1000 to see how many behavioral runs you get. After that, load the file with the provided runs. # nx sets the number of Monte Carlo runs # nx &lt;- 100 # number of runs # # # set up the parameter matrix for the loss function, transfer function, and objective functions # param &lt;- tibble( # Run = vector(mode = &quot;double&quot;, nx), # counter for model run # b1 = vector(mode = &quot;double&quot;, nx), # b2 = vector(mode = &quot;double&quot;, nx), # b3 = vector(mode = &quot;double&quot;, nx), # a = vector(mode = &quot;double&quot;, nx), # b = vector(mode = &quot;double&quot;, nx), # nse = vector(mode = &quot;double&quot;, nx), # kge = vector(mode = &quot;double&quot;, nx), # rmse = vector(mode = &quot;double&quot;, nx), # mae = vector(mode = &quot;double&quot;, nx) # ) 7.4.4 MC Model run This is the main model chunk. The tic() and toc() statements measure the execution time for the whole chunk. There is also a progressbar in the chunk that will run in the console and inform you about the progress of the MC simulation. The loss function parameters are set in the loss function, the TF parameters in the loss function code. For each loop iteration, we will store the parameter values and the simulated discharge in the “param” dataframe. So, if we ran the MC simulation 100 times, we would end up with 100 parameter combinations and simulated discharges. Q1 (3 pt) How are the loss function and TF parameters being sampled? That is, what is the underlying distribution and why did we choose it? (2-3 sentences) ANSWER: Extra point: What does the while loop do in the TF calculation? And why is it in there? (1-2 sentences) ANSWER: Save or load data After setting up the MC simulation, we will actually use a pre-created dataset. Running the MC simulation tens of thousands of times will take multiple hours. For that reason, we will use an existing data set. # save.image(file = &#39;AllData.RData&#39;) # load the MC data # load(&quot;AllData.RData&quot;) Best run Now that we have the dataframe with all parameter combinations and all efficiencies, we can plot the best simulation and compare it to the observed discharge # # take the best run (i.e., first row in param df), unlist the simulated discharge, and store it in a dataframe # PQ$Qsim &lt;- unlist(param$qsim[1]) # # # make long form # PQ_long &lt;- PQ %&gt;% # select(-wtr_yr, -RainMelt_mm) %&gt;% # remove the water year and rainmelt columns # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # # # plot observed and best simulated discharge in the same figure against the date # ggplot(PQ_long, aes(x = Date, y = value, color = key)) + # theme_bw(base_size = 15) + # geom_line() + # labs(y = &quot;Discharge (mm/day)&quot;, color = { # }) 7.4.5 Sensitivity analysis We will use the GLUE methodology to assess parameter sensitivity. We will use GLUE for two things: 1) To assess parameter sensitivity, and 2) to create an envelope of model simulations. For the first step, we need to bring the data into shape so that we can plot dotty plots and density plots. We will use NSE for this. You want to plot each parameter in its own box. Look up facet_wrap() for how to do this! You want the axis scaling to be “free”. # # select columns and make long form data # param_long &lt;- param %&gt;% # select(-Run, -kge, -rmse, -mae, -qsim) %&gt;% # remove unnecessary columns. We only want the five parameters and nse # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -nse) # make long form # # # # set nse cutoff for behavioral model simulations. use 0.7 as threshold # cutoff &lt;- 0.8 # param_long &lt;- param_long %&gt;% # filter(nse &gt; cutoff) # use filter() to only use runs with nse greater than the cutoff # # # # dotty plots # ggplot(param_long, aes(x = value, y = nse)) + # x is parameter value, y is nse value # geom_point() + # plot as points # facet_wrap(vars(key), scales = &quot;free&quot;) + # facets are the individual parameters # ylim(cutoff, 1) + # sets obj fct axis limit from the cutoff value to 1 # theme( # strip.text = element_text(face = &quot;bold&quot;, size = 8), # strip.background = element_rect( # fill = &quot;gray95&quot;, # colour = &quot;gray&quot;, # size = 0.5 # ) # ) # # # # density plots # ggplot() + # theme_bw(base_size = 15) + # geom_density(data = param_long, aes(x = value)) + # geom_density() to plot the density # facet_wrap(~key, scales = &quot;free&quot;) + # facet_wrap() to get each parameter in its own box # theme( # strip.text = element_text(face = &quot;bold&quot;, size = 8), # strip.background = element_rect( # fill = &quot;gray95&quot;, # colour = &quot;gray&quot;, # size = 0.5 # ) # ) # # # # 2d density plots # ggplot() + # geom_density_2d_filled( # geom_density_2d_filled() for the actual density plot # data = param_long, # aes(x = value, y = nse), # alpha = 1, # contour_var = &quot;ndensity&quot; # ) + # geom_point( # geom_point() to show the indivudal runs # data = param_long, # aes(x = value, y = nse), # shape = 1, # alpha = 0.2, # size = 0.5, # stroke = 0.2, # color = &quot;black&quot; # ) + # theme_bw(base_size = 15) + # facet_wrap(~key, scales = &quot;free&quot;) + # theme( # strip.text = element_text(face = &quot;bold&quot;, size = 8), # strip.background = element_rect( # fill = &quot;gray95&quot;, # colour = &quot;gray&quot;, # linewidth = 0.5 # ) # ) + # labs(x = &quot;Value&quot;, y = &quot;NSE (-)&quot;) + # theme(legend.title = element_blank(), legend.position = &quot;none&quot;) Q2 (5 pts) Describe the sensitivity analysis with the three different plots. Are the parameters sensitive? Which ones are, which ones are not? Does this affect your “trust” in the model? (5-8 sentences) ANSWER: Q3 (4 pts) What are the differences between the dotty plots and the density plots? What are the differences between the two density plots? (2-3 sentences) ANSWER: Uncertainty bounds In this chunk, we will generate uncertainty bounds for the simulation. We will again only use the runs that we identified as behavioral. # # remove non-behavioral runs using the cutoff # param_ci &lt;- param %&gt;% # filter(nse &gt; 0.872) # only use obj function (nse) values above the previously defined threshold # # # make df with all of the top runs. this saves each of the top simulated Q time series in its own column. # # the columns are called V1 through Vn, with V1 being the best simulation and Vn the worst simulation of the behavioral runs. # Qruns &lt;- # as_tibble(matrix( # unlist(param_ci$qsim), # ncol = nrow(param_ci), # byrow = F # ), .name_repair = NULL) # # # combine Qruns with date and observed runoff from the PQ data frame. additionally, calculate mins and maxs # Qruns &lt;- # bind_cols(Date = PQ$Date, Qruns) %&gt;% # bind_cols() to combine Date, RainMelt_mm, and Qruns # mutate(Qmin = apply(Qruns, 1, min)) %&gt;% # get the rowmin for simulated Qs # mutate(Qmax = apply(Qruns, 1, max)) # get the rowmax for simualted Qs # # # long form # Qruns_long &lt;- Qruns %&gt;% # # select(-Discharge_mm, -Qsim) %&gt;% # remove observed and best simulated discharge # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # make long form # # # plot with all simulated runs # ggplot() + # geom_line(data = Qruns_long, aes(x = Date, y = value, color = key)) + # plot of all simulated runs. Use Qruns_long here. # guides(color=FALSE) + # geom_line( # data = Qruns, # aes(x = Date, y = V1, color = &quot;Best run&quot;), # size = 1 # ) + # plot of best simulation. Use Qruns here. V1 is the best simulated discharge # labs(y = &quot;Q (mm/day)&quot;, x = { # }, color = { # }) # # # real min and max envelope. You need Qruns for the simulated Q and envelope and PQ to plot the observed streamflow. # ggplot() + # geom_ribbon(data = Qruns, aes(x = Date, ymin = Qmin, ymax = Qmax)) + # plot of envelopes. look up geom function that allows you to plot a shaded area between two lines # # plot the best simulated run. remember, that is V1 in the Qruns df # geom_line( # data = Qruns, # aes(x = Date, y = V1, color = &quot;Best run&quot;), # size = 0.6 # ) + # plot of best simulation # geom_line( # data = PQ, # aes(x = Date, y = Discharge_mm, color = &quot;Observed Q&quot;), # size = 0.6 # ) + # plot of observed Q # labs(y = &quot;Q (mm/day)&quot;, x = { # }, color = { # }) Q4 (2 pts) Describe what the envelope actually is. Could we say we are dealing with confidence or prediction intervals? (2-3 sentences) ANSWER: Q5 (3 pts) If you inspect the individual model runs (in the Qruns df), you will notice that they all perform somewhat poorly when it comes to the initial baseflow. Why is that and what could you do to change this? (Note: you don’t have to actually do this, just describe how you might approach that issue (2-3 sentences) ANSWER: Q6 (3 pts) Plot AND compare the best model run for the four objective functions. Note: This requires you to completely examine the figure generated in the previous code chunk. Remember that you can zoom into the plot to better see differences in peakflow and baseflow, timing, etc.(4+ sentences) ANSWER: The coding steps are: 1) get the simulated q vector with the best run for each objective function, 2) put them in a dataframe/tibble, 3) create the long form, 4) plot the long form dataframe/tibble. These steps are just ONE possible outline how the coding steps can be broken up. # # sort the param dataframe to select the best objective functions and unlist the relevant qsim values from the initial param dataset # param &lt;- arrange(param, desc(param$nse)) # nse # q1 &lt;- unlist(param$qsim[1]) # param &lt;- arrange(param, desc(param$kge)) # kge # q2 &lt;- unlist(param$qsim[1]) # param &lt;- arrange(param, param$rmse) # rmse # q3 &lt;- unlist(param$qsim[1]) # param &lt;- arrange(param, param$mae) # mae # q4 &lt;- unlist(param$qsim[1]) # # # # Create dataframe with date and the best runs for each obj function # PQobjfuns &lt;- tibble( # Date = PQ$Date, # Qsim_nse = q1, # Qsim_kge = q2, # Qsim_rmse = q3, # Qsim_mae = q4, # Qobs = Qobs # ) # # # Create a long form tibble # PQobjfuns_long &lt;- PQobjfuns %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # # # Plot up the data # # # Plot up the four simulations and label them accordingly in the legend. # # places the legend inside the plot window in the upper left corner to maximize plot space # # remember that you can zoom into the plot with plotly # all_obj_fcts &lt;- ggplot() + # geom_line(data = PQobjfuns_long, aes(x = Date, y = value, color = key)) + # labs(y = &quot;Discharge (mm/day)&quot;) + # scale_color_discrete( # name = &quot;Obj. Function&quot;, # labels = c( # &quot;Observed Runoff&quot;, # &quot;Kling-Gupta Efficiency&quot;, # &quot;Mean Absolute Error&quot;, # &quot;Nash-Sutliffe Efficiency&quot;, # &quot;Root Mean Square Error&quot; # ) # ) + # theme(legend.position = c(0.2, 0.7)) # places the legend inside the plot window # # ggplotly(all_obj_fcts) Response surface # param_surf &lt;- param %&gt;% # select(-Run, -qsim) %&gt;% # drop_na() %&gt;% # filter(nse &gt;= 0) # # ggplot() + # geom_density_2d_filled(data = param_surf, aes(x = a, y = b)) # # ggplot() + # geom_contour(data = param_surf, aes(x = a, y = b, z = nse)) "],["classifying-model-structure---lecture-only-0-pts.html", "Chapter 8 Classifying model structure - Lecture only (0 pts) 8.1 Spatial Complexity 8.2 Modeling Approaches", " Chapter 8 Classifying model structure - Lecture only (0 pts) Throughout the rest of the course, we will gather data and create models to explore how environmental factors, such as snowmelt, land cover, evapotranspiration (ET), and topography, impact runoff. To discuss these methods, we should review some modeling terminology describing model complexity and type. Environmental models, including hydrological models, are built around simplifying assumptions of natural systems. The complexity of the model may depend on its application. Effective hydrological models share key traits: they are simple, parsimonious, and robust across various watersheds. In other words, they are easy to understand and streamlined and consistently perform well across different basins or even geographical areas. Therefore, more complex is only sometimes better. 8.1 Spatial Complexity There are general terms that classify the spatial complexity of hydrological models: A lumped system is one in which the dependent variables of interest are a function of time alone, and the study basin is spatially ‘lumped’ or assumed to be spatially homogeneous across the basin. So far in this course, we have focused mainly on lumped models. You may remember the figure below from the transfer functions module. It represents the lumped watershed as a bucket with a single input, outlet output, and storage volume for each timestep. A distributed system is one in which all dependent variables are functions of time and one or more spatial variables. Modeling a distributed system means partitioning our basins into raster cells (grids) and assigning inputs, outputs, and the spatial variables that affect inputs and outputs across these cells. We then calculate the processes at the cell level and route them downstream. These models allow us to represent the spatial complexity of physically based processes. They can simulate or forecast parameters other than streamflow, such as soil moisture, evapotranspiration, and groundwater recharge. A semi-distributed system is an intermediate approach that combines elements of both lumped and distributed systems. Certain variables may be spatially distributed, while others are treated as lumped. Alternatively, we can divide the watershed into sub-basins and treat each sub-basin as a lumped basin. Outputs from each sub-basin are then linked together and routed downstream. Semi-distribution allows for a more nuanced representation of the basin’s characteristics, acknowledging spatial variability where needed while maintaining some simplifications for computation efficiency. In small-scale studies, we can design a model structure that fits the specific situation well. However, when we are dealing with larger areas, model design may be challenging. Our data might differ across regions with variable climate and landscape features. Sometimes, it is best to use a complex model to capture all the different processes happening over a big area. However, it could be better to stick with a simpler model because we might have limited data or the number of calculations is very computationally expensive. It is up to the modeler to determine the simplest model that meets the desired application. 8.2 Modeling Approaches Empirical Models are based on empirical analysis of observed inputs (e.g., rainfall) or outputs (ET, discharge). These simple models may not be transferable to other watersheds. Also, they may not reveal much about the physical processes influencing runoff. Therefore, these types of models may not be valid if the study area experiences land use or climate change. Conceptual Models describe processes with simple mathematical equations. For example, we might use a simple linear equation to interpolate precipitation inputs over a watershed with a high elevation gradient using precipitation measurements from two points (high and low). This represents the basic relationship between precipitation and elevation, but does not capture all features that affect precipitation patterns (e.g. aspect, prevailing winds). The combined impact of these factors is probably negligible compared to the substantial amount of data required to accurately model them. Physically Based Models These models offer deep insights into the processes governing runoff generation by relying on fundamental physical equations like mass conservation. However, they come with drawbacks. Their implementation often demands complex numerical solving methods and a significant volume of input data. Without empirical data to validate these techniques, there is a risk of introducing substantial uncertainty into our models, reducing their reliability and effectiveness When modeling watersheds, we often use a mix of empirical, conceptual, and physically based models. The choice of model type depends on factors like the data we have, the time or computing resources we can allocate, and how we plan to use the model. "],["snowmelt-models-for-runoff-timing---lecture-lab-20-pts.html", "Chapter 9 Snowmelt Models for runoff timing - Lecture &amp; Lab (20 pts) 9.1 Module repo 9.2 Background: 9.3 Model Approaches 9.4 Spatial complexity 9.5 Model choices: My snow is different from your snow 9.6 Codework 9.7 Modeling SWE", " Chapter 9 Snowmelt Models for runoff timing - Lecture &amp; Lab (20 pts) 9.1 Module repo 9.2 Background: Understanding snowmelt runoff is crucial for managing water resources and assessing flood risks, as it plays a significant role in the hydrologic cycle. Annual runoff and peak flow are influenced by snowmelt, rainfall, or a combination of both. In regions with a snowmelt-driven hydrologic cycle, such as the Rocky Mountains, snowpack acts as a natural reservoir, storing water during the winter months and releasing it gradually during the spring and early summer, thereby playing a vital role in maintaining water availability for various uses downstream. By examining how snowmelt interacts with other factors like precipitation, land cover, and temperature, we can better anticipate water supply fluctuations and design effective flood management strategies. Learning objectives: In this module, our primary focus will be modeling snowmelt as runoff, enabling us to predict when it will impact streamflow timing. We will consider some factors that may influence runoff timing. However, the term ‘snowmelt modeling’ is a field in itself and can represent a lifetime worth of work. There are many uses for snowmelt modeling (e.g., climate science and avalanche forecasting). If you are interested in exploring more on this subject, there is an excellent Snow Hydrology: Focus on Modeling series offered by CUAHSI’s Virtual University on YouTube. Helpful terms: The most common way to measure the water content of the snowpack is by the Snow Water Equivalent or SWE. The SWE is the water depth resulting from melting a unit column of the snowpack. 9.3 Model Approaches Similar to the model development structure we discussed in the last module, snowmelt models are generally classified into three different types of abalation algorithms Empirical and Data-Driven Models: These models use historical data and statistical techniques to predict runoff based on the relationship between snow characteristics (like snow area) and runoff. They use past patterns to make predictions about the future. The emergence of data-driven models has benefited from the growth of massive data and the rapid increase in computational power. These models simulate the changes in snowmelt runoff using machine learning algorithms to select appropriate parameters (e.g., daily rainfall, temperature, solar radiation, snow area, and snow water equivalent) from different data sources. Conceptual Models: These models simplify the snowmelt process by establishing a simple, rule-based relationship between snowmelt and temperature. These models use a basic formula based on temperature to estimate how much snow will melt. Physical Models: The physical snowmelt models calculate snowmelt based on the energy balance of snow cover. If all the heat fluxes toward the snowpack are considered positive and those away are considered negative, the sum of these fluxes is equal to the change in heat content of the snowpack for a given time period. Fluxes considered may be net solar radiation (solar incoming minus albedo), thermal radiation, sensible heat transfer of air (e.g., when air is a different temperature than snowpack), latent heat of vaporization from condensation or evaporation/sublimation, heat conducted from the ground, advected heat from precipitation examples: layered snow thermal model (SNTHERM) and physical snowpack model (SNOWPACK), Many effective models may incorporate elements from some or all of these modeling approaches. 9.4 Spatial complexity We may also identify models based on the model architecture or spatial complexity. The architecture can be designed based on assumptions about the physical processes that may affect the snowmelt to runoff volume and timing. Homogenous basin modeling: You may also hear these types of models referred to as ‘black box’ models. Black-box models do not provide a detailed description of the underlying hydrological processes. Instead, they are typically expressed as empirical models that rely on statistical relationships between input and output variables. While these models can predict specific outcomes effectively, they may not be ideal for understanding the physical mechanisms that drive hydrological processes. In terms of snow cover, this is a simplistic case model where we assume: the snow is consistent from top to bottom of the snow column and across the watershed melt appears at the top of the snowpack water immediately flows out the bottom This type of modeling may work well if the snowpack is isothermal, if we are interested in runoff over large timesteps, or if we are modeling annual water budgets in lumped models. Vertical layered modeling: Depending on the desired application of the model, snowmelt may be modeled in multiple layers in the snow column (air-snow surface to ground). Climate models, for example, may estimate phase changes or heat flux and consider the snowpack in 5 or more layers. Avalanche forecasters may need to consider grain evolution, density, water content, and more over hundreds of layers! Hydrologists may also choose variable layers, but many will choose single- or two-layer models for basin-wide studies, as simple models can be effective when estimating basin runoff. Here is a study by Dutra et al. (2012) that looked at the effect of the number of snow layers, liquid water representation, snow density, and snow albedo parameterizations within their tested models. Table 1 and figures 1-3 will be sufficient to understand the effects of changes to these parameters on modeled runoff and SWE. In this case, the three-layer model performed best when predicting the timing of the SWE and runoff, but density improved more by changing other parameters rather than layers (Figure 1). Lateral spatial heterogeneity: The spatial extent of the snow cover determines the area contributing to runoff at any given time during the melt period. The more snow there is, the more water there will be when it melts. Therefore, snow cover tells us which areas will contribute water to rivers and streams as the snow melts. In areas with a lot of accumulated snow, the amount of snow covering the ground gradually decreases as the weather warms up. This melting process can take several months. How quickly the snow disappears depends on the landscape. For example, in mountainous ecosystems, factors like elevation, slope aspect, slope gradient, and forest structure affect how the snow can accumulate, evaporate or sublimate and how quickly the snow melts. For mountain areas, similar patterns of depletion occur from year to year and can be related to the snow water equivalent (SWE) at a site, accumulated ablation, accumulated degree-days, or to runoff from the watershed using depletion curves from historical data. Here is an example of snow depletion curves developed using statistical modeling and remotely sensed data. The use of remotely sensed data can be incredibly helpful to providing estimates in remote areas with infrequent measurements. Observing depletion patterns may not be effective in ecosystems where patterns are more variable (e.g., prairies). However, stratified sampling with snow surveys, snow telemetry networks (SNOTEL) or continuous precipitation measurements can be used with techniques like cluster analyses or interpolation, to determine variables that influence SWE and estimate SWE depth or runoff over heterogeneous systems. You can further explore all readings linked in the above section. These readings may assist in developing the workflow for your term project, though they are optional for completing this assignment. However, it is recommended that you review the figures to grasp the concepts and retain them for future reference if necessary. 9.5 Model choices: My snow is different from your snow When determining the architecture of your snow model, your model choices will reflect the application of your model and the processes you are trying to represent. Recall that parsimony and simplicity often make for the most effective models. So, how do we know if we have a good model? Here are a few things we can check to validate our model choices: Model Variability: A good model should produce consistent results when given the same inputs and conditions. Variability between model runs should be minimal if the watershed or environment is not changing. Versatility: Check the model under a range of scenarios different from the conditions under which it was developed. The model should apply to similar systems or scenarios beyond the initial scope of development Sensitivity Analysis: We reviewed this a bit in the Monte Carlo module. How do changes in model inputs impact outputs? A good model will show reasonable sensitivity changes in input parameters, with outputs responding as expected. Validation with empirical data: Comparison with real-world data checks whether the model accurately represents the actual system Applicability and simplicity: A good model should provide valuable insights or aid in decision-making processes relevant to the problem it addresses. It strikes a balance between complexity and simplicity, avoiding unnecessary intricacies that can lead to overfitting or computational inefficiency while sufficiently capturing the system’s complexities. 9.6 Codework 9.6.1 Download the repo for this lab HERE In this module, we will simulate snowmelt in a montane watershed in central Colorado with a simple temperature model. The Fool Creek watershed is located in the Fraser Experimental Forest (FEF), 137 km west of Denver, Colorado. The FEF contains several headwater streams (Strahler order 1-3) within the Colorado Headwaters watershed which supplies much of the water to the populated Colorado Front Range. This Forest has been the site of many studies to evaluate the effects of land cover change on watershed hydrology. The Fool Creek is a small, forested headwater stream, with an approximate watershed area of 2.75 km^2. The watershed elevation ranges from approximately 2,930m (9,600ft) at the USFS monitoring station to 3,475m (11,400ft) at the highest point. There is a SNOTEL station located at 3,400m. Fool Creek delineated watershed Question 1: Using this map, what differences can you see between these two sites or what variation do you see in the watershed that may affect snow accumulation and melt? (2 points)? ANSWER: Let’s look at some data for Fool Creek: This script collects SNOTEL input data using snotelr. The SNOTEL automated collection site in Fool Creek supplies SWE data that can simplify our runoff modeling. Lets explore the sites available through snotelr and select the site of interest. # Save metadata for review. # meta_data &lt;- snotel_info() # In the view window, you can find Fool Creek site information by sorting by state or site name using the arrows at the top of each column. # View(meta_data) # Locate the site_id for Fool Creek 9.6.2 Import data: We will generate a conceptual runoff model using the date, daily mean temperature, SWE depth in mm, and daily precipitation (mm). We will use select() to keep the desired columns only. Then we will use mutate() to add a water year, and group_by() to add a cumulative precipitation column for each water year. # # Let&#39;s download data for the water years 2021 to 2022: # df &lt;- snotel_download(site_id = 1186, internal = TRUE) %&gt;% # mutate(date = lubridate::ymd(date)) %&gt;% # format the date # filter(between(date, lubridate::ymd(&quot;2020-10-01&quot;), lubridate::ymd(&quot;2022-09-30&quot;))) %&gt;% # select desired dates # rename(swe.mm = snow_water_equivalent, # rename columns for easier use # precip.mm = precipitation, # temp_mean.c = temperature_mean) %&gt;% # # only select data we need # select(date, swe.mm, precip.mm, temp_mean.c) %&gt;% # mutate(wtr_yr = if_else(month(date) &lt; 10, year(date), year(date) + 1)) %&gt;% # group_by(wtr_yr) %&gt;% # add a water year column # mutate(pcumul.mm = cumsum(precip.mm)) %&gt;% # generate a cumulative sum column for each water year # ungroup() We will also download flow data collected by USFS at the Fool Creek outlet at 2,930m. # fool_q &lt;- read.csv(&quot;LFC_weir WY21-WY22.csv&quot;) %&gt;% # mutate(date = mdy_hm(TIMESTAMP)) %&gt;% # # convert cubic feet per second to metric (m/s^3) # # 1 cfs is equal to 0.028316847 cubic meter/second # mutate(m3_s = Q..cfs.*0.028316847) Let’s look at the data for the SNOTEL station at 3400m in elevation. # Plot imported SWE, precip and depth data data # ggplot(data = df) + # geom_line(aes(x = date, y = swe.mm, color = &#39;swe.mm&#39;)) + # geom_line(aes(x = date, y = pcumul.mm, color = &#39;pcumul.mm&#39;)) + # geom_line(aes(x=date, y = precip.mm, color = &#39;precip.mm&#39;)) + # theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels by 45 degrees # axis.line = element_line(color = &quot;black&quot;), # Add black axis lines # panel.background = element_rect(fill = &quot;white&quot;), # Set panel background to white # panel.grid.major = element_blank(), # Remove major grid lines # panel.grid.minor = element_blank()) + # Remove minor grid lines # ylab(&quot;mm&quot;) + # xlab(&#39;Date&#39;) 9.6.3 Calculate liquid input to the ground by analyzing the daily changes in Snow Water Equivalent (SWE) and precipitation. This script incorporates temperature conditions to determine when to add changes to liquid input. # sntl &lt;- df # # sntl &lt;- sntl %&gt;% # # calculate differences for SWE and cumulative P and put 0 as the first value # mutate(SWEdiff.mm = c(0, diff(sntl$swe.mm, lag = 1)), #find daily change in SWE # Pdiff.mm = c(0, diff(pcumul.mm, lag = 1))) %&gt;% # # remove the negative values at the start of each year # mutate(Pdiff.mm = if_else(Pdiff.mm &lt; 0, 0, Pdiff.mm)) %&gt;% # # conditional statements for calculate liquid input to the ground # mutate(input.mm = if_else(Pdiff.mm &gt; 0 &amp; SWEdiff.mm, 0, 0), # input.mm = if_else(Pdiff.mm == 0 &amp; SWEdiff.mm &lt; 0, -SWEdiff.mm, input.mm), # input.mm = if_else(Pdiff.mm &gt; 0 &amp; SWEdiff.mm &lt; 0, Pdiff.mm - SWEdiff.mm, input.mm), # input.mm = if_else(Pdiff.mm &gt; 0 &amp; SWEdiff.mm == 0, Pdiff.mm, input.mm)) %&gt;% # # this replaces missing air temp values with 0 # mutate(temp_mean.c = replace_na(temp_mean.c, 0)) %&gt;% # # this is a check that prevents liquid inputs when the air temp is below 0 # mutate(input.mm = if_else(temp_mean.c &lt; 0 &amp; input.mm &gt; 0, 0, input.mm)) %&gt;% # mutate(date = lubridate::ymd(date)) Question 2: In plain language, explain each step of the 5 if-else statements above (~5 brief sentences) (3pts). ANSWER: # # Create a cumulative input column for visualization and calculations # sntl &lt;- sntl %&gt;% # group_by(wtr_yr) %&gt;% # mutate(cum_input.mm = cumsum(input.mm)) %&gt;% # generate a cumulative sum column for each water year # ungroup() Let’s visualize the timing disparities between precipitation and melt input in relation to discharge. # # Convert date column to POSIXct format # fool_q$date &lt;- as.POSIXct(fool_q$date) # sntl$date &lt;- as.POSIXct(sntl$date) # # # Plot imported SWE, precip, and depth data # ggplot() + # # Scale discharge for the right y-axis # geom_line(data = fool_q, aes(x = date, y = m3_s*10000, color = &#39;m^3/second&#39;)) + # geom_point(data = sntl, aes(x = date, y = cum_input.mm, color = &#39;cumulative input (mm)&#39;)) + # geom_point(data = sntl, aes(x = date, y = pcumul.mm, color = &#39;cumulative precipitation (mm)&#39;)) + # theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels by 45 degrees # axis.line = element_line(color = &quot;black&quot;), # Add black axis lines # panel.background = element_rect(fill = &quot;white&quot;), # Set panel background to white # panel.grid.major = element_blank(), # Remove major grid lines # panel.grid.minor = element_blank(), # Remove minor grid lines # ) + # ylab(&quot;mm&quot;) + # xlab(&#39;Date&#39;) + # scale_y_continuous(sec.axis = sec_axis(trans = ~ . / 10000, name = expression(Q[paste(m^3,&#39;/second&#39;)]))) #print(max(sntl$cum_input.mm, na.rm = TRUE)) #print(max(sntl$pcumul.mm, na.rm = TRUE )) Question 3. What processes could account for the differences between cumulative input, calculated from SWE data, and cumulative precipitation, both measured at the same SNOTEL station? (1 or 2 processes) What could explain the discrepancy if the cumulative input, is found to be less than the cumulative precipitation measured at the same SNOTEL station? (at least 2 possible processes) (4 points) (2-3 sentences) ANSWER: 9.7 Modeling SWE Let’s assume that we have a precipitation gage at 3400m in Fool Creek, but without SWE data, how could we model SWE using temperature? # # Filter to a single water year for the snowmelt model # wateryr = &#39;2021&#39; # # # set the year # sntl_SINGLE_YEAR &lt;- sntl %&gt;% # filter(wtr_yr == wateryr) # # print(sum(sntl_SINGLE_YEAR$input.mm)) The model below is a variation of the degree-day method. Here we are running a similar script to the input calculations above, using temperature as the primary determinant. When the temperature falls below a certain threshold and precipitation occurs, an equivalent amount is added to our SWE accumulation. We’ll conduct a simulation with 100 Monte Carlo runs and analyze the optimal outcome. The parameters we are testing are pack threshold, a degree-day factor, and the threshold temperature for determining rain from snow. # ######################################## # # nx sets the number of Monte Carlo runs # nx &lt;- 100 # number of runs # # # set up the parameter matrix # param &lt;- tibble( # Run = vector(mode = &quot;double&quot;, nx), # ddf = vector(mode = &quot;double&quot;, nx), # Tb = vector(mode = &quot;double&quot;, nx), # pack_threshold = vector(mode = &quot;double&quot;, nx), # nse = vector(mode = &quot;double&quot;, nx), # kge = vector(mode = &quot;double&quot;, nx), # SWEsim = vector(mode = &quot;list&quot;, nx) # ) # # tic(&quot;MC Simulation&quot;) # pb &lt;- progress_bar$new( # format = &quot; downloading [:bar] :percent eta: :eta&quot;, # total = nx, clear = FALSE, width = 60 # ) # initialize progress bar # # #### MONTE CARLO SETUP (use ii as index) # for (n in 1:nx) { # pb$tick() # needed for progress bar # # # write run number into the parameter matrix # param$Run[n] &lt;- n # # ##################### # # initiate parameters # param$ddf[n] &lt;- runif(1, 0, 5) # Threshold temperature for distinguishing rain from snow. # param$Tb[n] &lt;- runif(1, -5, 5) # Degree-day factor for snowmelt calculations, utilized in calculating the potential melt (Melt) based on the difference between the mean temperature (temp_mean.c) and Tb. # param$pack_threshold[n] &lt;- runif(1, 0, 100) #threshold value used in determining whether precipitation falls as snow or accumulates on existing snowpack # # ######################################## # # generate columns with snow, rain, melt # SWE.Model &lt;- sntl_SINGLE_YEAR %&gt;% # # liquid precipitation # mutate(Rain = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &gt;= param$Tb[n], Pdiff.mm, 0)) %&gt;% # # snow accumulation # mutate(Snow = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &lt; param$Tb[n], Pdiff.mm, 0)) %&gt;% # # potential melt # mutate(Melt = if_else(temp_mean.c &gt; param$Tb[n], param$ddf[n]*(temp_mean.c - param$Tb[n]), 0)) %&gt;% # # cumulative SWE without melt # mutate(SWE.cumul = cumsum(Snow)) %&gt;% # # no melt if swe &lt;= 0 (here only for first few steps) # mutate(Melt = if_else(SWE.cumul == 0, 0, Melt)) # # # ######################################## # # for loop to loop through each timestep # SWEsim &lt;- vector() # SWEsim[1] &lt;- 0 # set first value to 0 # for (ii in 2:dim(SWE.Model)[1]) { # # # snow accumulation if snowfall happens # SWEsim[ii] &lt;- if_else(SWE.Model$Snow[ii] &gt; 0, SWE.Model$Snow[ii]+SWEsim[ii-1], # # #This condition checks if the existing snow water equivalent (SWE) surpasses the pack threshold. If so, then the precipitation is considered to contribute to the snowpack rather than falling as rain # # snow accumulation if liquid precip but snow on the ground # if_else(SWE.Model$Rain[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; param$pack_threshold[n], SWE.Model$Rain[ii]+SWEsim[ii-1], # # # melt if SWEsim on the ground larger than the melt pulse # if_else(SWE.Model$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; SWE.Model$Melt[ii], SWEsim[ii-1] - SWE.Model$Melt[ii], # # # melt if SWEsim is present but smaller than melt pulse # if_else(SWE.Model$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &lt; SWE.Model$Melt[ii], 0, SWEsim[ii-1])))) # } # # # store simulated SWEsim in df # param$SWEsim[n] &lt;- list(SWEsim) # # # #################### # # We will store efficiencies comparing observed SWE with simulated SWE. # # NSE # param$nse[n] &lt;- 1 - ((sum((SWE.Model$swe.mm - SWEsim)^2)) / sum(((SWE.Model$swe.mm - mean(SWE.Model$swe.mm))^2))) # # # KGE # kge_r &lt;- cor(SWE.Model$swe.mm, SWEsim) # kge_beta &lt;- mean(SWEsim) / mean(SWE.Model$swe.mm) # kge_gamma &lt;- (sd(SWEsim) / mean(SWEsim)) / (sd(SWE.Model$swe.mm) / mean(SWE.Model$swe.mm)) # param$kge[n] &lt;- 1 - sqrt((kge_r - 1)^2 + (kge_beta - 1)^2 + (kge_gamma - 1)^2) # # # } # toc() # # ######################################### # # sort parameter matrix from highest to lowest NSE and save it # param &lt;- arrange(param, desc(nse)) # save(param, file = paste(wateryr, &quot;SWE.Model.Runs.RData&quot;, sep = &quot;.&quot;)) # # # best run # # take the best run, get the list of simulated values and unlist it in order to store it in a dataframe # SWE.Model$SWEsim.mm &lt;- unlist(param$SWEsim[1]) # # Plot best run # swe_plot &lt;- ggplot(data = SWE.Model) + # geom_line(aes(x = date, y = SWEsim.mm, color = &quot;Simulated SWE&quot;)) + # geom_line(aes(x = date, y = swe.mm, color = &quot;Actual SWE&quot;)) + # annotate(&quot;text&quot;, x = mean(SWE.Model$date, na.rm = TRUE), y = min(SWE.Model$swe.mm, na.rm = TRUE), # label = paste(&quot;NSE =&quot;, round(param$nse[1], 2)), hjust = 1, vjust = 1) # # ggplotly(swe_plot) This simple model seems to simulate SWE well if assessed with the NSE. Let’s model SWE at the Fool Creek outlet, where we do not have daily SWE data. First we’ll bring in precipitation data, measured at a USFS maintained meteorological station located near the Fool outlet: # fc_precip &lt;- read.csv(&quot;LFCmet_WY21_WY22.csv&quot;) %&gt;% # mutate(date = lubridate::mdy_hm(TS)) %&gt;% # select(date, water_year, PPT_10min..mm.) %&gt;% # filter(water_year %in% c(2021, 2022)) We also need temperature data. This is also collected at the USFS Lower Fool meteorological station. # fc_temp &lt;- read.csv(&quot;lfc_airtemp.csv&quot;) %&gt;% # mutate(date = lubridate::mdy_hm(TMSTAMP)) %&gt;% # mutate(wtr_yr = if_else(month(date) &lt; 10, year(date), year(date) + 1)) %&gt;% # select(date, Air_TempC_Avg, wtr_yr) %&gt;% # filter(wtr_yr %in% c(2021, 2022)) # # lfc_data &lt;- merge(fc_temp, fc_precip, by = &quot;date&quot;, validate = &quot;one_to_one&quot;) %&gt;% # select(date, Air_TempC_Avg, wtr_yr, PPT_10min..mm.) %&gt;% # rename(precip.mm = PPT_10min..mm., # temp_mean.c = Air_TempC_Avg) # # rename columns so we can reuse the SWE model without changing variable names within the simulation code # # # Lets add cumulative precip totals by water year # lfc_data &lt;- lfc_data %&gt;% # group_by(wtr_yr) %&gt;% # mutate(pcumul.mm = cumsum(precip.mm)) %&gt;% # generate a cumulative sum column for each water year # ungroup() # # # Aggregate all data to daily means for the sake of simulation time # lfc_data &lt;- lfc_data %&gt;% # group_by(date = as.Date(date)) %&gt;% # summarise(across(.cols = everything(), .fns = mean, na.rm = TRUE)) Even though this meteorological station is very close to the SNOTEL station, lets compare the values between the upper watershed and the lower # # Convert date column in sntl to Date format that matches lfc_data # sntl$date &lt;- as.Date(sntl$date) # # ggplot() + # # Scale discharge for the right y-axis # geom_point(data = lfc_data, aes(x = date, y = pcumul.mm, color = &#39;lower fool creek precip (mm)&#39;)) + # geom_point(data = sntl, aes(x = date, y = pcumul.mm, color = &#39;upper fool creek precip (mm)&#39;)) + # theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis # panel.grid.minor = element_blank(), # Remove minor grid lines # ) + # ylab(&quot;mm&quot;) + # xlab(&#39;Date&#39;) Now let’s estimate SWE for the Lower Fool Creek. Again, we’ll just simulate a single year. # # Filter to a single water year for the snowmelt model # wateryr = &#39;2021&#39; # # # aggregate data to daily means for the sake of simulation time # lfc_data &lt;- lfc_data %&gt;% # mutate(Pdiff.mm = pcumul.mm - lag(pcumul.mm, default = first(pcumul.mm))) # # # set the year # lfc_SINGLE_YEAR &lt;- lfc_data %&gt;% # filter(wtr_yr == wateryr) Run another set of simulations and compare the values of the best performing parameters across sites. # ######################################## # # nx sets the number of Monte Carlo runs # nx &lt;- 100 # number of runs # # # set up the parameter matrix # param &lt;- tibble( # Run = vector(mode = &quot;double&quot;, nx), # ddf = vector(mode = &quot;double&quot;, nx), # Tb = vector(mode = &quot;double&quot;, nx), # pack_threshold = vector(mode = &quot;double&quot;, nx), # nse = vector(mode = &quot;double&quot;, nx), # kge = vector(mode = &quot;double&quot;, nx), # SWEsim = vector(mode = &quot;list&quot;, nx) # ) # # tic(&quot;MC Simulation&quot;) # pb &lt;- progress_bar$new( # format = &quot; downloading [:bar] :percent eta: :eta&quot;, # total = nx, clear = FALSE, width = 60 # ) # initialize progress bar # # #### MONTE CARLO SETUP (use ii as index) # for (n in 1:nx) { # pb$tick() # needed for progress bar # # # write run number into the parameter matrix # param$Run[n] &lt;- n # # ##################### # # initiate parameters # param$ddf[n] &lt;- runif(1, 0, 5) # Threshold temperature for distinguishing rain from snow. # param$Tb[n] &lt;- runif(1, -5, 5) # Degree-day factor for snowmelt calculations, utilized in calculating the potential melt (Melt) based on the difference between the mean temperature (temp_mean.c) and Tb. # param$pack_threshold[n] &lt;- runif(1, 0, 100) #threshold value used in determining whether precipitation falls as snow or accumulates on existing snowpack # # ######################################## # # generate columns with snow, rain, melt # SWE.Modellfc &lt;- lfc_SINGLE_YEAR %&gt;% # # liquid precipitation # mutate(Rain = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &gt;= param$Tb[n], Pdiff.mm, 0)) %&gt;% # # snow accumulation # mutate(Snow = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &lt; param$Tb[n], Pdiff.mm, 0)) %&gt;% # # potential melt # mutate(Melt = if_else(temp_mean.c &gt; param$Tb[n], param$ddf[n]*(temp_mean.c - param$Tb[n]), 0)) %&gt;% # # cumulative SWE without melt # mutate(SWE.cumul = cumsum(Snow)) %&gt;% # # no melt if swe &lt;= 0 (here only for first few steps) # mutate(Melt = if_else(SWE.cumul == 0, 0, Melt)) # # # ######################################## # # for loop to loop through each timestep # SWEsim &lt;- vector() # SWEsim[1] &lt;- 0 # set first value to 0 # for (ii in 2:dim(SWE.Modellfc)[1]) { # # # snow accumulation if snowfall happens # SWEsim[ii] &lt;- if_else(SWE.Modellfc$Snow[ii] &gt; 0, SWE.Modellfc$Snow[ii]+SWEsim[ii-1], # # # snow accumulation if liquid precip but snow on the ground # if_else(SWE.Modellfc$Rain[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; param$pack_threshold[n], SWE.Model$Rain[ii]+SWEsim[ii-1], # # # melt if SWEsim on the ground larger than the melt pulse # if_else(SWE.Modellfc$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; SWE.Modellfc$Melt[ii], SWEsim[ii-1] - SWE.Modellfc$Melt[ii], # # # melt if SWEsim is present but smaller than melt pulse # if_else(SWE.Modellfc$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &lt; SWE.Modellfc$Melt[ii], 0, SWEsim[ii-1])))) # } # # # store simulated SWEsim in df # param$SWEsim[n] &lt;- list(SWEsim) # # # #################### # # We will store efficiencies comparing observed SWE with simulated SWE. # # NSE # param$nse[n] &lt;- 1 - ((sum((SWE.Model$swe.mm - SWEsim)^2)) / sum(((SWE.Model$swe.mm - mean(SWE.Model$swe.mm))^2))) # # # KGE # kge_r &lt;- cor(SWE.Model$swe.mm, SWEsim) # kge_beta &lt;- mean(SWEsim) / mean(SWE.Model$swe.mm) # kge_gamma &lt;- (sd(SWEsim) / mean(SWEsim)) / (sd(SWE.Model$swe.mm) / mean(SWE.Model$swe.mm)) # param$kge[n] &lt;- 1 - sqrt((kge_r - 1)^2 + (kge_beta - 1)^2 + (kge_gamma - 1)^2) # # # } # toc() # # ######################################### # # sort parameter matrix from highest to lowest NSE and save it # param_lfc &lt;- arrange(param, desc(nse)) # # Can save simulated data if desired # #save(param_lfc, file = paste(wateryr, &quot;SWE_lfc.Model.Runs.RData&quot;, sep = &quot;.&quot;)) # # # # take the best run, get the list of simulated values and unlist it in order to store it in a dataframe # SWE.Modellfc$SWEsim.mm &lt;- unlist(param_lfc$SWEsim[1]) Since we don’t have SWE measurement for Lower Fool Creek, let’s see how the simulated values compare to acutal Upper Fool SNOTEL SWE measurements. # SWE.Model$date &lt;- as.Date(SWE.Model$date) # # Plot best run # swe_plot &lt;- ggplot() + # geom_line(data = SWE.Modellfc, aes(x = date, y = SWEsim.mm, color = &quot;Simulated SWE&quot;)) + # geom_line(data = SWE.Model, aes(x = date, y = swe.mm, color = &quot;Actual SWE&quot;)) + # annotate(&quot;text&quot;, x = mean(SWE.Model$date, na.rm = TRUE), y = min(SWE.Model$swe.mm, na.rm = TRUE), # label = paste(&quot;NSE =&quot;, round(param$nse[1], 2)), hjust = 1, vjust = 1) # # ggplotly(swe_plot) # compare the best performing parameters for the Upper Fool Creek and Lower Fool Creek # print(param[1, ]) # print(param_lfc[1, ]) # # # You can change the indexing and wording in this statement if you want to explore all parameters. # print(paste(&#39;The estimated threshold temperature distinguishing rain from snow in Lower Fool is &#39;, round(param_lfc[1, 2], 2), &#39;degrees and in Upper Fool Creek is&#39;, round(param[1, 2],2), &#39;degrees&#39;)) Question 3: Based on the differences in precipitation and temperature totals between the two sites, are SWE simulation results as you expected? (1 sentence)(2pnts) ANSWER: Question 4: How does the estimated threshold temperature distinguishing rain from snow change with elevation? Is the relationship between your parameter estimations as expected? How does this influence your confidence in the model? (2pnts) ANSWER Question 4: Compare the precipitation and snow accumulation patterns between two sites. Discuss at least 3 potential natural and/or anthropogenic mechanisms driving the observed variations in precipitation and snow accumulation. Consider the factors you observed in question 1 (3 points)(2-3 sentences). ANSWER: Question 5: Let’s say you want to model runoff for the Fool Creek catchment identified above. To do that, you want to estimate the timing and volume of snowmelt for this watershed. Refer to at least 2 model types reviewed in the 06_model_classifications lecture or terms in the lecture above and qualitatively describe a model that you could use to capture the spatial heterogeneity of snow accumulation in this watershed (4 points) (2-4 sentences). ANSWER: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
