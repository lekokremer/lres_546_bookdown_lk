[["index.html", "Quantitative Environmental Methods: LRES 546 Chapter 1 Introduction 1.1 Course overview and objectives 1.2 Course Description 1.3 Structure 1.4 Philosophical approach and resources 1.5 Tentative schedule, subject to change", " Quantitative Environmental Methods: LRES 546 YOUR NAME HERE Chapter 1 Introduction This book provides the materials that we will use in Quantitative Environmental Methods (LRES 546). In this class we will be learning the fundamentals of environmental data analysis and simulation in R. This class can be taken online or in-person. The online will be asynchronous. As a function of demand, in 2024, this class will be taught online. Instructor: Dr. Tim Covino Class times (depending on demand): T 13:40 – 14:55; Th 13:40 – 14:55 - For 2024 the class will be online, asynchronous. Office hours: By appointment Email: timothy.covino@montana.edu TA: Lauren Kremer Email: lauren.kremer@montana.edu 1.1 Course overview and objectives provide theoretical understanding and practical experience with common analysis and modeling techniques relevant to watershed hydrology/environmental science. provide training in analyzing, simulating, and presenting scientific data in written and oral formats. 1.2 Course Description This course will focus on development of quantitative analysis and modeling skills in watershed and environmental science. Students will develop skills necessary to perform quantitative analyses, describe and evaluate model structure, evaluate the merit of different models of varying type and complexity, and use quantitative analyses to address problems in environmental/watershed science. Students will apply computer programming in R to analyze and simulate watershed and/or environmental dynamics spanning simple to complex processes, analyses, and simulations. Technical skills and conceptual understanding will be built through lectures, readings, and hands-on quantitative projects. Note: If you aren’t familiar with R, or don’t have coding experience, don’t worry. We will walk you through all of the coding. I hope that by the end of this class each student will be a strong quantitative scientist with equally strong coding skills. 1.3 Structure This class will utilize hands-on/active learning, and students will be conducting watershed analyses and modeling exercises. We will be doing our analysis and modeling in R and will do some R coding in each class/work session. Programming is best learned by doing it often. Each week there will be recorded videos and/or readings, where we will talk about and work through various types of hydrological analyses. We will then put the content from the recorded lectures to work in a lab where students will complete a variety of hydrological analyses in R. Students can work through material on their schedule, but Dr. Covino and Lauren Kremer (TA) will be available during lab (Thursday 13:40 - 14:55) to help with technical coding problems or to answer other questions on weekly labs. 1.4 Philosophical approach and resources This course will use all online and open-source resources and will follow FAIR (findability, accessibility, interoperability, and reusability) data principles and promote sharing of hydrological education and research materials. Our computing will utilize open source R and RStudio software. Books/resources, we may not use all of these, but they are good references: - R for Data Science - Statistical Methods in Water Resources - Tidy modeling with R - ggplot2: Elegant Graphics for Data Analysis - Advanced R - R Packages - Environmental Data Science Additional readings will be made available on this bookdown page as needed. 1.5 Tentative schedule, subject to change Week 1 (Unit 0): - Introduction, overview, and technical skills. - If you need a refresher for R please see Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). Week 2 (Unit 1): - Hydrograph separation. Week 3 (Unit 2): Week 4 (Unit 2): - Frequency analysis Week 5 (Unit 3): - Rational and curve number (CN) methods Week 6 (Unit 4): - Transfer function lumped hydrological model Week 7 (Unit 5): - Monte Carlo and sensitivity analysis Week 8 (Unit 6): - HBV model – lumped to semi-distributed modeling Week 9: Spring Break Week 10 (Unit 7): - DEM processing and analysis in Whitebox Week 11 (Unit 8): - Gridded and flow data retrieval Week 12 (Unit 9): - Precipitation interpolation Week 13 (Unit 10): - Evapotranspiration (ET) models Week 14: - Snowmelt models Week 15: - Distributed model (WECOH or ATS) and/or term project work Week 16: - Final presentations "],["hydrograph-separation-learning-module.html", "Chapter 2 Hydrograph separation: learning module 2.1 Summary 2.2 Overall Learning Objectives 2.3 Lecture 2.4 Assessment 2.5 Interactive application and questions", " Chapter 2 Hydrograph separation: learning module This module was developed by Ana Bergrstrom and Matt Ross. 2.1 Summary Streamflow can come from a range of water sources. When it is not raining, streams are fed by the slow drainage of groundwater. When a rainstorm occurs, streamflow increases and water enters the stream more quickly. The rate at which water reaches the stream and the partitioning between groundwater and faster flow pathways is variable across watersheds. It is important to understand how water is partitioned between fast and slow pathways (baseflow and stormflow) and what controls this partitioning in order to better predict susceptibility to flooding and if and how groundwater can sustain streamflow in long periods of drought. In this module we will introduce the components of streamflow during a rain event, and how event water moves through a hillslope to reach a stream. We will discuss methods for partitioning a hydrograph between baseflow (groundwater) and storm flow (event water). Finally, we will explore how characteristics of a watershed might lead to more or less water being partitioned into baseflow vs. stormflow. We will test understanding through evaluating data collected from watersheds in West Virginia to determine how mountaintop mining, which fundamentally changed the watershed structure, affects baseflow. Note: While this assessment is written for watersheds in West Virginia, we have designed the course so it is adaptable. Instructors should be able to easily substitute in their own data for the assessment. 2.2 Overall Learning Objectives At the end of this module, students should be able to describe the components of streamflow, the basics of how water moves through a hillslope, and the watershed characteristics that affect partitioning between baseflow and stormflow. 2.3 Lecture 2.3.1 Components of streamflow during a rain event During a rainstorm, precipitation is falling across the watershed: close to the stream, on a hillslope, and up at the watershed divide. This water that falls across the watershed flows downslope toward the stream via a number of flow pathways. Here we define and describe the basic flow pathways during a rain event. The first component is channel interception. This is water that falls directly on the water surface of the stream. The amount of water that falls directly on the channel is a function of stream size, if we have a very small, narrow creek, this will be a very small quantity. However, you can imagine that in a very large, broad river such as the Amazon, this volume of water is much larger. Channel interception is the first component during a rain event that causes streamflow to increase because it is contributing directly to the stream and therefore has no travel time. The second is overland flow, which is water that flows on the land surface to the stream. Overland flow can occur via a number of mechanisms which we will not explore too deeply here, but encourage further study on your own (resources provided). Briefly, overland flow includes water that falls on an impermeable surface such as pavement, water that runs downslope due to rain falling faster than the rate at which it can infiltrate the ground surface, and water that flows over the land surface because the ground is completely saturated. Overland flow is typically faster than water that travels through soils and deeper flow pathways and therefore is the next major component that starts to contribute to the increase in streamflow during a rain event. The third component is subsurface flow. This is water that infiltrates the land surface and flows downslope through shallow groundwater flow pathways. This is the last component that increases streamflow during a storm event, is the slowest of the stormflow components, and can contribute to elevated streamflow for a while after precipitation ends. The final component is baseflow. Baseflow can also be described as groundwater. This component is what sustains streamflow between rain events, but also continues to contribute during a rain event. Of water that infiltrates the ground surface, some moves quickly to the stream as subsurface flow, but some moves deeper and becomes part of deeper groundwater and baseflow. Thus baseflow can increase in times of higher wetness in the watershed, particularly during and right after rainy seasons or spring snowmelt. We can simplify this partitioning into baseflow and stormflow (often called quickflow). Baseflow being groundwater that moves more slowly and sustains streamflow between rain events. Stormflow is water that contributes to streamflow as a result of a rain event. Under this definition we can lump channel interception, overland flow, and subsurface flow into stormflow. 2.3.2 Storm flow through a hillslope When rain falls on the land surface, much of it infiltrates into the soil. Water moves through the soil column until it meets a layer of lower permeability and runs down the hillslope as subsurface flow. This layer of lower permeability allows some water to move through it, contributing to groundwater. Frequently the layer of lower permeability is the interface between soil and rock. Therefore the depth of soil has a large effect on how much water moves through soil vs. how much moves deeper into groundwater, becoming baseflow. 2.3.3 How we quantify baseflow It is impossible to know the amount of water moving as overland, subsurface, and base flow in all parts of a watershed. So in order to quantify how much water in a stream at any given time is storm flow vs. baseflow, we need to use some simplified methods. These frequently involve using the hydrograph (plot of streamflow over time) drawing lines, and calculating the volume of water above and below the line. This can be somewhat arbitrary and there are a variety of methods for delineating the cutoff between baseflow and stormflow. Despite what method you use and how simplified it is, this technique still provides valuable information and allows us to make comparisons across watersheds in order to understand how they function and what their structural properties are. 2.3.4 Baseflow separation methods One of the most basic methods for calculating base flow vs. storm flow is the straight line method. First, find the value of discharge at the point that streamflow begins to rise due to a storm. A straight line is drawn at that value until it intersects with the hydrograph (i.e. streamflow recedes back to the discharge it was at before the rainfall event started. Anything below this line is base flow and anything above it is storm flow. We learned above that some rainfall can move deep into the soil profile and contribute to baseflow. We might expect baseflow to increase over time and thus would want to use a method that can account for this. An addition to the straight line method was posed by Hewlett and Hibbert, 1967. This method, which we’ll call the Hewlett and Hibbert method finds the discharge at the starting point of a storm. Then, rather than a straight line of 0 slope and in the straight line method, we assume a slope of 0.05 cubic feet per second per square mile. The line with this calculated slope is drawn until it intersects with the hydrograph receding at the end of the storm. There are myriad other methods for baseflow separation of a wide range of complexity. We will give an example of one more method: a recursive filter method established by Lyne and Hollick (1976). This method smooths the hydrograph and partitions part of that smoothed signal into baseflow. The equation for this method is: You can see from this equation that a filter parameter, a, must be chosen. This parameter can be decided by the user, takes a value between 0 and 1, and is typically close to 1. Additionally this filtering method must be constrained so that baseflow is not negative or greater than the total streamflow. Output from this method for the Harvey River in Australia is originally published in Lyne and Hollick (1976) below (notice in the caption that the a parameter was set to 0.8): 2.3.5 Watershed controls on baseflow and stormflow The way a watershed is structured has a strong control on how water is partitioned into baseflow and stormflow. Below is a list of key structural properties: Land Use and Land Cover: If a watershed is developed or natural can dictate how much water infiltrates the land surface and how quickly. For example, a watershed with lots of pavement means that much more water will be overland flow with fewer opportunities to recharge baseflow. Furthermore, how a watershed is developed will affect partitioning. For example a residential area with houses on large, grassy lots will allow for more infiltration than a shopping center with large parking lots. Land cover in natural areas will also affect partitioning. Some other variables to consider may be: Land cover in natural areas: a dense forest vs. a recently harvested hillside. Soil type: clayey soils vs. sandy soils Depth to impeding layer: could be the bedrock interface, but could also be a low permeability clay layer in the soil Permeability of the underlying rock: Highly fractured sandstone vs. solid granite Slope: steeper slopes vs. flatter areas The partitioning is a combination of all of these factors. A watershed may have a very low slope, suggesting that it might have less stormflow. But if the soils in this watershed have an impermeable clay layer near the soil surface, a lot more water may end up as stormflow than one would expect. 2.4 Assessment To assess and improve your understanding we’ve developed an interactive application to explore runoff generation and sources. Check out the application and respond to and submit answers to the synthesis questions in a Word .doc. 2.5 Interactive application and questions This interactive web application highlighting how a major disturbance (mountaintop-mining) can change how water moves through a system. The web app is here: https://cuahsi.shinyapps.io/mtm_baseflow/ 2.5.1 Initial data exploration In the geomorphology tab, click on each of the four watersheds in the map on the left to view a 3-D rendering of its topography and read about its characteristics. Compare and contrast the watershed structure of the paired watersheds (both large and small). Describe the topography and the slope and soil characteristics. (1 pts) Click on the baseflow tab and make sure you have “Flow separation at each site” selected in the choose baseflow data display drop-down menu. Describe which line is baseflow and how you would estimate total storm flow for an event. (1 pt) Zoom into the storms that occurred between 3 Apr and 19 Apr. Compare the overall shape of the hydrographs. Which watersheds have higher peaks? How do the peaks change over successive storms? (2 pts) Move your cursor over the hydrographs and look at the values displayed in the top right corner of each graph. Determine if the reference or mined watersheds have higher baseflow. Describe how the baseflow changes over time in successive storms. (2 pts) 2.5.2 Synthesis Questions What do you think is happening in the hillslopes to cause the baseflow to change over time in successive storms? (3 pts) Describe how differences in watershed structure might contribute to the differences in baseflow between the mined and unmined watersheds. (4 pts) Look at the specific conductance tab and look at the mined vs. unmined. Describe the differences between the two. How do these data support some of your conclusions above? (4 pts) What would you expect a hydrograph for a watershed in Hawaii (Steep slopes, shallow soils) to look like? Would it have relatively high or low baseflow? What about a watershed in Southern Michigan (low slopes, deep soils)? (3 pts) "],["hydrograph-separation-lab-module-20-pts.html", "Chapter 3 Hydrograph separation: lab module (20 pts) 3.1 Intro 3.2 Reading for this lab 3.3 Repo", " Chapter 3 Hydrograph separation: lab module (20 pts) 3.1 Intro In this lab we will analyze stream flow (Q) and precipitation (P) data from Tenderfoot Creek Experimental Forest (TCEF). TCEF is located in central Montana, north of White Sulphur Springs. See here for information about TCEF. You will do some data analysis on flows, calculate annual runoff ratios, and perform a hydrograph separation. 3.2 Reading for this lab Ladson, A. R., R. Brown, B. Neal and R. Nathan (2013) A standard approach to baseflow separation using the Lyne and Hollick filter. Australian Journal of Water Resources 17(1): 173-18 Ladson et al., 2013 Lynne, V., Hollick, M. (1979) Stochastic time-variable rainfall-runoff modelling. In: pp. 89-93 Institute of Engineers Australia National Conference. Perth. Lyne and Hollick, 1979 3.3 Repo Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd” or “_partial.Rmd”. Always be sure to read the README.md files in the GitHub repo. Sometimes they are useful, sometimes they aren’t, but always have a look. As I mentioned above you will work through the “_blank.Rmd” or “_partial.Rmd”. However, there is also a “_complete.Rmd” in the repo. This has all the code. So you can use it as a cheat sheet, but if you want to learn how to code in R, I encourage you to work through the blank version as much as possible. Also, if you don’t have much R background this lab might seem kind of challenging. But don’t worry. I’m challenging you right now, but I’m going to post videos explaining how I would code this and walk you through everything. So don’t get frustrated if this seems tough right now. Soon you will be rattling off code with ease. Conversely, if you are an experienced coder and have ideas for how to do this in ways other than what I’ve shown here, please share code with your colleagues and help them develop their coding skills! OK. Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. The use of projects is highly recommended and is the practice we will follow in this class. See here for an overview of projects and why you should use them from Jenny Bryan. If you are new to R, or need a refresher, please read Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). In this unit we want to start familiarizing ourselves with R by visualizing some hydrological data. "],["learning-module-2---return-intervals-14-pts.html", "Chapter 4 Learning module 2 - Return intervals (14 pts) 4.1 Background information 4.2 Intro 4.3 Packages 4.4 Precipitation return intervals", " Chapter 4 Learning module 2 - Return intervals (14 pts) In this learning module we will focus on return intervals. We first need to understand return intervals before we can move onto the rational method and curve numbers. The repo for this module can be found here 4.1 Background information Lecture from colleague Joel Sholtes on precipitation frequency analysis. Short lecture on Intensity-Duration-Frequency (IDF) curves Reading on frequency analysis of flow (e.g., floods). You should notice that the frequency analysis is the same whether we apply it to Q (flow) or P (precipitation). So as long as you understand the fundamental principles you will be able to do frequency analysis on either Q or P. USGS reading on flow frequency analysis There are also probability lecture slides on D2L. Titled “probability.pptx”. 4.2 Intro In this lab we will look at some precipitation data to get an idea of return intervals for a given rain event. A return interval is the inverse of the probability. So if a certain rain even has a 10% probability of happening any year it has a 1/p return interval, so: R = 1/0.1 = 10 years. This means on average you can expect that size event about every ten years. From a probability perspective it is actually more correct to state that there is a 10% chance of that size rain event in any year. The reason this is better is that it communicates that you certainly can have a 10% probability event occur in back-to-back years. After computing some return intervals we will then use some of the simpler rainfall-runoff modeling approaches (the rational method and the curve number method) to simulate runoff for a hypothetical basin in our next unit. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 4.3 Packages We have a few new packages today. Those include rnoaa and leaflet. rnooa is a package used to download NOAA climate data. leaflet is a package for interactive mapping. Recall that if you have not installed these packages on your computer you will need to run: install.packages(“rnoaa”) and so on for the others. # library(tidyverse) # library(rnoaa) #this package is being deprecated, unfortunately. But should work for now. # library(plotly) # library(leaflet) # library(lubridate) 4.4 Precipitation return intervals First, let’s start by getting some precipitation (P) data. rnoaa has a function called ghcnd_stations(). This function will download the site information for all stations in the GHCND network. GHCND network - Link # # download all available station metadata # # THIS CAN TAKE A WHILE! I&#39;d suggest to run it only once and then comment it out with #. # station_data_download &lt;- ghcnd_stations() # # # filter to only keep MT stations # station_data &lt;- station_data_download %&gt;% # filter(state == &quot;BLANK&quot;) # # # that&#39;s still a lot of stations. Let thin for &quot;BOZEMAN&quot; # # station_data_bzn &lt;- station_data %&gt;% # filter(str_detect(name, &quot;BLANK&quot;)) %&gt;% # # get rid of duplicate ids to make plotting easier # dplyr::distinct(id, .keep_all = TRUE) # # # Now, let&#39;s create a zoomable map with the stations around Gallatin County/Bozeman. Click around and find the station id for the precip gauge with the LONGEST timeseries. If you click on a station symbol, the first and last data year will appear. Obviously you can get this information from the station_data_gal data frame, but I wanted to show you the mapping capabilities. # # gauge_map &lt;- leaflet() %&gt;% # # add a basemap # addTiles() %&gt;% # # add markers for your station. The parameters are pretty self-explanatory # addAwesomeMarkers(data = station_data_bzn, lat = ~latitude, lng = ~longitude, label = ~id, popup = ~paste(&quot;Start date:&quot;, first_year, &quot;&lt;br&gt;End date:&quot;, last_year)) # # gauge_map Ok, so we want station USC00241044, which runs from 1892 to now. We use the meteo_pull_monitors() function to do that. # climate_data &lt;- meteo_pull_monitors(c(&quot;BLANK&quot;), # # precip, snow, min air temp, max air temp (we really won&#39;t use the temp and snow data, though, this is only to show you what else would be available) # var = c(&quot;PRCP&quot;, &quot;SNOW&quot;, &quot;TMIN&quot;, &quot;TMAX&quot;), # date_min = &quot;1893-10-01&quot;, # # set end date to September 30, 2022 # date_max = &quot;2022-09-30&quot;) Now we have some climate data. Take a minute to look at the climate_data df. First, just looking at the df we see that the data don’t actually start until 1894. It is also always a good idea to just plot some data. Below plot prcp, snow, tmax and tmin. You can just make 4 different plots. This is just for visual inspection. This part of the process is called exploratory data analysis (EDA). This should always be the first step when downloading data whether you download the data from the internet or from a sensor in the field. # climate_data %&gt;% # ggplot(aes(x = date, y = prcp)) + # geom_point() You might have noticed that the values seem to be much too large! Did you notice that? This is a great skill to develop. Have a look at the data and ask “are these numbers reasonable?”. In this case, the answer would be no! One thing to note is that NOAA data comes in tenths of degrees for temp and tenths of millimeters for precip. Type ?meteo_pull_monitors into the console and the help screen will tell you that. So, we need to clean up the df a bit. Let’s do that here. # climate_data_corr &lt;- climate_data %&gt;% # mutate( # # change names # name = recode(id, USC00241044 # = &quot;msu_campus&quot;), # # division by 10 turns it into a normal decimal, 15.6 instead of 156 # tmin = tmin / 10, # tmax = tmax / 10, # prcp = prcp / 10, # snow = snow / 10) %&gt;% # # only take important stuff # select(name, id, everything()) Now that we’ve converted units, it is a good idea to plot your data again for some EDA. Make plots of each of the variables (prcp, snow, tmax, and tmin) over time to inspect. # ggplotly( # climate_data_corr %&gt;% # ggplot(aes(x = date, y = prcp)) + # geom_point() + # geom_line(linetype = &quot;dashed&quot;) # ) # I commented this out just because plotly is slow. # climate_data_corr %&gt;% # ggplot(aes(x = date, y = snow)) + # geom_point() How do the data look? Do they make sense? Do you see any strange values? There is a large snow event in 1951. We can assume that is “real”, so let’s keep it in the analysis. But you should think through how you could exclude it from the analysis. How could you use the filter function to do that? Next, we want to use some skills from the hydrograph sep lab to add a water year (WY) column. Try that here. # climate_data_corr &lt;- climate_data_corr %&gt;% # mutate(month = month(date), # year = year(date), # wy = BLANK(month &gt; 9, year + 1, year)) I like to rearrange the order of columns. Using: # climate_data_corr &lt;- climate_data_corr %&gt;% # select(name, id, date, wy, everything()) Now, create a new df called climate_an where you calculate the total P (i.e., the sum) for each water year. Use group_by and summarize (or better yet, reframe). Also keep in mind that you will need to deal with NA values in the df. How do you do that in summarize? As a note, reframe can be used instead of summarize and is a more general purpose function. You can try each. # climate_an &lt;- climate_data_corr %&gt;% # group_by(wy) %&gt;% # BLANK(tot_p = sum(prcp, BLANK), # mean_max = mean(tmax, BLANK), # mean_min = min(tmin, BLANK)) # I also calculate some temp stats here. Just out of curiosity. We don&#39;t use them in this lab. What happens if you don’t deal with NA values by using something like na.rm = TRUE? Now, plot total anual P on the Y and water year on the x. What do you see? # climate_an %&gt;% # ggplot(aes(x = wy, y = tot_p)) + # geom_point() + # geom_line(linetype = &quot;dashed&quot;, color = &quot;blue&quot;) Now let’s calculate some probabilities. Look up the pnorm() function for this (either type it into the Help window, or type ?pnorm in the console. You only need x, the mean, and standard deviation (sd) for the calculations. 4.4.1 Q1. (2 pts) What is the probability that the annual precipitation in a given year is less than 400 mm? This is the F(A) in the CDF in the probability lecture slides. # p_400 &lt;- pnorm(400, mean(climate_an$tot_p), sd(climate_an$tot_p)) # p_400 Q1 ANSWER: 4.4.2 Q2. (2 pts) What is the probability that the maximum annual precipitation in a given year is GREATER than 500 mm? # p_500 &lt;- pnorm(500, mean(climate_an$tot_p), sd(climate_an$tot_p)) # ex_500 &lt;- 1 - p_500 # ex_500 Q2 ANSWER: 4.4.3 Q3. (2 pts) What is the probability that the annual P is between 400 and 500 mm? # p_500_400 &lt;- p_500 - p_400 # # p_500_400 4.4.4 Q4. (2 pts) What is the return period for a year with AT LEAST 550 mm of precip? The return period, Tr, is calculated as Tr = 1/p, with p being the probability for an event to occur. # p_550 &lt;- pnorm(550, mean(climate_an$tot_p), sd(climate_an$tot_p)) # ex_550 &lt;- 1 - p_550 # # ri &lt;- 1/ex_550 # ri 4.4.5 Q5. (6 pts) Explain why probability analysis of climate data assumes the data are normally distributed and stationary? Below provide a histogram and a density plot of the total annual P data and comment on the visual appearance in terms of normality. Next use google and the links below to test for normality and stationarity. Be quantitative in commenting on the normality and stationarity of the total P data. here here here #shapiro.test(climate_an$tot_p) #lag.length = 25 #Box.test(climate_an$tot_p, lag = lag.length, type = &quot;Ljung-Box&quot;) # test stationary signal # climate_an %&gt;% # ggplot(aes(x = tot_p)) + # theme_bw() + # geom_histogram(binwidth = 20, aes(y = after_stat(density)), colour = &quot;black&quot;, fill = &quot;gray&quot;) + # histogram # geom_density(alpha = 0.2, fill = &quot;red&quot;) # density plot "],["rational-method-and-nrcs-curve-number-14-pts.html", "Chapter 5 Rational method and NRCS curve number (14 pts) 5.1 Rational Method 5.2 Curve Number 5.3 Codework", " Chapter 5 Rational method and NRCS curve number (14 pts) The repo for this module can be found here 5.1 Rational Method 5.1.1 Background information The Rational Method is a type of simple hydrological analysis used to estimate the peak runoff rate from a small watershed during a rainfall event. It is particularly useful for estimating the amount of water that will flow through a particular area during a storm, like a drainage system or culvert. Some helpful terminology: runoff coefficient - represents how much rainfall actually becomes runoff time of concentration - the time it takes for some mass of precipitation to travel from the most remote point in a watershed to the outlet or point of interest. e.g., how long it takes a drop of rain to reach a culvert after it falls to the ground. Here is a 5-minute video that describes the equation: 5.1.1.1 Reading - Rational Method. Read at least sections 2 and 3 Link 5.2 Curve Number 5.2.1 Background information The NRCS (Natural Resources Conservation Science) curve number (CN) is a tool used to estimate the total runoff volume of water that will run off an ungaged watershed during a storm event. The curve number is based on soil type, land use and antecedent moisture conditions. You may also see SCS CN in texts. NRCS was previously known as Soil Conservation Service, they are the same. It was designed as a simple tool to describe typical watershed response from infrequent rainfall anywhere in the US for watersheds with the same soil type, land use, and surface runoff conditions. The CN method is a single event model to estimate of runoff volume from rainfall events (not peak discharge or a hydrograph). To understand the function and derivation of the CN number, let’s start with the NRCS runoff equation: \\[ Q = \\frac{{(P - I_{a})^2}}{{P - I_{a} + S}} \\] Where Q = runoff(in) P = rainfall (in) S = potential maximum retention after runoff begins Ia = initial abstraction (initial amount of rainfall that is intercepted by the watershed surface and does not immediately contribute to runoff) Ia is assumed to reduce to 0.2S based on empirical observations by NRCS. If: \\[ S = \\frac{{1000}}{{CN}} - 10 \\] the runoff equation therefore reduces to: \\[ Q = \\frac{{[P - 0.02\\left(\\frac{{1000}}{{CN}} - 10\\right)]^2}}{{P + 0.8\\left(\\frac{{1000}}{{CN}} - 10\\right)}} \\] 5.2.1.1 Reading - Curve numbers Curve Number selection tables are available from the USDA. Slides on selecting curve number start around slide 8. - Link 5.2.1.2 Reading - Supporting material Time of concentration. Up to “other considerations”, pages 15-1 to 15-9. - Link We will use the Kirpich method to calculate the time of concentration. Here is the citation for your reference. Kirpich, Z.P. (1940). “Time of concentration of small agricultural watersheds”. Civil Engineering. 10 (6): 362. 5.3 Codework In this module, you will apply the Rational Method and the SCS curve number (CN) method to estimate peak flows and effective rainfall/runoff volumes. This lab also includes two new coding techniques, namely writing ‘for loops’ and writing functions. We will start with the Rational Method and functions. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 5.3.1 Packages 5.3.2 Part I - Rational Method The goal is to calculate peak runoff (cfs) for a small 280-acre rangeland watershed near Bozeman for multiple events with different return periods. You will first have to calculate the time of concentration and then look up the the rainfall values for the different return intervals. The longest flowpath in the watershed is 6300 ft long, average watershed slope is 1.95%. Look at this table to select C. 5.3.2.1 Time of Concentration Time of concentration is the time it takes water to travel along the longest flowpath in the watershed and exit the watershed. # A &lt;- BLANK # Area in acres # L &lt;- BLANK # Longest flowpatch length (ft) # S &lt;- 0.0195 # Slope (ft/ft) # # tc &lt;- 0.0078 * L^0.77 * S^-0.385 # Kirpich concentration time # tc 5.3.2.2 Storm Depths Now that you have the time of concentration, you need to find the corresponding 1, 2, 5, 10, 25, 50, and 100 year storm depths for a duration that works for the Rational Method in that particular watershed (that is, the duration that is closest to tc). Create a df called “storms” that has a column for the return period, Tr, the storm depth, Pin, and the average storm intensity over ONE HOUR, Pin_hr. Typically hourly depths may be determined from a rainfall analysis. For the sake of the assignment, approximate daily depths corresponding to appropriate frequencies are provided here for Bozeman, MT. To obtain 1 hour depths by dividing daily depth by 24. # storms &lt;- tibble( # Tr = c(BLANK), # column for return intervals # Pday = c(1.0, 1.18, 1.75, 2.10, 2.50, 2.81), # column for storm depth # Pin_hr = Pday / 24) # storm depth converted to an intensity of inches per hour 5.3.2.3 Example for-loop Now that we have the rainfall intensities, we need to set up a way that calculates Qp for each of those intensities, without us having to go back and manually enter them each time. We do this with a for-loop. Let’s first look at how for loops work in R. There is one thing we need to take care of first, though. We have to preallocate the vector y, that is, we are creating an empty vector of the desired length. # x &lt;- seq(0, 10, 2) # create vector from 0 to 10 in increments of 2 # y &lt;- vector(mode = &quot;double&quot;, length(x)) # preallocate y vector with length of x # for (i in 1:length(x)) { # loop through i # y[i] &lt;- x[i]^2 # calculation # } # y Please note that we wouldn’t need a for-loop for this operation, this is just for demonstration. We have created a vector x from 0 to 10 in increments of 2. The for-loop takes each element of that vector and squares it. The “i” is called an index and runs from 1 through 6 (the length of x). During the first iteration i is 1, during the second iteration i is 2, and so on. We are then writing the results from the calculation into a new vector, y. When i is 1, we are squaring the first value in vector x, which is 0. The first value in y will be zero as well. When i is 2, the second value in x gets squared, which is 2^2. The second value in y is going to be 4. 5.3.2.4 Calculate Qp with for-loop Let’s set up the for-loop for the Rational Method. We need to set up a for-loop that does the same calculation 7 times, the number of precip values in “storms” (remember that ‘length()’ for a dataframe returns the number of columns, so you want to use ‘nrow()’. The calculated peak discharges should go into a new column of our “storms” df. However, indexing is slow for dataframes, that is why we will write the new values into a new vector, Qp, and then after the loop insert it into the dataframe. If you are using the complete version of the assignment, do not assume that this C value is correct # # DEFINE C if you haven&#39;t already done so further up (uncomment the line below) # C &lt;- BLANK # runoff coefficient # # # preallocate vector (think about what the length needs to be and how to get it without just typing in the number) # Qp &lt;- vector(mode = &quot;double&quot;, nrow(storms)) # # for (i in 1:nrow(storms)) { # # the actual Rational Method calculation goes here. For the precip, you need to use the column in the storms df that has the one hour precip intensities (make sure to not forget the correct index symbol!) # Qp[i] &lt;- C * A * storms$Pin_hr[i] # } # # # # this adds the new peakflows to the existing &quot;storms&quot; df # storms &lt;- storms %&gt;% # mutate(Qp = Qp) # # # # FULL DISCLAIMER # # We could have accomplished the same with vectorization # # As a matter of fact, you should try to avoid for loops whenever possible. # Qp_vectorized &lt;- C*A*storms$Pin_hr 5.3.2.5 Plot Tr and Qp Now plot the return interval against the storm peakflow. You only need three to four lines for this: 1st sets up the data, 2nd defines the theme that removes the gray background and sets the axes labels to a proper size, 3rd plots the data with geom_points, 4th makes the axes labels with labs. # ggplot(storms, aes(x = Tr, y = Qp)) + # theme_bw(base_size = BLANK) + # geom_point(size = 4, color = BLANK) + # labs(x = &quot;Return Period (Years)&quot;, y = &quot;Peakflow (cfs)&quot;) 1. (1 pt) What does the C in the Rational Method do? ANSWER: 2. (2 pt) What is the the time of concentration and why does it need to be taken into account for the Rational Method? What is a common issue among many tc methods? ANSWER: 5.3.3 Part II - NRCS CN In this exercise we will write a function that takes the necessary inputs for the NRCS CN method and returns a value based on the parameters. This is not fully automated, you will still have to look up the CN yourself for a given land use. The goal is to write a function that requires P, CN, and AMC (antecedent moisture condition) as an input in order to calculate Q. 5.3.3.1 AMC Table 5.3.3.2 Function example Let’s look at a simple example for a function. Assume we want a number with an exponent and we want to be able to choose both the base and the exponent. The function ‘function()’ defines the inputs in (), the actual calculation then follows in {}. #multiply &lt;- function(base, exponent) { # new_number &lt;- base^exponent # return(new_number) #} Run the above function. You will notice that a function was added to the Global Environment all the way at the bottom. Now let’s test the function with a base of 2 and an exponent of 3. This should perform the calculation 2x2x2 = 8 #multiply(2, 3) 5.3.3.3 NRCS CN test Before we write a function, let’s make sure we can set up the correct steps WITHOUT the function. Let’s try it for P = 2.5 inches, CN = 90, and AMC = 3. The biggest problem here is going to be creating a lookup table for the AMC. I will provide the basic structure of the code for this. # P &lt;- 2.5 # 50-year, 24-hour rainfall (in.), from TP40 # CN &lt;- 90 # AMC &lt;- 3 # # ### Adjust for AMC # # use the equations from the lecture to adjust the CN to dry or wet conditions if necessary # # AMC I # if (AMC == 1) { # CN_adj &lt;- (4.2 * CN) / (10 + 0.13 * CN) # } # # # AMC II The curve number remains whatever the input is. # if (AMC == 2) { # CN_adj &lt;- CN # } # # # AMC III # if (AMC == 3) { # CN_adj &lt;- (23 * CN) / (10 + 0.13 * CN) # } # # # # calculate Si # Si &lt;- 1000 / CN_adj - 10 # Storage part # # # estimate initial abstraction # Ia &lt;- 0.2 * Si # # # Check equation from lecture # # calculate Q # Q &lt;- (P - 0.2 * Si)^2 / (P + BLANK * Si) # SCS CN runoff (inches) # Q 3. (2 pt) What is one of the critical assumptions for the SCS CN method? ANSWER: Now write a function called “scs_cn” that takes the inputs P, CN, and AMC (all numeric) and returns Q, Ia, Si, and the RR as a df called “scs_out”. Test the function for P = 2.5 in, CN = 90, AMC = 3. # scs_cn &lt;- function(P, CN, AMC) { # # ### Adjust for AMC # # AMC I # if (AMC == 1) { # CN_adj &lt;- BLANK # } # # # AMC II # if (AMC == 2) { # CN_adj &lt;- CN # } # # # AMC III # if (AMC == 3) { # CN_adj &lt;- (23 * CN) / (10 + 0.13 * CN) # } # # # Check equation from lecture # # calculate S # S &lt;- BLANK / CN_adj - 10 # Storage part # # # estimate initial abstraction # Ia &lt;- 0.2 * S # # Q &lt;- (P - 0.2 * S)^2 / (P + 0.8 * S) # SCS CN runoff (inches) # # # statement that spits out a warning if the initial abstraction is greater than the precipitation amount # if (Ia &gt; P) { # warning(&quot;Ia is greater than P. Resulting Q is 0.&quot;) # } # # scs_out &lt;- tibble(Si, Ia, Q, P, RR = Q / P) # return(scs_out) # } # # # # TEST THE FUNCTION with P = 2.5, CN = 90, and AMC = 3 # SCS_example &lt;- scs_cn(0.01, 90, 3) # SCS_example 4.(3 pts) Describe what factors into the curve number (and most notably what is missing). What does it mean when the CN is 100 or 0? ANSWER: General question: 5. (2 pts) What is the difference between a deterministic and a stochastic model? Describe one example each. ANSWER: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
