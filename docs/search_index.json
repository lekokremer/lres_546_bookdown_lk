[["index.html", "Quantitative Environmental Methods: LRES 546 Chapter 1 Introduction 1.1 Course overview and objectives 1.2 Course Description 1.3 Structure 1.4 Philosophical approach and resources 1.5 Tentative schedule, subject to change", " Quantitative Environmental Methods: LRES 546 Tim Covino &amp; Lauren Kremer Chapter 1 Introduction This book provides the materials that we will use in Quantitative Environmental Methods (LRES 546). In this class we will be learning the fundamentals of environmental data analysis and simulation in R. This class can be taken online or in-person. The online will be asynchronous. As a function of demand, in 2024, this class will be taught online. Instructor: Dr. Tim Covino Class times (depending on demand): T 13:40 – 14:55; Th 13:40 – 14:55 - For 2024 the class will be online, asynchronous. Office hours: By appointment Email: timothy.covino@montana.edu TA: Lauren Kremer Email: lauren.kremer@montana.edu 1.1 Course overview and objectives provide theoretical understanding and practical experience with common analysis and modeling techniques relevant to watershed hydrology/environmental science. provide training in analyzing, simulating, and presenting scientific data in written and oral formats. 1.2 Course Description This course will focus on development of quantitative analysis and modeling skills in watershed and environmental science. Students will develop skills necessary to perform quantitative analyses, describe and evaluate model structure, evaluate the merit of different models of varying type and complexity, and use quantitative analyses to address problems in environmental/watershed science. Students will apply computer programming in R to analyze and simulate watershed and/or environmental dynamics spanning simple to complex processes, analyses, and simulations. Technical skills and conceptual understanding will be built through lectures, readings, and hands-on quantitative projects. Note: If you aren’t familiar with R, or don’t have coding experience, don’t worry. We will walk you through all of the coding. I hope that by the end of this class each student will be a strong quantitative scientist with equally strong coding skills. 1.3 Structure This class will utilize hands-on/active learning, and students will be conducting watershed analyses and modeling exercises. We will be doing our analysis and modeling in R and will do some R coding in each class/work session. Programming is best learned by doing it often. Each week there will be recorded videos and/or readings, where we will talk about and work through various types of hydrological analyses. We will then put the content from the recorded lectures to work in a lab where students will complete a variety of hydrological analyses in R. Students can work through material on their schedule, but Dr. Covino and Lauren Kremer (TA) will be available during lab (Thursday 13:40 - 14:55) to help with technical coding problems or to answer other questions on weekly labs. 1.4 Philosophical approach and resources This course will use all online and open-source resources and will follow FAIR (findability, accessibility, interoperability, and reusability) data principles and promote sharing of hydrological education and research materials. Our computing will utilize open source R and RStudio software. Books/resources, we may not use all of these, but they are good references: - R for Data Science - Statistical Methods in Water Resources - Tidy modeling with R - ggplot2: Elegant Graphics for Data Analysis - Advanced R - R Packages - Environmental Data Science Additional readings will be made available on this bookdown page as needed. 1.5 Tentative schedule, subject to change Week 1: - Introduction, overview, and technical skills. - If you need a refresher for R please see Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). Week 2: - Hydrograph separation. Week 3: Week 4: - Frequency analysis Week 5: - Rational and curve number (CN) methods Week 6: - Transfer function lumped hydrological model Week 7: - Monte Carlo and sensitivity analysis Week 8: - Term project proposal Week 9: Spring Break Week 10: - Snowmelt models Week 11: - Gridded and flow data retrieval Week 12: - DEM processing and analysis in Whitebox Week 13: - Evapotranspiration (ET) models Week 14: - HBV model – lumped to semi-distributed modeling OR Precipitation interpolation Week 15: - Term project work Week 16: - Final presentations "],["hydrograph-separation-learning-module.html", "Chapter 2 Hydrograph separation: learning module 2.1 Summary 2.2 Overall Learning Objectives 2.3 Lecture 2.4 Assessment 2.5 Interactive application and questions", " Chapter 2 Hydrograph separation: learning module This module was developed by Ana Bergrstrom and Matt Ross. 2.1 Summary Streamflow can come from a range of water sources. When it is not raining, streams are fed by the slow drainage of groundwater. When a rainstorm occurs, streamflow increases and water enters the stream more quickly. The rate at which water reaches the stream and the partitioning between groundwater and faster flow pathways is variable across watersheds. It is important to understand how water is partitioned between fast and slow pathways (baseflow and stormflow) and what controls this partitioning in order to better predict susceptibility to flooding and if and how groundwater can sustain streamflow in long periods of drought. In this module we will introduce the components of streamflow during a rain event, and how event water moves through a hillslope to reach a stream. We will discuss methods for partitioning a hydrograph between baseflow (groundwater) and storm flow (event water). Finally, we will explore how characteristics of a watershed might lead to more or less water being partitioned into baseflow vs. stormflow. We will test understanding through evaluating data collected from watersheds in West Virginia to determine how mountaintop mining, which fundamentally changed the watershed structure, affects baseflow. Note: While this assessment is written for watersheds in West Virginia, we have designed the course so it is adaptable. Instructors should be able to easily substitute in their own data for the assessment. 2.2 Overall Learning Objectives At the end of this module, students should be able to describe the components of streamflow, the basics of how water moves through a hillslope, and the watershed characteristics that affect partitioning between baseflow and stormflow. 2.3 Lecture 2.3.1 Components of streamflow during a rain event During a rainstorm, precipitation is falling across the watershed: close to the stream, on a hillslope, and up at the watershed divide. This water that falls across the watershed flows downslope toward the stream via a number of flow pathways. Here we define and describe the basic flow pathways during a rain event. The first component is channel interception. This is water that falls directly on the water surface of the stream. The amount of water that falls directly on the channel is a function of stream size, if we have a very small, narrow creek, this will be a very small quantity. However, you can imagine that in a very large, broad river such as the Amazon, this volume of water is much larger. Channel interception is the first component during a rain event that causes streamflow to increase because it is contributing directly to the stream and therefore has no travel time. The second is overland flow, which is water that flows on the land surface to the stream. Overland flow can occur via a number of mechanisms which we will not explore too deeply here, but encourage further study on your own (resources provided). Briefly, overland flow includes water that falls on an impermeable surface such as pavement, water that runs downslope due to rain falling faster than the rate at which it can infiltrate the ground surface, and water that flows over the land surface because the ground is completely saturated. Overland flow is typically faster than water that travels through soils and deeper flow pathways and therefore is the next major component that starts to contribute to the increase in streamflow during a rain event. The third component is subsurface flow. This is water that infiltrates the land surface and flows downslope through shallow groundwater flow pathways. This is the last component that increases streamflow during a storm event, is the slowest of the stormflow components, and can contribute to elevated streamflow for a while after precipitation ends. The final component is baseflow. Baseflow can also be described as groundwater. This component is what sustains streamflow between rain events, but also continues to contribute during a rain event. Of water that infiltrates the ground surface, some moves quickly to the stream as subsurface flow, but some moves deeper and becomes part of deeper groundwater and baseflow. Thus baseflow can increase in times of higher wetness in the watershed, particularly during and right after rainy seasons or spring snowmelt. We can simplify this partitioning into baseflow and stormflow (often called quickflow). Baseflow being groundwater that moves more slowly and sustains streamflow between rain events. Stormflow is water that contributes to streamflow as a result of a rain event. Under this definition we can lump channel interception, overland flow, and subsurface flow into stormflow. 2.3.2 Storm flow through a hillslope When rain falls on the land surface, much of it infiltrates into the soil. Water moves through the soil column until it meets a layer of lower permeability and runs down the hillslope as subsurface flow. This layer of lower permeability allows some water to move through it, contributing to groundwater. Frequently the layer of lower permeability is the interface between soil and rock. Therefore the depth of soil has a large effect on how much water moves through soil vs. how much moves deeper into groundwater, becoming baseflow. 2.3.3 How we quantify baseflow It is impossible to know the amount of water moving as overland, subsurface, and base flow in all parts of a watershed. So in order to quantify how much water in a stream at any given time is storm flow vs. baseflow, we need to use some simplified methods. These frequently involve using the hydrograph (plot of streamflow over time) drawing lines, and calculating the volume of water above and below the line. This can be somewhat arbitrary and there are a variety of methods for delineating the cutoff between baseflow and stormflow. Despite what method you use and how simplified it is, this technique still provides valuable information and allows us to make comparisons across watersheds in order to understand how they function and what their structural properties are. 2.3.4 Baseflow separation methods One of the most basic methods for calculating base flow vs. storm flow is the straight line method. First, find the value of discharge at the point that streamflow begins to rise due to a storm. A straight line is drawn at that value until it intersects with the hydrograph (i.e. streamflow recedes back to the discharge it was at before the rainfall event started. Anything below this line is base flow and anything above it is storm flow. We learned above that some rainfall can move deep into the soil profile and contribute to baseflow. We might expect baseflow to increase over time and thus would want to use a method that can account for this. An addition to the straight line method was posed by Hewlett and Hibbert, 1967. This method, which we’ll call the Hewlett and Hibbert method finds the discharge at the starting point of a storm. Then, rather than a straight line of 0 slope and in the straight line method, we assume a slope of 0.05 cubic feet per second per square mile. The line with this calculated slope is drawn until it intersects with the hydrograph receding at the end of the storm. There are myriad other methods for baseflow separation of a wide range of complexity. We will give an example of one more method: a recursive filter method established by Lyne and Hollick (1976). This method smooths the hydrograph and partitions part of that smoothed signal into baseflow. The equation for this method is: You can see from this equation that a filter parameter, a, must be chosen. This parameter can be decided by the user, takes a value between 0 and 1, and is typically close to 1. Additionally this filtering method must be constrained so that baseflow is not negative or greater than the total streamflow. Output from this method for the Harvey River in Australia is originally published in Lyne and Hollick (1976) below (notice in the caption that the a parameter was set to 0.8): 2.3.5 Watershed controls on baseflow and stormflow The way a watershed is structured has a strong control on how water is partitioned into baseflow and stormflow. Below is a list of key structural properties: Land Use and Land Cover: If a watershed is developed or natural can dictate how much water infiltrates the land surface and how quickly. For example, a watershed with lots of pavement means that much more water will be overland flow with fewer opportunities to recharge baseflow. Furthermore, how a watershed is developed will affect partitioning. For example a residential area with houses on large, grassy lots will allow for more infiltration than a shopping center with large parking lots. Land cover in natural areas will also affect partitioning. Some other variables to consider may be: Land cover in natural areas: a dense forest vs. a recently harvested hillside. Soil type: clayey soils vs. sandy soils Depth to impeding layer: could be the bedrock interface, but could also be a low permeability clay layer in the soil Permeability of the underlying rock: Highly fractured sandstone vs. solid granite Slope: steeper slopes vs. flatter areas The partitioning is a combination of all of these factors. A watershed may have a very low slope, suggesting that it might have less stormflow. But if the soils in this watershed have an impermeable clay layer near the soil surface, a lot more water may end up as stormflow than one would expect. 2.4 Assessment To assess and improve your understanding we’ve developed an interactive application to explore runoff generation and sources. Check out the application and respond to and submit answers to the synthesis questions in a Word .doc. 2.5 Interactive application and questions This interactive web application highlighting how a major disturbance (mountaintop-mining) can change how water moves through a system. The web app is here: https://cuahsi.shinyapps.io/mtm_baseflow/ 2.5.1 Initial data exploration In the geomorphology tab, click on each of the four watersheds in the map on the left to view a 3-D rendering of its topography and read about its characteristics. Compare and contrast the watershed structure of the paired watersheds (both large and small). Describe the topography and the slope and soil characteristics. (1 pts) Click on the baseflow tab and make sure you have “Flow separation at each site” selected in the choose baseflow data display drop-down menu. Describe which line is baseflow and how you would estimate total storm flow for an event. (1 pt) Zoom into the storms that occurred between 3 Apr and 19 Apr. Compare the overall shape of the hydrographs. Which watersheds have higher peaks? How do the peaks change over successive storms? (2 pts) Move your cursor over the hydrographs and look at the values displayed in the top right corner of each graph. Determine if the reference or mined watersheds have higher baseflow. Describe how the baseflow changes over time in successive storms. (2 pts) 2.5.2 Synthesis Questions What do you think is happening in the hillslopes to cause the baseflow to change over time in successive storms? (3 pts) Describe how differences in watershed structure might contribute to the differences in baseflow between the mined and unmined watersheds. (4 pts) Look at the specific conductance tab and look at the mined vs. unmined. Describe the differences between the two. How do these data support some of your conclusions above? (4 pts) What would you expect a hydrograph for a watershed in Hawaii (Steep slopes, shallow soils) to look like? Would it have relatively high or low baseflow? What about a watershed in Southern Michigan (low slopes, deep soils)? (3 pts) "],["hydrograph-separation-lab-module-20-pts.html", "Chapter 3 Hydrograph separation: lab module (20 pts) 3.1 Intro 3.2 Reading for this lab 3.3 Repo", " Chapter 3 Hydrograph separation: lab module (20 pts) 3.1 Intro In this lab we will analyze stream flow (Q) and precipitation (P) data from Tenderfoot Creek Experimental Forest (TCEF). TCEF is located in central Montana, north of White Sulphur Springs. See here for information about TCEF. You will do some data analysis on flows, calculate annual runoff ratios, and perform a hydrograph separation. 3.2 Reading for this lab Ladson, A. R., R. Brown, B. Neal and R. Nathan (2013) A standard approach to baseflow separation using the Lyne and Hollick filter. Australian Journal of Water Resources 17(1): 173-18 Ladson et al., 2013 Lynne, V., Hollick, M. (1979) Stochastic time-variable rainfall-runoff modelling. In: pp. 89-93 Institute of Engineers Australia National Conference. Perth. Lyne and Hollick, 1979 3.3 Repo Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd” or “_partial.Rmd”. Always be sure to read the README.md files in the GitHub repo. Sometimes they are useful, sometimes they aren’t, but always have a look. As I mentioned above you will work through the “_blank.Rmd” or “_partial.Rmd”. However, there is also a “_complete.Rmd” in the repo. This has all the code. So you can use it as a cheat sheet, but if you want to learn how to code in R, I encourage you to work through the blank version as much as possible. Also, if you don’t have much R background this lab might seem kind of challenging. But don’t worry. I’m challenging you right now, but I’m going to post videos explaining how I would code this and walk you through everything. So don’t get frustrated if this seems tough right now. Soon you will be rattling off code with ease. Conversely, if you are an experienced coder and have ideas for how to do this in ways other than what I’ve shown here, please share code with your colleagues and help them develop their coding skills! OK. Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. The use of projects is highly recommended and is the practice we will follow in this class. See here for an overview of projects and why you should use them from Jenny Bryan. If you are new to R, or need a refresher, please read Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). In this unit we want to start familiarizing ourselves with R by visualizing some hydrological data. "],["learning-module-2---return-intervals-14-pts.html", "Chapter 4 Learning module 2 - Return intervals (14 pts) 4.1 Background information 4.2 Intro 4.3 Packages 4.4 Precipitation return intervals", " Chapter 4 Learning module 2 - Return intervals (14 pts) In this learning module we will focus on return intervals. We first need to understand return intervals before we can move onto the rational method and curve numbers. The repo for this module can be found here 4.1 Background information Lecture from colleague Joel Sholtes on precipitation frequency analysis. Short lecture on Intensity-Duration-Frequency (IDF) curves Reading on frequency analysis of flow (e.g., floods). You should notice that the frequency analysis is the same whether we apply it to Q (flow) or P (precipitation). So as long as you understand the fundamental principles you will be able to do frequency analysis on either Q or P. USGS reading on flow frequency analysis There are also probability lecture slides on D2L. Titled “probability.pptx”. 4.2 Intro In this lab we will look at some precipitation data to get an idea of return intervals for a given rain event. A return interval is the inverse of the probability. So if a certain rain even has a 10% probability of happening any year it has a 1/p return interval, so: R = 1/0.1 = 10 years. This means on average you can expect that size event about every ten years. From a probability perspective it is actually more correct to state that there is a 10% chance of that size rain event in any year. The reason this is better is that it communicates that you certainly can have a 10% probability event occur in back-to-back years. After computing some return intervals we will then use some of the simpler rainfall-runoff modeling approaches (the rational method and the curve number method) to simulate runoff for a hypothetical basin in our next unit. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 4.3 Packages We have a few new packages today. Those include rnoaa and leaflet. rnooa is a package used to download NOAA climate data. leaflet is a package for interactive mapping. Recall that if you have not installed these packages on your computer you will need to run: install.packages(“rnoaa”) and so on for the others. # library(tidyverse) # library(rnoaa) #this package is being deprecated, unfortunately. But should work for now. # library(plotly) # library(leaflet) # library(lubridate) 4.4 Precipitation return intervals First, let’s start by getting some precipitation (P) data. rnoaa has a function called ghcnd_stations(). This function will download the site information for all stations in the GHCND network. GHCND network - Link # # download all available station metadata # # THIS CAN TAKE A WHILE! I&#39;d suggest to run it only once and then comment it out with #. # station_data_download &lt;- ghcnd_stations() # # # filter to only keep MT stations # station_data &lt;- station_data_download %&gt;% # filter(state == &quot;BLANK&quot;) # # # that&#39;s still a lot of stations. Let thin for &quot;BOZEMAN&quot; # # station_data_bzn &lt;- station_data %&gt;% # filter(str_detect(name, &quot;BLANK&quot;)) %&gt;% # # get rid of duplicate ids to make plotting easier # dplyr::distinct(id, .keep_all = TRUE) # # # Now, let&#39;s create a zoomable map with the stations around Gallatin County/Bozeman. Click around and find the station id for the precip gauge with the LONGEST timeseries. If you click on a station symbol, the first and last data year will appear. Obviously you can get this information from the station_data_gal data frame, but I wanted to show you the mapping capabilities. # # gauge_map &lt;- leaflet() %&gt;% # # add a basemap # addTiles() %&gt;% # # add markers for your station. The parameters are pretty self-explanatory # addAwesomeMarkers(data = station_data_bzn, lat = ~latitude, lng = ~longitude, label = ~id, popup = ~paste(&quot;Start date:&quot;, first_year, &quot;&lt;br&gt;End date:&quot;, last_year)) # # gauge_map Ok, so we want station USC00241044, which runs from 1892 to now. We use the meteo_pull_monitors() function to do that. # climate_data &lt;- meteo_pull_monitors(c(&quot;BLANK&quot;), # # precip, snow, min air temp, max air temp (we really won&#39;t use the temp and snow data, though, this is only to show you what else would be available) # var = c(&quot;PRCP&quot;, &quot;SNOW&quot;, &quot;TMIN&quot;, &quot;TMAX&quot;), # date_min = &quot;1893-10-01&quot;, # # set end date to September 30, 2022 # date_max = &quot;2022-09-30&quot;) Now we have some climate data. Take a minute to look at the climate_data df. First, just looking at the df we see that the data don’t actually start until 1894. It is also always a good idea to just plot some data. Below plot prcp, snow, tmax and tmin. You can just make 4 different plots. This is just for visual inspection. This part of the process is called exploratory data analysis (EDA). This should always be the first step when downloading data whether you download the data from the internet or from a sensor in the field. # climate_data %&gt;% # ggplot(aes(x = date, y = prcp)) + # geom_point() You might have noticed that the values seem to be much too large! Did you notice that? This is a great skill to develop. Have a look at the data and ask “are these numbers reasonable?”. In this case, the answer would be no! One thing to note is that NOAA data comes in tenths of degrees for temp and tenths of millimeters for precip. Type ?meteo_pull_monitors into the console and the help screen will tell you that. So, we need to clean up the df a bit. Let’s do that here. # climate_data_corr &lt;- climate_data %&gt;% # mutate( # # change names # name = recode(id, USC00241044 # = &quot;msu_campus&quot;), # # division by 10 turns it into a normal decimal, 15.6 instead of 156 # tmin = tmin / 10, # tmax = tmax / 10, # prcp = prcp / 10, # snow = snow / 10) %&gt;% # # only take important stuff # select(name, id, everything()) Now that we’ve converted units, it is a good idea to plot your data again for some EDA. Make plots of each of the variables (prcp, snow, tmax, and tmin) over time to inspect. # ggplotly( # climate_data_corr %&gt;% # ggplot(aes(x = date, y = prcp)) + # geom_point() + # geom_line(linetype = &quot;dashed&quot;) # ) # I commented this out just because plotly is slow. # climate_data_corr %&gt;% # ggplot(aes(x = date, y = snow)) + # geom_point() How do the data look? Do they make sense? Do you see any strange values? There is a large snow event in 1951. We can assume that is “real”, so let’s keep it in the analysis. But you should think through how you could exclude it from the analysis. How could you use the filter function to do that? Next, we want to use some skills from the hydrograph sep lab to add a water year (WY) column. Try that here. # climate_data_corr &lt;- climate_data_corr %&gt;% # mutate(month = month(date), # year = year(date), # wy = BLANK(month &gt; 9, year + 1, year)) I like to rearrange the order of columns. Using: # climate_data_corr &lt;- climate_data_corr %&gt;% # select(name, id, date, wy, everything()) Now, create a new df called climate_an where you calculate the total P (i.e., the sum) for each water year. Use group_by and summarize (or better yet, reframe). Also keep in mind that you will need to deal with NA values in the df. How do you do that in summarize? As a note, reframe can be used instead of summarize and is a more general purpose function. You can try each. # climate_an &lt;- climate_data_corr %&gt;% # group_by(wy) %&gt;% # BLANK(tot_p = sum(prcp, BLANK), # mean_max = mean(tmax, BLANK), # mean_min = min(tmin, BLANK)) # I also calculate some temp stats here. Just out of curiosity. We don&#39;t use them in this lab. What happens if you don’t deal with NA values by using something like na.rm = TRUE? Now, plot total anual P on the Y and water year on the x. What do you see? # climate_an %&gt;% # ggplot(aes(x = wy, y = tot_p)) + # geom_point() + # geom_line(linetype = &quot;dashed&quot;, color = &quot;blue&quot;) Now let’s calculate some probabilities. Look up the pnorm() function for this (either type it into the Help window, or type ?pnorm in the console. You only need x, the mean, and standard deviation (sd) for the calculations. 4.4.1 Q1. (2 pts) What is the probability that the annual precipitation in a given year is less than 400 mm? This is the F(A) in the CDF in the probability lecture slides. # p_400 &lt;- pnorm(400, mean(climate_an$tot_p), sd(climate_an$tot_p)) # p_400 Q1 ANSWER: 4.4.2 Q2. (2 pts) What is the probability that the maximum annual precipitation in a given year is GREATER than 500 mm? # p_500 &lt;- pnorm(500, mean(climate_an$tot_p), sd(climate_an$tot_p)) # ex_500 &lt;- 1 - p_500 # ex_500 Q2 ANSWER: 4.4.3 Q3. (2 pts) What is the probability that the annual P is between 400 and 500 mm? # p_500_400 &lt;- p_500 - p_400 # # p_500_400 4.4.4 Q4. (2 pts) What is the return period for a year with AT LEAST 550 mm of precip? The return period, Tr, is calculated as Tr = 1/p, with p being the probability for an event to occur. # p_550 &lt;- pnorm(550, mean(climate_an$tot_p), sd(climate_an$tot_p)) # ex_550 &lt;- 1 - p_550 # # ri &lt;- 1/ex_550 # ri 4.4.5 Q5. (6 pts) Explain why probability analysis of climate data assumes the data are normally distributed and stationary? Below provide a histogram and a density plot of the total annual P data and comment on the visual appearance in terms of normality. Next use google and the links below to test for normality and stationarity. Be quantitative in commenting on the normality and stationarity of the total P data. here here here #shapiro.test(climate_an$tot_p) #lag.length = 25 #Box.test(climate_an$tot_p, lag = lag.length, type = &quot;Ljung-Box&quot;) # test stationary signal # climate_an %&gt;% # ggplot(aes(x = tot_p)) + # theme_bw() + # geom_histogram(binwidth = 20, aes(y = after_stat(density)), colour = &quot;black&quot;, fill = &quot;gray&quot;) + # histogram # geom_density(alpha = 0.2, fill = &quot;red&quot;) # density plot "],["rational-method-and-nrcs-curve-number-20-pts.html", "Chapter 5 Rational method and NRCS curve number (20 pts) 5.1 Rational Method 5.2 Curve Number 5.3 Codework", " Chapter 5 Rational method and NRCS curve number (20 pts) The repo for this module can be found here 5.1 Rational Method 5.1.1 Background information The Rational Method is a type of simple hydrological analysis used to estimate the peak runoff rate from a small watershed during a rainfall event. It is particularly useful for estimating the amount of water that will flow through a particular area during a storm, like a drainage system or culvert. Some helpful terminology: runoff coefficient - represents how much rainfall actually becomes runoff time of concentration - the time it takes for some mass of precipitation to travel from the most remote point in a watershed to the outlet or point of interest. e.g., how long it takes a drop of rain to reach a culvert after it falls to the ground. Here is a 5-minute video that describes the equation: 5.1.1.1 Reading - Rational Method. Read at least sections 2 and 3 Link 5.2 Curve Number 5.2.1 Background information The NRCS (Natural Resources Conservation Science) curve number (CN) is a tool used to estimate the total runoff volume of water that will run off an ungaged watershed during a storm event. The curve number is based on soil type, land use and antecedent moisture conditions. You may also see SCS CN in texts. NRCS was previously known as Soil Conservation Service, they are the same. It was designed as a simple tool to describe typical watershed response from infrequent rainfall anywhere in the US for watersheds with the same soil type, land use, and surface runoff conditions. The CN method is a single event model to estimate of runoff volume from rainfall events (not peak discharge or a hydrograph). To understand the function and derivation of the CN number, let’s start with the NRCS runoff equation: \\[ Q = \\frac{{(P - I_{a})^2}}{{P - I_{a} + S}} \\] Where Q = runoff(in) P = rainfall (in) S = potential maximum retention after runoff begins Ia = initial abstraction (initial amount of rainfall that is intercepted by the watershed surface and does not immediately contribute to runoff) Ia is assumed to reduce to 0.2S based on empirical observations by NRCS. If: \\[ S = \\frac{{1000}}{{CN}} - 10 \\] the runoff equation therefore reduces to: \\[ Q = \\frac{{[P - 0.02\\left(\\frac{{1000}}{{CN}} - 10\\right)]^2}}{{P + 0.8\\left(\\frac{{1000}}{{CN}} - 10\\right)}} \\] 5.2.1.1 Reading - Curve numbers Curve Number selection tables are available from the USDA. Slides on selecting curve number start around slide 8. - Link 5.2.1.2 Reading - Supporting material Time of concentration. Up to “other considerations”, pages 15-1 to 15-9. - Link We will use the Kirpich method to calculate the time of concentration. Here is the citation for your reference. Kirpich, Z.P. (1940). “Time of concentration of small agricultural watersheds”. Civil Engineering. 10 (6): 362. 5.3 Codework In this module, you will apply the Rational Method and the SCS curve number (CN) method to estimate peak flows and effective rainfall/runoff volumes. This lab also includes two new coding techniques, namely writing ‘for loops’ and writing functions. We will start with the Rational Method and functions. This is knitr settings. Knitr is a package that will turn this Rmd to an html. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = FALSE) 5.3.1 Packages 5.3.2 Part I - Rational Method The goal is to calculate peak runoff (cfs) for a small 280-acre rangeland watershed near Bozeman for multiple events with different return periods. You will first have to calculate the time of concentration and then look up the the rainfall values for the different return intervals. The longest flowpath in the watershed is 6300 ft long, average watershed slope is 1.95%. Look at this table to select C. 5.3.2.1 Time of Concentration Time of concentration is the time it takes water to travel along the longest flowpath in the watershed and exit the watershed. # A &lt;- BLANK # Area in acres # L &lt;- BLANK # Longest flowpatch length (ft) # S &lt;- 0.0195 # Slope (ft/ft) # # tc &lt;- 0.0078 * L^0.77 * S^-0.385 # Kirpich concentration time # tc 5.3.2.2 Storm Depths Now that you have the time of concentration, you need to find the corresponding 1, 2, 5, 10, 25, 50, and 100 year storm depths for a duration that works for the Rational Method in that particular watershed (that is, the duration that is closest to tc). Create a df called “storms” that has a column for the return period, Tr, the storm depth, Pin, and the average storm intensity over ONE HOUR, Pin_hr. Typically hourly depths may be determined from a rainfall analysis. For the sake of the assignment, approximate daily depths corresponding to appropriate frequencies are provided here for Bozeman, MT. To obtain 1 hour depths by dividing daily depth by 24. # storms &lt;- tibble( # Tr = c(BLANK), # column for return intervals # Pday = c(1.0, 1.18, 1.75, 2.10, 2.50, 2.81), # column for storm depth # Pin_hr = Pday / 24) # storm depth converted to an intensity of inches per hour 5.3.2.3 Example for-loop Now that we have the rainfall intensities, we need to set up a way that calculates Qp for each of those intensities, without us having to go back and manually enter them each time. We do this with a for-loop. Let’s first look at how for loops work in R. There is one thing we need to take care of first, though. We have to preallocate the vector y, that is, we are creating an empty vector of the desired length. # x &lt;- seq(0, 10, 2) # create vector from 0 to 10 in increments of 2 # y &lt;- vector(mode = &quot;double&quot;, length(x)) # preallocate y vector with length of x # for (i in 1:length(x)) { # loop through i # y[i] &lt;- x[i]^2 # calculation # } # y Please note that we wouldn’t need a for-loop for this operation, this is just for demonstration. We have created a vector x from 0 to 10 in increments of 2. The for-loop takes each element of that vector and squares it. The “i” is called an index and runs from 1 through 6 (the length of x). During the first iteration i is 1, during the second iteration i is 2, and so on. We are then writing the results from the calculation into a new vector, y. When i is 1, we are squaring the first value in vector x, which is 0. The first value in y will be zero as well. When i is 2, the second value in x gets squared, which is 2^2. The second value in y is going to be 4. 5.3.2.4 Calculate Qp with for-loop Let’s set up the for-loop for the Rational Method. We need to set up a for-loop that does the same calculation 7 times, the number of precip values in “storms” (remember that ‘length()’ for a dataframe returns the number of columns, so you want to use ‘nrow()’. The calculated peak discharges should go into a new column of our “storms” df. However, indexing is slow for dataframes, that is why we will write the new values into a new vector, Qp, and then after the loop insert it into the dataframe. If you are using the complete version of the assignment, do not assume that this C value is correct # # DEFINE C if you haven&#39;t already done so further up (uncomment the line below) # C &lt;- BLANK # runoff coefficient # # # preallocate vector (think about what the length needs to be and how to get it without just typing in the number) # Qp &lt;- vector(mode = &quot;double&quot;, nrow(storms)) # # for (i in 1:nrow(storms)) { # # the actual Rational Method calculation goes here. For the precip, you need to use the column in the storms df that has the one hour precip intensities (make sure to not forget the correct index symbol!) # Qp[i] &lt;- C * A * storms$Pin_hr[i] # } # # # # this adds the new peakflows to the existing &quot;storms&quot; df # storms &lt;- storms %&gt;% # mutate(Qp = Qp) # # # # FULL DISCLAIMER # # We could have accomplished the same with vectorization # # As a matter of fact, you should try to avoid for loops whenever possible. # Qp_vectorized &lt;- C*A*storms$Pin_hr 5.3.2.5 Plot Tr and Qp Now plot the return interval against the storm peakflow. You only need three to four lines for this: 1st sets up the data, 2nd defines the theme that removes the gray background and sets the axes labels to a proper size, 3rd plots the data with geom_points, 4th makes the axes labels with labs. # ggplot(storms, aes(x = Tr, y = Qp)) + # theme_bw(base_size = BLANK) + # geom_point(size = 4, color = BLANK) + # labs(x = &quot;Return Period (Years)&quot;, y = &quot;Peakflow (cfs)&quot;) 1. (2 pt) What does the C in the Rational Method do? ANSWER: 2. (4 pt) What is the the time of concentration and why does it need to be taken into account for the Rational Method? What is a common issue among many tc methods? ANSWER: 5.3.3 Part II - NRCS CN In this exercise we will write a function that takes the necessary inputs for the NRCS CN method and returns a value based on the parameters. This is not fully automated, you will still have to look up the CN yourself for a given land use. The goal is to write a function that requires P, CN, and AMC (antecedent moisture condition) as an input in order to calculate Q. 5.3.3.1 AMC Table 5.3.3.2 Function example Let’s look at a simple example for a function. Assume we want a number with an exponent and we want to be able to choose both the base and the exponent. The function ‘function()’ defines the inputs in (), the actual calculation then follows in {}. #multiply &lt;- function(base, exponent) { # new_number &lt;- base^exponent # return(new_number) #} Run the above function. You will notice that a function was added to the Global Environment all the way at the bottom. Now let’s test the function with a base of 2 and an exponent of 3. This should perform the calculation 2x2x2 = 8 #multiply(2, 3) 5.3.3.3 NRCS CN test Before we write a function, let’s make sure we can set up the correct steps WITHOUT the function. Let’s try it for P = 2.5 inches, CN = 90, and AMC = 3. The biggest problem here is going to be creating a lookup table for the AMC. I will provide the basic structure of the code for this. # P &lt;- 2.5 # 50-year, 24-hour rainfall (in.), from TP40 # CN &lt;- 90 # AMC &lt;- 3 # # ### Adjust for AMC # # use the equations from the lecture to adjust the CN to dry or wet conditions if necessary # # AMC I # if (AMC == 1) { # CN_adj &lt;- (4.2 * CN) / (10 + 0.13 * CN) # } # # # AMC II The curve number remains whatever the input is. # if (AMC == 2) { # CN_adj &lt;- CN # } # # # AMC III # if (AMC == 3) { # CN_adj &lt;- (23 * CN) / (10 + 0.13 * CN) # } # # # # calculate Si # Si &lt;- 1000 / CN_adj - 10 # Storage part # # # estimate initial abstraction # Ia &lt;- 0.2 * Si # # # Check equation from lecture # # calculate Q # Q &lt;- (P - 0.2 * Si)^2 / (P + BLANK * Si) # SCS CN runoff (inches) # Q 3. (4 pt) What is one of the critical assumptions for the SCS CN method? ANSWER: Now write a function called “scs_cn” that takes the inputs P, CN, and AMC (all numeric) and returns Q, Ia, Si, and the RR as a df called “scs_out”. Test the function for P = 2.5 in, CN = 90, AMC = 3. # scs_cn &lt;- function(P, CN, AMC) { # # ### Adjust for AMC # # AMC I # if (AMC == 1) { # CN_adj &lt;- BLANK # } # # # AMC II # if (AMC == 2) { # CN_adj &lt;- CN # } # # # AMC III # if (AMC == 3) { # CN_adj &lt;- (23 * CN) / (10 + 0.13 * CN) # } # # # Check equation from lecture # # calculate S # S &lt;- BLANK / CN_adj - 10 # Storage part # # # estimate initial abstraction # Ia &lt;- 0.2 * S # # Q &lt;- (P - 0.2 * S)^2 / (P + 0.8 * S) # SCS CN runoff (inches) # # # statement that spits out a warning if the initial abstraction is greater than the precipitation amount # if (Ia &gt; P) { # warning(&quot;Ia is greater than P. Resulting Q is 0.&quot;) # } # # scs_out &lt;- tibble(Si, Ia, Q, P, RR = Q / P) # return(scs_out) # } # # # # TEST THE FUNCTION with P = 2.5, CN = 90, and AMC = 3 # SCS_example &lt;- scs_cn(0.01, 90, 3) # SCS_example 4.(6 pts) Describe what factors into the curve number (and most notably what is missing). What does it mean when the CN is 100 or 0? ANSWER: General question: 5. (4 pts) What is the difference between a deterministic and a stochastic model? Describe one example each. ANSWER: "],["transfer-function-rainfall-runoff-models.html", "Chapter 6 Transfer function rainfall-runoff models 6.1 Summary 6.2 Overall Learning Objectives 6.3 Terminology 6.4 The Linear Time-Invariant TF 6.5 Codework - Transfer function rainfall-runoff model", " Chapter 6 Transfer function rainfall-runoff models 6.1 Summary In previous modules, we explored how watershed characteristics influence the flow of input water through or over hillslopes to quickly contribute to stormflow or to be stored for later contribution to baseflow. Therefore, the partitioning of flow into baseflow or stormflow can be determined by the time it spends in the watershed. Furthermore, the residence time of water in various pathways may affect weathering and solute transport within watersheds. To improve our understanding of water movement within a watershed, it can be crucial to incorporate water transit time into hydrological models. This consideration allows for a more realistic representation of how water moves through various storage compartments, such as soil, groundwater, and surface water, accounting for the time it takes for water to traverse these pathways. In this module, we will model the temporal aspects of runoff response to input using a transfer function. First, please read: TRANSEP - a combined tracer and runoff transfer function hydrograph separation model Then this chapter will step through key concepts in the paper to facilitate hands-on exploration of the rainfall-runoff portion of the TRANSEP model in the assessment. Then, we will introduce examples of other transfer functions to demonstrate alternative ways of representing time-induced patterns in hydrological modeling, prompting you to consider response patterns in your study systems. 6.2 Overall Learning Objectives At the end of this module, students should be able to describe several ways to model and identify transit time within hydrological models. They should have a general understanding of how water transit time may influence the timing and composition of runoff. 6.3 Terminology In modeling a flow system, note that consideration of time may vary depending on the questions being asked. Transit time is the average time required for water to travel through the entire flow system, from input (e.g., rainfall on soil surface) to output (e.g., discharge). Residence time is a portion of transit time, describing the amount of time water spends within a specific component of the flow system, like storage (e.g., in soil, groundwater, or a lake). Figure 6.3. Conceptual diagram of the lumped parameter transit time modeling approach (McGuire &amp; McDonnell, 2006) A transfer function (TF) is a mathematical representation of how a system responds to input signals. In a hydrological context, it describes the transformation of inputs (e.g. precipitation) to outputs (e.g. runoff). These models can be valuable tools for understanding the time-varying dynamics of a hydrological system. 6.4 The Linear Time-Invariant TF We’ll begin the discussion in the context of a linear reservoir. Linear reservoirs are simple models designed to simulate the storage and discharge of water in a catchment. These models assume that the catchment can be represented as single storage compartments or as a series of interconnected storage compartments and that the change the amount of water stored in the reservoir (or reservoirs) is directly proportional to the inflows and outflows. In other words, the linear relationship between inflows and outflows means that the rate of water release is proportional to the amount of water stored in the reservoir. 6.4.0.1 The Instantaneous Unit Hydrograph: The Instantaneous Unit Hydrograph (IUH) represents the linear rainfall-runoff model used in the TRANSEP model. It is an approach to hydrograph separation that is useful for analyzing the temporal distribution of runoff in response to a ‘unit’ pulse of rainfall (e.g. uniform one-inch depth over a unit area represented by a unit hydrograph). In other words, it is a hydrograph that results from one unit (e.g. 1 mm) of effective rainfall uniformly distributed over the watershed and occurring in a short duration. Therefore, the following assumptions are made when the IUH is used as a transfer function: 1. the IUH reflects the ensemble of watershed characteristics 2. the shape characteristics of the unit hydrograph are independent of time 3. the output response is linearly proportional to the input Figure 6.4.1.a Conceptual diagram of the linear unit hydrograph Peak discharge of the unit hydrograph, \\(u_p\\); Base time \\(t_b\\)is the total duration of the unit hydrograph; Increase time or time to peak \\(t_p\\) is the time between the start point of the hydrograph and the peak; Concentration time \\(t_c\\) is the time between the end of rainfall and the end of the hydrograph; Lag time \\(t_lag\\) is the time between half rainfall depth and the peak of the hydrograph. IUH as a transfer function allows the calculation of the direct runoff hydrograph for any given rainfall input. IUH is particularly valuable for understanding how the direct runoff is distributed over time in response to a rainfall event. It helps quantify the time distribution of runoff in relation to the rainfall input. Figure 6.4.1.b The discharge of the unit hydrograph(u) from a catchment at time (t) is expressed as \\(h \\cdot u(t)\\) where h is the amount of effective rainfall. In the TRANSEP model, this transfer function is represented as \\(g(\\tau)\\) and thus the rainfall-induced response to runoff. \\[ g(\\tau) = \\frac{\\tau^{\\alpha-1}}{\\mathrm{B}^{\\alpha}\\Gamma(\\alpha)}exp(-\\frac{\\tau}{\\alpha}) \\] The linear portion of the TRANSEP model describes a convolution of the effective precipitation and a runoff transfer function. \\[ Q(t)= \\int_{0}^{t} g(\\tau)p_{\\text{eff}}(t-\\tau)d\\tau \\] Whoa, wait…what? Tau, integrals, and convolution? Don’t worry about the details of the equations. Check out this video to have convolution described using dollars and cookies, then imagine each dollar as a rainfall unit and each cookie as a runoff unit. Review the equations again after the video. knitr::include_url(&quot;https://www.youtube.com/embed/aEGboJxmq-w&quot;) 6.4.0.2 The Loss Function: The loss function represents the linear rainfall-runoff model used in the TRANSEP model. \\[ s(t) = b_{1} p(t + 1 - b_{2}^{-1}) s(t - \\triangle t) \\] \\[ s(t = 0) = b_{3} \\] \\[ p_{\\text{eff}}(t) = p(t) s(t) \\] where \\(p_{\\text{eff}}(t)\\) is the effective precipitation. \\(s(t)\\) is the antecedent precipitation index which is determined by giving more importance to recent precipitation and gradually reducing that importance as we go back in time. The rate at which this importance decreases is controlled by the parameter \\(b_{2}\\). The parameter \\(b_{3}\\) sets the initial antecedent precipitation index at the beginning of the simulated time series. In other words, these equations are used to simulate the flow of water in a hydrological system over time. The first equation represents the change in stored water at each time step, taking into account precipitation, loss to runoff, and the system’s past state. The second equation sets the initial condition for the storage at the beginning of the simulation. The third equation calculates the effective precipitation, considering both precipitation and the current storage state. 6.4.0.3 How do we code this? We will use a skeletal version of TRANSEP, focusing only on the rainfall-runoff piece which includes the loss-function and the gamma transfer function. We will use rainfall and runoff data from TCEF to model annual streamflow at a daily time step. Then we can use this model as a jump-off point to start talking about model calibration and validation in future modules. 6.4.0.4 Final thoughts: If during your modeling experience, you find yourself wading through a bog of complex physics and multiple layers of transfer functions to account for every drop of input into a system, it is time to revisit your objectives. Remember that a model is always ‘wrong’. Like a map, it provides a simplified representation of reality. It may not be entirely accurate, but it serves a valuable purpose. Models help us understand complex systems, make predictions, and gain insights even if they are not an exact replica of the real world. Check out this paper for more: https://agupubs.onlinelibrary.wiley.com/doi/10.1029/93WR00877 6.5 Codework - Transfer function rainfall-runoff model 6.5.1 Download the repo for this lab HERE In this homework/lab, you will write a simple, lumped rainfall-runoff model. The foundation for this model is the TRANSEP model from Weiler et al., 2003. Since TRANSEP (tracer transfer function hydrograph separation model) contains a tracer module that we don’t need, we will only use the loss function (Jakeman and Hornberger) and the gamma transfer function for the water routing. The data for the model is from the Tenderfoot Creek Experimental Forest in central Montana. Load packages with a function that checks and installs packages if needed: #knitr::opts_chunk$set(echo = FALSE) # # # Write your package testing function # pkgTest &lt;- function(x) # { # if (x %in% rownames(installed.packages()) == FALSE) { # install.packages(x, dependencies= TRUE) # } # library(x, character.only = TRUE) # } # # # Make a vector of the packages you need # neededPackages &lt;- c(&#39;tidyverse&#39;, &#39;lubridate&#39;, &#39;tictoc&#39;, &#39;patchwork&#39;) #tools for plot titles # # # For every package in the vector, apply your pkgTest function # for (package in neededPackages){pkgTest(package)} # # # tictoc times the execution of different modeling methods # # patchwork is used for side-by-side plots # # Let&#39;s assume that you know you put your data file in the working directory, but cannot recall its name. Let&#39;s do some working directory exploration with script: # # # Check your working directory: # print(getwd()) # # # Check the datafile name or path by listing files in the working directory. # filepaths &lt;-list.files() # # # Here is an option to list only .csv files in your working directory: # csv_files &lt;- filepaths[grepl(&quot;.csv$&quot;, filepaths, ignore.case = TRUE)] # print(csv_files) # ``` # # #### Read in the data, convert the column that has the date to a (lubridate) date, and add a column that contains the water year # # Identify the path to the desired data. # filepath &lt;- &quot;P_Q_1996_2011.csv&quot; # indata &lt;- read.csv(filepath) # # indata &lt;- indata %&gt;% # mutate(Date = mdy(Date)) %&gt;% # convert &quot;Date&quot; to a date object with mdy() # mutate(wtr_yr = if_else(month(Date) &gt; 9, year(Date) + 1, year(Date))) Define input year We could use every year in the time series, but for starters, we’ll use 2006. Use filter() to extract the 2006 water year. # PQ &lt;- indata %&gt;% # filter(wtr_yr == 2006) # extract the 2006 water year with filter() # # # plot discharge for 2006 # ggplot(PQ, aes(x = Date, y = Discharge_mm)) + # geom_line() # # # make flowtime correction - flowtime is a time-weighted cumulative flow, which aids in understanding the temporal distribution of flow, giving more weight to periods of higher discharge. flow time correction is relevant in hydrology when analyzing Q or time series data, so we can compare hydrological events on a standardized time scale. It can help to identify patterns, assess the duration of high or low flows, and compare behavior of watersheds over time. # # PQ &lt;- PQ %&gt;% # mutate(flowtime = cumsum(Discharge_mm)/mean(Discharge_mm)) %&gt;% # mutate(counter = 1:nrow(PQ)) # # ggplot() + # geom_line(data=PQ, aes(x = flowtime, y = Discharge_mm)) + # geom_line(data=PQ, aes(x = counter, y = Discharge_mm), color=&quot;red&quot;) **1) QUESTION: What does the function cumsum() do?(1 pt) ANSWER: Define the initial inputs This chunk defines the initial inputs for measured precip and measured runoff # tau &lt;- 1:nrow(PQ) # simple timestep counter the same length as PQ # Pobs &lt;- PQ$RainMelt_mm # observed precipitation from PQ df # Qobs &lt;- PQ$Discharge_mm # observed streamflow from PQ df Parameterization We will use these parameters. You can change them if you want, but I’d suggest leaving them like this at least until you get the model to work. Even tiny changes can have a huge effect on the simulated runoff. # # Loss function parameters # b1 &lt;- 0.0018 # volume control parameter (b1 in eq 4a) # b2 &lt;- 50 # backwards weighting parameter (b2 in eq 4a) # # determines how much weight or importance is given to past precipitation events when calculating an antecedent precipitation index. &quot;Exponential weighting backward in time&quot; means that the influence of past precipitation events diminishes as you move further back in time, and this diminishing effect follows an exponential pattern. # # b3 &lt;- 0.135 # initial s(t) value for s(t=0) (b3 in eq 4b) - Initial antecedent precipitation index value. # # # Transfer function parameters # a &lt;- 1.84 # TF shape parameter # b &lt;- 3.29 # TF scale parameter Loss function This is the module for the Jakeman and Hornberger loss function where we turn our measured input precip into effective precipitation (p_eff). This part contains three steps. 1) preallocate a vector p_eff: Initiate an empty vector for effective precipitation that we will fill in with a loop using Peff(t) = p(t)s(t). Effective precipitation is the portion of precipitation that generates streamflow and event water contribution to the stream. It is separated to produce event water and displace pre-event water into the stream. 2) set the initial value for s: s(t) is an antecedent precipitation index. How much does antecedent precipitation affect effective precipitation? 3) generate p_eff inside of a for-loop Please note: The Weiler et al. (2003) paper states that one of the loss function parameters (vol_c) can be determined from the measured input. That is actually not the case. s(t) is the antecedent precipitation index that is calculated by exponentially weighting the precipitation backward in time according to the parameter b2 is a ‘dial’ that places weight on past precipitation events. # # preallocate the p_eff vector # p_eff &lt;- vector(mode = &quot;double&quot;, length(tau)) # # s &lt;- b3 # at this point, s is equal to b3, the start value of s # # # loop with loss function # for (i in 1:length(p_eff)) { # s &lt;- b1 * Pobs[i] + (1 - 1/b2) * s # this is eq 4a from Weiler et al. (2003) # p_eff[i] &lt;- Pobs[i] * s # this is eq 4c from Weiler et al. (2003) # } # # # # #### alternative way to calculate p_eff by populating an s vector # # preallocate s_alt vector # # s_alt &lt;- vector(mode = &quot;double&quot;, length(tau)) # # s_alt[1] &lt;- b3 # # # preallocate p_eff vector # # p_eff_new &lt;- vector(mode = &quot;double&quot;, length(tau)) # # # start loop # # for (i in 2:(length(p_eff))) { # # s_alt[i] &lt;- b1 * Pobs[i] + (1 - 1/b2) * s_alt[i-1] # # p_eff_new[i] &lt;- Pobs[i] * s_alt[i] # # } # #### # # set a starting value for s # # # ## plot observed and effective precipitation against the date # # wide to long with pivot_longer() # precip &lt;- tibble(date = PQ$Date, obs = Pobs, sim = p_eff) %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -date) # # # # a good way to plot P time series is with a step function. ggplot2 has geom_step() for this. # ggplot(precip, aes(x = date, y = value, color = key)) + # geom_step() + # labs(color = &quot;P&quot;) # # # ## plot the ratio of p_eff/Pobs (call that ratio &quot;frac&quot;) # precip_new &lt;- tibble(date = PQ$Date, obs = Pobs, sim = p_eff, frac = p_eff/Pobs) # # ggplot(data = precip_new, aes(x = date, y = frac)) + # geom_line() **2) QUESTION (3 pts): Interpret the two figures. What is the meaning of “frac”? For this answer, think about what the effective precipitation represents. When is frac “high”, when is “frac” low? ANSWER: Extra Credit (2 pt): Combine the two plots into one, with a meaningfully scaled secondary y-axis** Runoff transfer function Routing module - This short chunk sets up the TF used for the water routing. This part contains only two steps: 1) the calculation of the actual TF. 2) normalization of the TF so that the sum of the TF equals 1. tau(0) is the mean residence time # #gTF &lt;- tau # # # this is the &quot;raw&quot; transfer function. This is eq 13 from Weiler 2003. Use the time step for tau. The Gamma function in R is gamma(). NOTE THAT THIS EQ IN WEILER ET AL IS WRONG! It&#39;s exp(-tau / b) and not divided by alpha. # g &lt;- (tau^(a - 1)) / ((b^a * gamma(a))) * exp(-tau / b) # # # normalize the TF, g, by the total sum of g. # gTF &lt;- g / sum(g) # # # plot the TF as a line against tau. You need to put the two vectors tau and gTF into a new df/tibble for this. # tf_plot &lt;- tibble(tau, gTF) # ggplot(tf_plot, aes(tau, gTF)) + # geom_line() 3) QUESTION (2 pt): Why is it important to normalize the transfer function? ANSWER: 4) QUESTION (4 pt): Describe the transfer function. What are the units and what does the shape of the transfer function mean for runoff generation? ANSWER: Convolution This is the heart of the model. Here, we convolute the input with the TF to generate runoff. There is another thing you need to pay attention to: We are only interested in one year (365 days), but since our TF itself is 365 timesteps long, small parts of all inputs except for the first one, will be turned into streamflow AFTER the water year of interest, that is, in the following year. In practice, this means you have two options to handle this. 1) You calculate q_all and then manually cut the matrix/vector to the correct length at the end, or 2) You only populate a vector at each time step and put the generated runoff per iteration in the correct locations within the vector. For this to work, you would need to trim the length of the generated runoff by one during each iteration. This approach is more difficult to code, but saves a lot of memory since you are only calculating/storing one vector of length 365 (or 366 during a leapyear). We will go with option 1). The code for option 2 is shown at the end for reference. Convolution summarized (recall dollars and cookies): Each loop iteration results in a row of the matrix representing the convolution at a specific time step. each time step of p_eff is an iteration, and for each timestep, it multiplies the effective precipitation at that timestep by the entire transfer function. Then each row is summed and stored in the vector q_all. q_all_loop is an intermediate step in the convolution process and can help visualize how the convolution evolves over time. As an example, if we have precip at time step 1, we are interested in how this contributes to runoff at time steps 1,2,3 etc, all the way to 365 (or the end of our period). so the first row of the matrix q_all_loop represents the contribution of precipitation at timestep 1 to runoff at each timestep. the second row represents the contribution of precipitation at time step 2 to runoff at each timestep. Then when we sum up the rows, we get q_all, where each element represents the total runoff at a specific time step. # tic() # # preallocate qsim matrix with the correct dimensions. Remember that p_eff and gTF are the same length. # q_all_loop &lt;- matrix(0, length(p_eff) * 2, length(p_eff)) # set number of rows and columns # # # convolution for-loop # for (i in 1:length(p_eff)) { # loop through length of precipitation timeseries # q_all_loop[(i):(length(p_eff) + i - 1), i] &lt;- p_eff[i] * gTF # populate the q_all matrix (this is the same code as the UH convolution problem with one tiny change because you need to reference gTF and not UH) # } # # # # add up the rows of the matrix to generate the final runoff and replace matrix with final Q # q_all &lt;- apply(q_all_loop, 1, sum, na.rm = TRUE) # # # cut the vector to the appropriate length of one year (otherwise it won&#39;t fit into the original df with the observed data) # q_all &lt;- q_all[1:length(p_eff)] # # # Write the final runoff vector into the PQ df # PQ$Qsim &lt;- q_all # toc() 5) QUESTION (5 pts): We set the TF length to 365. What is the physical meaning of this (i.e., how well does this represent a real system and why)? Could the transfer function be shorter or longer than that? ANSWER: # ## THIS PART SAVES ALL HOURLY Q RESPONSES IN A NEW DF AND PLOTS THEM # Qall &lt;- as_tibble(q_all_loop, .name_repair = NULL) # Qall &lt;- Qall[ -as.numeric(which(apply(Qall, 2, var) == 0))] # toc() # # # Qall[Qall == 0] &lt;- NA # # Qall &lt;- Qall %&gt;% # mutate(Time_hrs = 1:nrow(Qall)) %&gt;% # add the time vector # gather(key, value, -Time_hrs) # # Qall$key &lt;- as.factor(Qall$key) # # # ggplot(Qall, aes(x = Time_hrs, y = value, fill = key)) + # geom_line(alpha = 0.2) + # theme(legend.position = &quot;none&quot;) + # lims(x = c(1, 365)) + # labs(x = &quot;DoY&quot;, y = &quot;Q (mm/day&quot;) Plots Plot the observed and simulated runoff. Include a legend and label the y-axis. # # make long form # PQ_long &lt;- PQ %&gt;% # select(-wtr_yr, -RainMelt_mm) %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # # # plot hydrographs (as lines) and label the y-axis # ggplot(data = PQ_long, aes(x = Date, y = value, color = key)) + # geom_line() + # labs(x = {}, y = &quot;Q (mm/day)&quot;, color = {}) + # lims(x = as.Date(c(&quot;2005-10-01&quot;, &quot;2006-09-30&quot;))) 6) QUESTION (3 pt): Evaluate how good or bad the model performed (i.e., visually compare simulated and observed streamflow, e.g., low flows and peak flows). ANSWER: 7) QUESTION (2 pt): Compare the effective precipitation total with the simulated runoff total and the observed runoff total. What is p_eff and how is it related to q_all? Discuss why there is a (small) mismatch between the sums of p_eff and q_all. ANSWER: # sum(Pobs) # observed P # # sum(p_eff) # effective (modeled) P # # sum(q_all) # modeled Q # # sum(Qobs) # observed Q THIS IS THE CODE FOR CONVOLUTION METHOD 2 This method saves storage requirements since only a vector is generated and not a full matrix that contains all response pulses. The workflow here is to generate a a response vector for the first pulse. This will take up 365 time steps. On the second time step, we generate another response pulse with 364 steps that starts at t=2. We then add that vector to the first one. On the third time step, we generate a response pulse of length 363 that starts at t=3 and then add it to the existing one. And so on and so forth. # # METHOD 2, Option 1 # tic() # # preallocate qsim vector # q_all &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # # start convolution # for (i in 1:length(p_eff)) { # A &lt;- p_eff[i] * gTF # vector with current Q pulse # B &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # reset/preallocate vector with 0s # B[i:length(p_eff)] &lt;- A[1:(length(p_eff) - i + 1)] # iteration one uses the full A, iteration 2 full lengthmodel minus one, etc # q_all &lt;- q_all + B # add new convoluted vector to total runoff # } # toc() # # # # Method 2, Option 2 # tic() # # preallocate qsim vector # q_all_2 &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # B &lt;- vector(mode = &quot;double&quot;, length(p_eff)) # preallocate vector with 0s # # # start convolution # for (i in 1:length(p_eff)) { # A &lt;- p_eff[i] * gTF # vector with current Q pulse # B[i:length(p_eff)] &lt;- A[1:(length(p_eff) - i + 1)] # iteration one uses the full A, iteration 2 full lengthmodel minus one, etc # q_all_2[i:length(p_eff)] &lt;- q_all_2[i:length(p_eff)] + B[i:length(p_eff)] # add new convoluted vector to total runoff at correct locations # } # toc() # # # plot to show the two Q timeseries are the same # # Create two ggplot objects # plot_q1 &lt;- ggplot(data=test_q_all, aes(x=time, y=q1)) + # geom_line() + # labs(title = &quot;Q1 Plot&quot;) # # plot_q2 &lt;- ggplot(data=test_q_all, aes(x=time, y=q2)) + # geom_line(color=&quot;blue&quot;) + # labs(title = &quot;Q2 Plot&quot;) # # # Combine plots side by side # combined_plots &lt;- plot_q1 + plot_q2 + plot_layout(ncol = 2) # # # Display combined plots # combined_plots "],["monte-carlo-simulation-20-pnts.html", "Chapter 7 Monte Carlo Simulation (20 pnts) 7.1 Background: 7.2 How does this apply to hydrological modeling? 7.3 How do we generate a simulation with code? 7.4 Codework", " Chapter 7 Monte Carlo Simulation (20 pnts) 7.1 Background: Monte Carlo Simulation is a method to estimate the probability of the outcomes of an uncertain event. It is based on a law of probability theory that says if we repeat an experiment many times, the average of the results will get closer to the true probability of those outcomes. First check out this video: 7.1.1 Reading Then read this: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2924739/ for a understanding of the fundamentals. 7.2 How does this apply to hydrological modeling? When modeling watershed hydrological processes, we often attempting to quantify watershed inputs e.g., precipitation watershed outputs e.g., evapotranspiration, sublimation, runoff/discharge watershed storage precipitation that is not immediately converted to runoff or ET rather stored as snow or subsurface water. Imagine we are trying to predict the percentage of precipitation stored in a watershed after a storm event. We have learned that there are may factors that affect this prediction, like antecedent conditions, that may be difficult to measure directly. Monte Carlo Simulation can offer a valuable approach to estimate the probability of obtaining certain measurements when those factors can not be directly observed or measured. We can approximate the likelihood of specific measurement by simulating a range of possible scenarios. Monte Carlo Simulation is not only useful for estimating probabilities, but for conducting sensitivity analysis. In any model, there are usually several input parameters. Sensitivity analysis helps us understand how changes in these parameters affect the predicted values. To perform a sensitivity analysis using a Monte Carlo Simulation we can: Define the realistic ranges for each parameter we want to analyze Using Monte Carlo Simulation, randomly sample values from the defined ranges for each parameter Analyze output to understand how different input sample values affect the predicted output 7.2.1 Example Let’s consider all of this in an example model called WECOH - Watershed ECOHydrology. In this study, researchers (Nippgen et al.) were interested in the change in subsurface water storage through time and space on a daily and seasonal basis. Evolution of watershed connectivity The authors directly measured runoff/discharge, collected precipitation data at several points within the watershed, used remote sensing to estimate how much water was lost to evapotranspiration, and used digital elevation models to characterize the watershed topography. As we learned in the hydrograph separation module, topographic characteristics can have a significant impact on storage. Though resources like USDA’s Web Soil Survey can provide a basic understanding underlying geology across a large region, characterizing the heterogeneous nature of soils within a watershed can be logistically unfeasible. To estimate the soil characteristics like storage capacity (how much water the soil can hold) and hydraulic conductivity (how easily water can move through soil) in the study watershed, the authors used available resources to determine the possible range of values for each of their unknown parameters. They then tested thousands of model simulations using randomly selected values with the predetermined range for each of the soil characteristics. They compared the simulated discharge from these simulations to the actual discharge measurements. The simulations that predicted discharge patterns that closely matched reality helped them to estimate the unknown soil properties. Additionally, from the results of these simulations, they could identify which model inputs had the most significant impact on the discharge predictions, and how sensitive the output was to changes in each parameter. In this case, they determined that the model and system were most sensitive to precipitation. This type of sensitivity analysis can help us interpret the relative importance of different parameters and understand the overall sensitivity of the model or system. The study is linked for your reference but a thorough reading is not required. 7.2.2 Reading However, do read this methods paper by Knighton et al. for an example of how Monte Carlo Simulation was used to estimate hydraulic conductivity in an urban system with varied land-cover. 7.3 How do we generate a simulation with code? For a 12-minute example in RStudio, check this out. If you are still learning the basics of R functionality, it may be helpful to code along with this video, pausing as needed. Note that this instruction is coding in an Rscript (after opening RStudio &gt; File &gt; New File &gt; R Script), rather than an Rmarkdown that we use in this class. 7.4 Codework 7.4.1 Download the repo for this lab HERE In this lab/homework, you will use the transfer function model from the previous module for some sensitivity analysis using Monte Carlo simulations. The code is mostly the same as last module with some small adjustments to save the parameters from each Monte Carlo run. As a reminder, in the Monte Carlo analysis, we will run the model x number of times, save the parameters and evaluate the fit after each run. After the completion of the MC runs, we will use the GLUE method to evaluate parameter sensitivity. There are three main objectives for this homework: 1) Set up the Monte Carlo analysis 2) Run the MC simulation for ONE of the years in the study period and perform a GLUE sensitivity analysis 3) Compare the different objective functions. A lot of the code in this homework will be provided. 7.4.2 Setup Import packages, including the “progress” and “tictoc” packages. These will allow us to time our loops and functions Read data - this is the same PQ data we have worked with in previous modules. # rm(list = ls(all = TRUE)) # clear global environment # # indata &lt;- read_csv(&quot;P_Q_1996_2011.csv&quot;) # indata &lt;- indata %&gt;% # mutate(Date = mdy(Date)) %&gt;% # bring the date colum in shape # mutate(wtr_yr = if_else(month(Date) &gt; 9, year(Date) + 1, year(Date))) # create a water year column # # # choose the 2006 water year # PQ &lt;- indata %&gt;% # filter(wtr_yr == 2006) Define variables # tau &lt;- 1:nrow(PQ) # simple timestep counter # Pobs &lt;- PQ$RainMelt_mm # observed precipitation # Qobs &lt;- PQ$Discharge_mm # observed streamflow 7.4.3 Parameter initialization This chunk has two purposes. The first is to set up the number of iterations for the Monte Carlo simulation. The entire model code is essentially wrapped into the main MC for loop. Each iteration of that loop is one full model realization: loss function, TF, convolution, model fit assessment (objective function). For each model run (each MC iteration), we will save the model parameters and respective objective functions in a dataframe. This will be the main source for the GLUE sensitivity analysis at the end. The model parameters are sampled in the main model chunk, this is just the preallocation of the dataframe. You will both run your own MC simulation, to set up the code, but will also receive a .Rdata file with 100,000 runs to give you more behavioral runs for the sensitivity analysis and uncertainty bounds. As a tip, while setting up the code, I would recommend setting the number of MC iterations to something low, for example 10 or maybe even 1. Once you have confirmed that your code works, crank up the number of iterations. Set it to 1000 to see how many behavioral runs you get. After that, load the file with the provided runs. # nx sets the number of Monte Carlo runs # nx &lt;- 100 # number of runs # # # set up the parameter matrix for the loss function, transfer function, and objective functions # param &lt;- tibble( # Run = vector(mode = &quot;double&quot;, nx), # counter for model run # b1 = vector(mode = &quot;double&quot;, nx), # b2 = vector(mode = &quot;double&quot;, nx), # b3 = vector(mode = &quot;double&quot;, nx), # a = vector(mode = &quot;double&quot;, nx), # b = vector(mode = &quot;double&quot;, nx), # nse = vector(mode = &quot;double&quot;, nx), # kge = vector(mode = &quot;double&quot;, nx), # rmse = vector(mode = &quot;double&quot;, nx), # mae = vector(mode = &quot;double&quot;, nx) # ) 7.4.4 MC Model run This is the main model chunk. The tic() and toc() statements measure the execution time for the whole chunk. There is also a progressbar in the chunk that will run in the console and inform you about the progress of the MC simulation. The loss function parameters are set in the loss function, the TF parameters in the loss function code. For each loop iteration, we will store the parameter values and the simulated discharge in the “param” dataframe. So, if we ran the MC simulation 100 times, we would end up with 100 parameter combinations and simulated discharges. Q1 (3 pt) How are the loss function and TF parameters being sampled? That is, what is the underlying distribution and why did we choose it? (2-3 sentences) ANSWER: Extra point: What does the while loop do in the TF calculation? And why is it in there? (1-2 sentences) ANSWER: Save or load data After setting up the MC simulation, we will actually use a pre-created dataset. Running the MC simulation tens of thousands of times will take multiple hours. For that reason, we will use an existing data set. # save.image(file = &#39;AllData.RData&#39;) # load the MC data # load(&quot;AllData.RData&quot;) Best run Now that we have the dataframe with all parameter combinations and all efficiencies, we can plot the best simulation and compare it to the observed discharge # # take the best run (i.e., first row in param df), unlist the simulated discharge, and store it in a dataframe # PQ$Qsim &lt;- unlist(param$qsim[1]) # # # make long form # PQ_long &lt;- PQ %&gt;% # select(-wtr_yr, -RainMelt_mm) %&gt;% # remove the water year and rainmelt columns # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # # # plot observed and best simulated discharge in the same figure against the date # ggplot(PQ_long, aes(x = Date, y = value, color = key)) + # theme_bw(base_size = 15) + # geom_line() + # labs(y = &quot;Discharge (mm/day)&quot;, color = { # }) 7.4.5 Sensitivity analysis We will use the GLUE methodology to assess parameter sensitivity. We will use GLUE for two things: 1) To assess parameter sensitivity, and 2) to create an envelope of model simulations. For the first step, we need to bring the data into shape so that we can plot dotty plots and density plots. We will use NSE for this. You want to plot each parameter in its own box. Look up facet_wrap() for how to do this! You want the axis scaling to be “free”. # # select columns and make long form data # param_long &lt;- param %&gt;% # select(-Run, -kge, -rmse, -mae, -qsim) %&gt;% # remove unnecessary columns. We only want the five parameters and nse # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -nse) # make long form # # # # set nse cutoff for behavioral model simulations. use 0.7 as threshold # cutoff &lt;- 0.8 # param_long &lt;- param_long %&gt;% # filter(nse &gt; cutoff) # use filter() to only use runs with nse greater than the cutoff # # # # dotty plots # ggplot(param_long, aes(x = value, y = nse)) + # x is parameter value, y is nse value # geom_point() + # plot as points # facet_wrap(vars(key), scales = &quot;free&quot;) + # facets are the individual parameters # ylim(cutoff, 1) + # sets obj fct axis limit from the cutoff value to 1 # theme( # strip.text = element_text(face = &quot;bold&quot;, size = 8), # strip.background = element_rect( # fill = &quot;gray95&quot;, # colour = &quot;gray&quot;, # size = 0.5 # ) # ) # # # # density plots # ggplot() + # theme_bw(base_size = 15) + # geom_density(data = param_long, aes(x = value)) + # geom_density() to plot the density # facet_wrap(~key, scales = &quot;free&quot;) + # facet_wrap() to get each parameter in its own box # theme( # strip.text = element_text(face = &quot;bold&quot;, size = 8), # strip.background = element_rect( # fill = &quot;gray95&quot;, # colour = &quot;gray&quot;, # size = 0.5 # ) # ) # # # # 2d density plots # ggplot() + # geom_density_2d_filled( # geom_density_2d_filled() for the actual density plot # data = param_long, # aes(x = value, y = nse), # alpha = 1, # contour_var = &quot;ndensity&quot; # ) + # geom_point( # geom_point() to show the indivudal runs # data = param_long, # aes(x = value, y = nse), # shape = 1, # alpha = 0.2, # size = 0.5, # stroke = 0.2, # color = &quot;black&quot; # ) + # theme_bw(base_size = 15) + # facet_wrap(~key, scales = &quot;free&quot;) + # theme( # strip.text = element_text(face = &quot;bold&quot;, size = 8), # strip.background = element_rect( # fill = &quot;gray95&quot;, # colour = &quot;gray&quot;, # linewidth = 0.5 # ) # ) + # labs(x = &quot;Value&quot;, y = &quot;NSE (-)&quot;) + # theme(legend.title = element_blank(), legend.position = &quot;none&quot;) Q2 (5 pts) Describe the sensitivity analysis with the three different plots. Are the parameters sensitive? Which ones are, which ones are not? Does this affect your “trust” in the model? (5-8 sentences) ANSWER: Q3 (4 pts) What are the differences between the dotty plots and the density plots? What are the differences between the two density plots? (2-3 sentences) ANSWER: Uncertainty bounds In this chunk, we will generate uncertainty bounds for the simulation. We will again only use the runs that we identified as behavioral. # # remove non-behavioral runs using the cutoff # param_ci &lt;- param %&gt;% # filter(nse &gt; 0.872) # only use obj function (nse) values above the previously defined threshold # # # make df with all of the top runs. this saves each of the top simulated Q time series in its own column. # # the columns are called V1 through Vn, with V1 being the best simulation and Vn the worst simulation of the behavioral runs. # Qruns &lt;- # as_tibble(matrix( # unlist(param_ci$qsim), # ncol = nrow(param_ci), # byrow = F # ), .name_repair = NULL) # # # combine Qruns with date and observed runoff from the PQ data frame. additionally, calculate mins and maxs # Qruns &lt;- # bind_cols(Date = PQ$Date, Qruns) %&gt;% # bind_cols() to combine Date, RainMelt_mm, and Qruns # mutate(Qmin = apply(Qruns, 1, min)) %&gt;% # get the rowmin for simulated Qs # mutate(Qmax = apply(Qruns, 1, max)) # get the rowmax for simualted Qs # # # long form # Qruns_long &lt;- Qruns %&gt;% # # select(-Discharge_mm, -Qsim) %&gt;% # remove observed and best simulated discharge # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # make long form # # # plot with all simulated runs # ggplot() + # geom_line(data = Qruns_long, aes(x = Date, y = value, color = key)) + # plot of all simulated runs. Use Qruns_long here. # guides(color=FALSE) + # geom_line( # data = Qruns, # aes(x = Date, y = V1, color = &quot;Best run&quot;), # size = 1 # ) + # plot of best simulation. Use Qruns here. V1 is the best simulated discharge # labs(y = &quot;Q (mm/day)&quot;, x = { # }, color = { # }) # # # real min and max envelope. You need Qruns for the simulated Q and envelope and PQ to plot the observed streamflow. # ggplot() + # geom_ribbon(data = Qruns, aes(x = Date, ymin = Qmin, ymax = Qmax)) + # plot of envelopes. look up geom function that allows you to plot a shaded area between two lines # # plot the best simulated run. remember, that is V1 in the Qruns df # geom_line( # data = Qruns, # aes(x = Date, y = V1, color = &quot;Best run&quot;), # size = 0.6 # ) + # plot of best simulation # geom_line( # data = PQ, # aes(x = Date, y = Discharge_mm, color = &quot;Observed Q&quot;), # size = 0.6 # ) + # plot of observed Q # labs(y = &quot;Q (mm/day)&quot;, x = { # }, color = { # }) Q4 (2 pts) Describe what the envelope actually is. Could we say we are dealing with confidence or prediction intervals? (2-3 sentences) ANSWER: Q5 (3 pts) If you inspect the individual model runs (in the Qruns df), you will notice that they all perform somewhat poorly when it comes to the initial baseflow. Why is that and what could you do to change this? (Note: you don’t have to actually do this, just describe how you might approach that issue (2-3 sentences) ANSWER: Q6 (3 pts) Plot AND compare the best model run for the four objective functions. Note: This requires you to completely examine the figure generated in the previous code chunk. Remember that you can zoom into the plot to better see differences in peakflow and baseflow, timing, etc.(4+ sentences) ANSWER: The coding steps are: 1) get the simulated q vector with the best run for each objective function, 2) put them in a dataframe/tibble, 3) create the long form, 4) plot the long form dataframe/tibble. These steps are just ONE possible outline how the coding steps can be broken up. # # sort the param dataframe to select the best objective functions and unlist the relevant qsim values from the initial param dataset # param &lt;- arrange(param, desc(param$nse)) # nse # q1 &lt;- unlist(param$qsim[1]) # param &lt;- arrange(param, desc(param$kge)) # kge # q2 &lt;- unlist(param$qsim[1]) # param &lt;- arrange(param, param$rmse) # rmse # q3 &lt;- unlist(param$qsim[1]) # param &lt;- arrange(param, param$mae) # mae # q4 &lt;- unlist(param$qsim[1]) # # # # Create dataframe with date and the best runs for each obj function # PQobjfuns &lt;- tibble( # Date = PQ$Date, # Qsim_nse = q1, # Qsim_kge = q2, # Qsim_rmse = q3, # Qsim_mae = q4, # Qobs = Qobs # ) # # # Create a long form tibble # PQobjfuns_long &lt;- PQobjfuns %&gt;% # pivot_longer(names_to = &quot;key&quot;, values_to = &quot;value&quot;, -Date) # # # Plot up the data # # # Plot up the four simulations and label them accordingly in the legend. # # places the legend inside the plot window in the upper left corner to maximize plot space # # remember that you can zoom into the plot with plotly # all_obj_fcts &lt;- ggplot() + # geom_line(data = PQobjfuns_long, aes(x = Date, y = value, color = key)) + # labs(y = &quot;Discharge (mm/day)&quot;) + # scale_color_discrete( # name = &quot;Obj. Function&quot;, # labels = c( # &quot;Observed Runoff&quot;, # &quot;Kling-Gupta Efficiency&quot;, # &quot;Mean Absolute Error&quot;, # &quot;Nash-Sutliffe Efficiency&quot;, # &quot;Root Mean Square Error&quot; # ) # ) + # theme(legend.position = c(0.2, 0.7)) # places the legend inside the plot window # # ggplotly(all_obj_fcts) Response surface # param_surf &lt;- param %&gt;% # select(-Run, -qsim) %&gt;% # drop_na() %&gt;% # filter(nse &gt;= 0) # # ggplot() + # geom_density_2d_filled(data = param_surf, aes(x = a, y = b)) # # ggplot() + # geom_contour(data = param_surf, aes(x = a, y = b, z = nse)) "],["classifying-model-structure---lecture-only-0-pts.html", "Chapter 8 Classifying model structure - Lecture only (0 pts) 8.1 Spatial Complexity 8.2 Modeling Approaches", " Chapter 8 Classifying model structure - Lecture only (0 pts) Throughout the rest of the course, we will gather data and create models to explore how environmental factors, such as snowmelt, land cover, evapotranspiration (ET), and topography, impact runoff. To discuss these methods, we should review some modeling terminology describing model complexity and type. Environmental models, including hydrological models, are built around simplifying assumptions of natural systems. The complexity of the model may depend on its application. Effective hydrological models share key traits: they are simple, parsimonious, and robust across various watersheds. In other words, they are easy to understand and streamlined and consistently perform well across different basins or even geographical areas. Therefore, more complex is only sometimes better. 8.1 Spatial Complexity There are general terms that classify the spatial complexity of hydrological models: A lumped system is one in which the dependent variables of interest are a function of time alone, and the study basin is spatially ‘lumped’ or assumed to be spatially homogeneous across the basin. So far in this course, we have focused mainly on lumped models. You may remember the figure below from the transfer functions module. It represents the lumped watershed as a bucket with a single input, outlet output, and storage volume for each timestep. A distributed system is one in which all dependent variables are functions of time and one or more spatial variables. Modeling a distributed system means partitioning our basins into raster cells (grids) and assigning inputs, outputs, and the spatial variables that affect inputs and outputs across these cells. We then calculate the processes at the cell level and route them downstream. These models allow us to represent the spatial complexity of physically based processes. They can simulate or forecast parameters other than streamflow, such as soil moisture, evapotranspiration, and groundwater recharge. A semi-distributed system is an intermediate approach that combines elements of both lumped and distributed systems. Certain variables may be spatially distributed, while others are treated as lumped. Alternatively, we can divide the watershed into sub-basins and treat each sub-basin as a lumped basin. Outputs from each sub-basin are then linked together and routed downstream. Semi-distribution allows for a more nuanced representation of the basin’s characteristics, acknowledging spatial variability where needed while maintaining some simplifications for computation efficiency. In small-scale studies, we can design a model structure that fits the specific situation well. However, when we are dealing with larger areas, model design may be challenging. Our data might differ across regions with variable climate and landscape features. Sometimes, it is best to use a complex model to capture all the different processes happening over a big area. However, it could be better to stick with a simpler model because we might have limited data or the number of calculations is very computationally expensive. It is up to the modeler to determine the simplest model that meets the desired application. 8.2 Modeling Approaches Empirical Models are based on empirical analysis of observed inputs (e.g., rainfall) or outputs (ET, discharge). These simple models may not be transferable to other watersheds. Also, they may not reveal much about the physical processes influencing runoff. Therefore, these types of models may not be valid if the study area experiences land use or climate change. Conceptual Models describe processes with simple mathematical equations. For example, we might use a simple linear equation to interpolate precipitation inputs over a watershed with a high elevation gradient using precipitation measurements from two points (high and low). This represents the basic relationship between precipitation and elevation, but does not capture all features that affect precipitation patterns (e.g. aspect, prevailing winds). The combined impact of these factors is probably negligible compared to the substantial amount of data required to accurately model them. Physically Based Models These models offer deep insights into the processes governing runoff generation by relying on fundamental physical equations like mass conservation. However, they come with drawbacks. Their implementation often demands complex numerical solving methods and a significant volume of input data. Without empirical data to validate these techniques, there is a risk of introducing substantial uncertainty into our models, reducing their reliability and effectiveness When modeling watersheds, we often use a mix of empirical, conceptual, and physically based models. The choice of model type depends on factors like the data we have, the time or computing resources we can allocate, and how we plan to use the model. "],["snowmelt-models-for-runoff-timing---lecture-lab-20-pts.html", "Chapter 9 Snowmelt Models for runoff timing - Lecture &amp; Lab (20 pts) 9.1 Module repo 9.2 Background: 9.3 Model Approaches 9.4 Spatial complexity 9.5 Model choices: My snow is different from your snow 9.6 Codework 9.7 Modeling SWE", " Chapter 9 Snowmelt Models for runoff timing - Lecture &amp; Lab (20 pts) 9.1 Module repo 9.2 Background: Understanding snowmelt runoff is crucial for managing water resources and assessing flood risks, as it plays a significant role in the hydrologic cycle. Annual runoff and peak flow are influenced by snowmelt, rainfall, or a combination of both. In regions with a snowmelt-driven hydrologic cycle, such as the Rocky Mountains, snowpack acts as a natural reservoir, storing water during the winter months and releasing it gradually during the spring and early summer, thereby playing a vital role in maintaining water availability for various uses downstream. By examining how snowmelt interacts with other factors like precipitation, land cover, and temperature, we can better anticipate water supply fluctuations and design effective flood management strategies. Learning objectives: In this module, our primary focus will be modeling snowmelt as runoff, enabling us to predict when it will impact streamflow timing. We will consider some factors that may influence runoff timing. However, the term ‘snowmelt modeling’ is a field in itself and can represent a lifetime worth of work. There are many uses for snowmelt modeling (e.g., climate science and avalanche forecasting). If you are interested in exploring more on this subject, there is an excellent Snow Hydrology: Focus on Modeling series offered by CUAHSI’s Virtual University on YouTube. Helpful terms: The most common way to measure the water content of the snowpack is by the Snow Water Equivalent or SWE. The SWE is the water depth resulting from melting a unit column of the snowpack. 9.3 Model Approaches Similar to the model development structure we discussed in the last module, snowmelt models are generally classified into three different types of abalation algorithms Empirical and Data-Driven Models: These models use historical data and statistical techniques to predict runoff based on the relationship between snow characteristics (like snow area) and runoff. They use past patterns to make predictions about the future. The emergence of data-driven models has benefited from the growth of massive data and the rapid increase in computational power. These models simulate the changes in snowmelt runoff using machine learning algorithms to select appropriate parameters (e.g., daily rainfall, temperature, solar radiation, snow area, and snow water equivalent) from different data sources. Conceptual Models: These models simplify the snowmelt process by establishing a simple, rule-based relationship between snowmelt and temperature. These models use a basic formula based on temperature to estimate how much snow will melt. Physical Models: The physical snowmelt models calculate snowmelt based on the energy balance of snow cover. If all the heat fluxes toward the snowpack are considered positive and those away are considered negative, the sum of these fluxes is equal to the change in heat content of the snowpack for a given time period. Fluxes considered may be net solar radiation (solar incoming minus albedo), thermal radiation, sensible heat transfer of air (e.g., when air is a different temperature than snowpack), latent heat of vaporization from condensation or evaporation/sublimation, heat conducted from the ground, advected heat from precipitation examples: layered snow thermal model (SNTHERM) and physical snowpack model (SNOWPACK), Many effective models may incorporate elements from some or all of these modeling approaches. 9.4 Spatial complexity We may also identify models based on the model architecture or spatial complexity. The architecture can be designed based on assumptions about the physical processes that may affect the snowmelt to runoff volume and timing. Homogenous basin modeling: You may also hear these types of models referred to as ‘black box’ models. Black-box models do not provide a detailed description of the underlying hydrological processes. Instead, they are typically expressed as empirical models that rely on statistical relationships between input and output variables. While these models can predict specific outcomes effectively, they may not be ideal for understanding the physical mechanisms that drive hydrological processes. In terms of snow cover, this is a simplistic case model where we assume: the snow is consistent from top to bottom of the snow column and across the watershed melt appears at the top of the snowpack water immediately flows out the bottom This type of modeling may work well if the snowpack is isothermal, if we are interested in runoff over large timesteps, or if we are modeling annual water budgets in lumped models. Vertical layered modeling: Depending on the desired application of the model, snowmelt may be modeled in multiple layers in the snow column (air-snow surface to ground). Climate models, for example, may estimate phase changes or heat flux and consider the snowpack in 5 or more layers. Avalanche forecasters may need to consider grain evolution, density, water content, and more over hundreds of layers! Hydrologists may also choose variable layers, but many will choose single- or two-layer models for basin-wide studies, as simple models can be effective when estimating basin runoff. Here is a study by Dutra et al. (2012) that looked at the effect of the number of snow layers, liquid water representation, snow density, and snow albedo parameterizations within their tested models. Table 1 and figures 1-3 will be sufficient to understand the effects of changes to these parameters on modeled runoff and SWE. In this case, the three-layer model performed best when predicting the timing of the SWE and runoff, but density improved more by changing other parameters rather than layers (Figure 1). Lateral spatial heterogeneity: The spatial extent of the snow cover determines the area contributing to runoff at any given time during the melt period. The more snow there is, the more water there will be when it melts. Therefore, snow cover tells us which areas will contribute water to rivers and streams as the snow melts. In areas with a lot of accumulated snow, the amount of snow covering the ground gradually decreases as the weather warms up. This melting process can take several months. How quickly the snow disappears depends on the landscape. For example, in mountainous ecosystems, factors like elevation, slope aspect, slope gradient, and forest structure affect how the snow can accumulate, evaporate or sublimate and how quickly the snow melts. For mountain areas, similar patterns of depletion occur from year to year and can be related to the snow water equivalent (SWE) at a site, accumulated ablation, accumulated degree-days, or to runoff from the watershed using depletion curves from historical data. Here is an example of snow depletion curves developed using statistical modeling and remotely sensed data. The use of remotely sensed data can be incredibly helpful to providing estimates in remote areas with infrequent measurements. Observing depletion patterns may not be effective in ecosystems where patterns are more variable (e.g., prairies). However, stratified sampling with snow surveys, snow telemetry networks (SNOTEL) or continuous precipitation measurements can be used with techniques like cluster analyses or interpolation, to determine variables that influence SWE and estimate SWE depth or runoff over heterogeneous systems. You can further explore all readings linked in the above section. These readings may assist in developing the workflow for your term project, though they are optional for completing this assignment. However, it is recommended that you review the figures to grasp the concepts and retain them for future reference if necessary. 9.5 Model choices: My snow is different from your snow When determining the architecture of your snow model, your model choices will reflect the application of your model and the processes you are trying to represent. Recall that parsimony and simplicity often make for the most effective models. So, how do we know if we have a good model? Here are a few things we can check to validate our model choices: Model Variability: A good model should produce consistent results when given the same inputs and conditions. Variability between model runs should be minimal if the watershed or environment is not changing. Versatility: Check the model under a range of scenarios different from the conditions under which it was developed. The model should apply to similar systems or scenarios beyond the initial scope of development Sensitivity Analysis: We reviewed this a bit in the Monte Carlo module. How do changes in model inputs impact outputs? A good model will show reasonable sensitivity changes in input parameters, with outputs responding as expected. Validation with empirical data: Comparison with real-world data checks whether the model accurately represents the actual system Applicability and simplicity: A good model should provide valuable insights or aid in decision-making processes relevant to the problem it addresses. It strikes a balance between complexity and simplicity, avoiding unnecessary intricacies that can lead to overfitting or computational inefficiency while sufficiently capturing the system’s complexities. 9.6 Codework 9.6.1 Download the repo for this lab HERE In this module, we will simulate snowmelt in a montane watershed in central Colorado with a simple temperature model. The Fool Creek watershed is located in the Fraser Experimental Forest (FEF), 137 km west of Denver, Colorado. The FEF contains several headwater streams (Strahler order 1-3) within the Colorado Headwaters watershed which supplies much of the water to the populated Colorado Front Range. This Forest has been the site of many studies to evaluate the effects of land cover change on watershed hydrology. The Fool Creek is a small, forested headwater stream, with an approximate watershed area of 2.75 km^2. The watershed elevation ranges from approximately 2,930m (9,600ft) at the USFS monitoring station to 3,475m (11,400ft) at the highest point. There is a SNOTEL station located at 3,400m. Fool Creek delineated watershed Question 1: Using this map, what differences can you see between these two sites or what variation do you see in the watershed that may affect snow accumulation and melt? (2 points)? ANSWER: Let’s look at some data for Fool Creek: This script collects SNOTEL input data using snotelr. The SNOTEL automated collection site in Fool Creek supplies SWE data that can simplify our runoff modeling. Lets explore the sites available through snotelr and select the site of interest. # Save metadata for review. # meta_data &lt;- snotel_info() # In the view window, you can find Fool Creek site information by sorting by state or site name using the arrows at the top of each column. # View(meta_data) # Locate the site_id for Fool Creek 9.6.2 Import data: We will generate a conceptual runoff model using the date, daily mean temperature, SWE depth in mm, and daily precipitation (mm). We will use select() to keep the desired columns only. Then we will use mutate() to add a water year, and group_by() to add a cumulative precipitation column for each water year. # # Let&#39;s download data for the water years 2021 to 2022: # df &lt;- snotel_download(site_id = 1186, internal = TRUE) %&gt;% # mutate(date = lubridate::ymd(date)) %&gt;% # format the date # filter(between(date, lubridate::ymd(&quot;2020-10-01&quot;), lubridate::ymd(&quot;2022-09-30&quot;))) %&gt;% # select desired dates # rename(swe.mm = snow_water_equivalent, # rename columns for easier use # precip.mm = precipitation, # temp_mean.c = temperature_mean) %&gt;% # # only select data we need # select(date, swe.mm, precip.mm, temp_mean.c) %&gt;% # mutate(wtr_yr = if_else(month(date) &lt; 10, year(date), year(date) + 1)) %&gt;% # group_by(wtr_yr) %&gt;% # add a water year column # mutate(pcumul.mm = cumsum(precip.mm)) %&gt;% # generate a cumulative sum column for each water year # ungroup() We will also download flow data collected by USFS at the Fool Creek outlet at 2,930m. # fool_q &lt;- read.csv(&quot;LFC_weir WY21-WY22.csv&quot;) %&gt;% # mutate(date = mdy_hm(TIMESTAMP)) %&gt;% # # convert cubic feet per second to metric (m/s^3) # # 1 cfs is equal to 0.028316847 cubic meter/second # mutate(m3_s = Q..cfs.*0.028316847) Let’s look at the data for the SNOTEL station at 3400m in elevation. # Plot imported SWE, precip and depth data data # ggplot(data = df) + # geom_line(aes(x = date, y = swe.mm, color = &#39;swe.mm&#39;)) + # geom_line(aes(x = date, y = pcumul.mm, color = &#39;pcumul.mm&#39;)) + # geom_line(aes(x=date, y = precip.mm, color = &#39;precip.mm&#39;)) + # theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels by 45 degrees # axis.line = element_line(color = &quot;black&quot;), # Add black axis lines # panel.background = element_rect(fill = &quot;white&quot;), # Set panel background to white # panel.grid.major = element_blank(), # Remove major grid lines # panel.grid.minor = element_blank()) + # Remove minor grid lines # ylab(&quot;mm&quot;) + # xlab(&#39;Date&#39;) 9.6.3 Calculate liquid input to the ground by analyzing the daily changes in Snow Water Equivalent (SWE) and precipitation. This script incorporates temperature conditions to determine when to add changes to liquid input. # sntl &lt;- df # # sntl &lt;- sntl %&gt;% # # calculate differences for SWE and cumulative P and put 0 as the first value # mutate(SWEdiff.mm = c(0, diff(sntl$swe.mm, lag = 1)), #find daily change in SWE # Pdiff.mm = c(0, diff(pcumul.mm, lag = 1))) %&gt;% # # remove the negative values at the start of each year # mutate(Pdiff.mm = if_else(Pdiff.mm &lt; 0, 0, Pdiff.mm)) %&gt;% # # conditional statements for calculate liquid input to the ground # mutate(input.mm = if_else(Pdiff.mm &gt; 0 &amp; SWEdiff.mm, 0, 0), # input.mm = if_else(Pdiff.mm == 0 &amp; SWEdiff.mm &lt; 0, -SWEdiff.mm, input.mm), # input.mm = if_else(Pdiff.mm &gt; 0 &amp; SWEdiff.mm &lt; 0, Pdiff.mm - SWEdiff.mm, input.mm), # input.mm = if_else(Pdiff.mm &gt; 0 &amp; SWEdiff.mm == 0, Pdiff.mm, input.mm)) %&gt;% # # this replaces missing air temp values with 0 # mutate(temp_mean.c = replace_na(temp_mean.c, 0)) %&gt;% # # this is a check that prevents liquid inputs when the air temp is below 0 # mutate(input.mm = if_else(temp_mean.c &lt; 0 &amp; input.mm &gt; 0, 0, input.mm)) %&gt;% # mutate(date = lubridate::ymd(date)) Question 2: In plain language, explain each step of the 5 if-else statements above (~5 brief sentences) (3pts). ANSWER: # # Create a cumulative input column for visualization and calculations # sntl &lt;- sntl %&gt;% # group_by(wtr_yr) %&gt;% # mutate(cum_input.mm = cumsum(input.mm)) %&gt;% # generate a cumulative sum column for each water year # ungroup() Let’s visualize the timing disparities between precipitation and melt input in relation to discharge. # # Convert date column to POSIXct format # fool_q$date &lt;- as.POSIXct(fool_q$date) # sntl$date &lt;- as.POSIXct(sntl$date) # # # Plot imported SWE, precip, and depth data # ggplot() + # # Scale discharge for the right y-axis # geom_line(data = fool_q, aes(x = date, y = m3_s*10000, color = &#39;m^3/second&#39;)) + # geom_point(data = sntl, aes(x = date, y = cum_input.mm, color = &#39;cumulative input (mm)&#39;)) + # geom_point(data = sntl, aes(x = date, y = pcumul.mm, color = &#39;cumulative precipitation (mm)&#39;)) + # theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis labels by 45 degrees # axis.line = element_line(color = &quot;black&quot;), # Add black axis lines # panel.background = element_rect(fill = &quot;white&quot;), # Set panel background to white # panel.grid.major = element_blank(), # Remove major grid lines # panel.grid.minor = element_blank(), # Remove minor grid lines # ) + # ylab(&quot;mm&quot;) + # xlab(&#39;Date&#39;) + # scale_y_continuous(sec.axis = sec_axis(trans = ~ . / 10000, name = expression(Q[paste(m^3,&#39;/second&#39;)]))) #print(max(sntl$cum_input.mm, na.rm = TRUE)) #print(max(sntl$pcumul.mm, na.rm = TRUE )) Question 3. What processes could account for the differences between cumulative input, calculated from SWE data, and cumulative precipitation, both measured at the same SNOTEL station? (1 or 2 processes) What could explain the discrepancy if the cumulative input, is found to be less than the cumulative precipitation measured at the same SNOTEL station? (at least 2 possible processes) (4 points) (2-3 sentences) ANSWER: 9.7 Modeling SWE Let’s assume that we have a precipitation gage at 3400m in Fool Creek, but without SWE data, how could we model SWE using temperature? # # Filter to a single water year for the snowmelt model # wateryr = &#39;2021&#39; # # # set the year # sntl_SINGLE_YEAR &lt;- sntl %&gt;% # filter(wtr_yr == wateryr) # # print(sum(sntl_SINGLE_YEAR$input.mm)) The model below is a variation of the degree-day method. Here we are running a similar script to the input calculations above, using temperature as the primary determinant. When the temperature falls below a certain threshold and precipitation occurs, an equivalent amount is added to our SWE accumulation. We’ll conduct a simulation with 100 Monte Carlo runs and analyze the optimal outcome. The parameters we are testing are pack threshold, a degree-day factor, and the threshold temperature for determining rain from snow. # ######################################## # # nx sets the number of Monte Carlo runs # nx &lt;- 100 # number of runs # # # set up the parameter matrix # param &lt;- tibble( # Run = vector(mode = &quot;double&quot;, nx), # ddf = vector(mode = &quot;double&quot;, nx), # Tb = vector(mode = &quot;double&quot;, nx), # pack_threshold = vector(mode = &quot;double&quot;, nx), # nse = vector(mode = &quot;double&quot;, nx), # kge = vector(mode = &quot;double&quot;, nx), # SWEsim = vector(mode = &quot;list&quot;, nx) # ) # # tic(&quot;MC Simulation&quot;) # pb &lt;- progress_bar$new( # format = &quot; downloading [:bar] :percent eta: :eta&quot;, # total = nx, clear = FALSE, width = 60 # ) # initialize progress bar # # #### MONTE CARLO SETUP (use ii as index) # for (n in 1:nx) { # pb$tick() # needed for progress bar # # # write run number into the parameter matrix # param$Run[n] &lt;- n # # ##################### # # initiate parameters # param$ddf[n] &lt;- runif(1, 0, 5) # Threshold temperature for distinguishing rain from snow. # param$Tb[n] &lt;- runif(1, -5, 5) # Degree-day factor for snowmelt calculations, utilized in calculating the potential melt (Melt) based on the difference between the mean temperature (temp_mean.c) and Tb. # param$pack_threshold[n] &lt;- runif(1, 0, 100) #threshold value used in determining whether precipitation falls as snow or accumulates on existing snowpack # # ######################################## # # generate columns with snow, rain, melt # SWE.Model &lt;- sntl_SINGLE_YEAR %&gt;% # # liquid precipitation # mutate(Rain = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &gt;= param$Tb[n], Pdiff.mm, 0)) %&gt;% # # snow accumulation # mutate(Snow = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &lt; param$Tb[n], Pdiff.mm, 0)) %&gt;% # # potential melt # mutate(Melt = if_else(temp_mean.c &gt; param$Tb[n], param$ddf[n]*(temp_mean.c - param$Tb[n]), 0)) %&gt;% # # cumulative SWE without melt # mutate(SWE.cumul = cumsum(Snow)) %&gt;% # # no melt if swe &lt;= 0 (here only for first few steps) # mutate(Melt = if_else(SWE.cumul == 0, 0, Melt)) # # # ######################################## # # for loop to loop through each timestep # SWEsim &lt;- vector() # SWEsim[1] &lt;- 0 # set first value to 0 # for (ii in 2:dim(SWE.Model)[1]) { # # # snow accumulation if snowfall happens # SWEsim[ii] &lt;- if_else(SWE.Model$Snow[ii] &gt; 0, SWE.Model$Snow[ii]+SWEsim[ii-1], # # #This condition checks if the existing snow water equivalent (SWE) surpasses the pack threshold. If so, then the precipitation is considered to contribute to the snowpack rather than falling as rain # # snow accumulation if liquid precip but snow on the ground # if_else(SWE.Model$Rain[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; param$pack_threshold[n], SWE.Model$Rain[ii]+SWEsim[ii-1], # # # melt if SWEsim on the ground larger than the melt pulse # if_else(SWE.Model$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; SWE.Model$Melt[ii], SWEsim[ii-1] - SWE.Model$Melt[ii], # # # melt if SWEsim is present but smaller than melt pulse # if_else(SWE.Model$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &lt; SWE.Model$Melt[ii], 0, SWEsim[ii-1])))) # } # # # store simulated SWEsim in df # param$SWEsim[n] &lt;- list(SWEsim) # # # #################### # # We will store efficiencies comparing observed SWE with simulated SWE. # # NSE # param$nse[n] &lt;- 1 - ((sum((SWE.Model$swe.mm - SWEsim)^2)) / sum(((SWE.Model$swe.mm - mean(SWE.Model$swe.mm))^2))) # # # KGE # kge_r &lt;- cor(SWE.Model$swe.mm, SWEsim) # kge_beta &lt;- mean(SWEsim) / mean(SWE.Model$swe.mm) # kge_gamma &lt;- (sd(SWEsim) / mean(SWEsim)) / (sd(SWE.Model$swe.mm) / mean(SWE.Model$swe.mm)) # param$kge[n] &lt;- 1 - sqrt((kge_r - 1)^2 + (kge_beta - 1)^2 + (kge_gamma - 1)^2) # # # } # toc() # # ######################################### # # sort parameter matrix from highest to lowest NSE and save it # param &lt;- arrange(param, desc(nse)) # save(param, file = paste(wateryr, &quot;SWE.Model.Runs.RData&quot;, sep = &quot;.&quot;)) # # # best run # # take the best run, get the list of simulated values and unlist it in order to store it in a dataframe # SWE.Model$SWEsim.mm &lt;- unlist(param$SWEsim[1]) # # Plot best run # swe_plot &lt;- ggplot(data = SWE.Model) + # geom_line(aes(x = date, y = SWEsim.mm, color = &quot;Simulated SWE&quot;)) + # geom_line(aes(x = date, y = swe.mm, color = &quot;Actual SWE&quot;)) + # annotate(&quot;text&quot;, x = mean(SWE.Model$date, na.rm = TRUE), y = min(SWE.Model$swe.mm, na.rm = TRUE), # label = paste(&quot;NSE =&quot;, round(param$nse[1], 2)), hjust = 1, vjust = 1) # # ggplotly(swe_plot) This simple model seems to simulate SWE well if assessed with the NSE. Let’s model SWE at the Fool Creek outlet, where we do not have daily SWE data. First we’ll bring in precipitation data, measured at a USFS maintained meteorological station located near the Fool outlet: # fc_precip &lt;- read.csv(&quot;LFCmet_WY21_WY22.csv&quot;) %&gt;% # mutate(date = lubridate::mdy_hm(TS)) %&gt;% # select(date, water_year, PPT_10min..mm.) %&gt;% # filter(water_year %in% c(2021, 2022)) We also need temperature data. This is also collected at the USFS Lower Fool meteorological station. # fc_temp &lt;- read.csv(&quot;lfc_airtemp.csv&quot;) %&gt;% # mutate(date = lubridate::mdy_hm(TMSTAMP)) %&gt;% # mutate(wtr_yr = if_else(month(date) &lt; 10, year(date), year(date) + 1)) %&gt;% # select(date, Air_TempC_Avg, wtr_yr) %&gt;% # filter(wtr_yr %in% c(2021, 2022)) # # lfc_data &lt;- merge(fc_temp, fc_precip, by = &quot;date&quot;, validate = &quot;one_to_one&quot;) %&gt;% # select(date, Air_TempC_Avg, wtr_yr, PPT_10min..mm.) %&gt;% # rename(precip.mm = PPT_10min..mm., # temp_mean.c = Air_TempC_Avg) # # rename columns so we can reuse the SWE model without changing variable names within the simulation code # # # Lets add cumulative precip totals by water year # lfc_data &lt;- lfc_data %&gt;% # group_by(wtr_yr) %&gt;% # mutate(pcumul.mm = cumsum(precip.mm)) %&gt;% # generate a cumulative sum column for each water year # ungroup() # # # Aggregate all data to daily means for the sake of simulation time # lfc_data &lt;- lfc_data %&gt;% # group_by(date = as.Date(date)) %&gt;% # summarise(across(.cols = everything(), .fns = mean, na.rm = TRUE)) Even though this meteorological station is very close to the SNOTEL station, lets compare the values between the upper watershed and the lower # # Convert date column in sntl to Date format that matches lfc_data # sntl$date &lt;- as.Date(sntl$date) # # ggplot() + # # Scale discharge for the right y-axis # geom_point(data = lfc_data, aes(x = date, y = pcumul.mm, color = &#39;lower fool creek precip (mm)&#39;)) + # geom_point(data = sntl, aes(x = date, y = pcumul.mm, color = &#39;upper fool creek precip (mm)&#39;)) + # theme(axis.text.x = element_text(angle = 45, hjust = 1), # Rotate x-axis # panel.grid.minor = element_blank(), # Remove minor grid lines # ) + # ylab(&quot;mm&quot;) + # xlab(&#39;Date&#39;) Now let’s estimate SWE for the Lower Fool Creek. Again, we’ll just simulate a single year. # # Filter to a single water year for the snowmelt model # wateryr = &#39;2021&#39; # # # aggregate data to daily means for the sake of simulation time # lfc_data &lt;- lfc_data %&gt;% # mutate(Pdiff.mm = pcumul.mm - lag(pcumul.mm, default = first(pcumul.mm))) # # # set the year # lfc_SINGLE_YEAR &lt;- lfc_data %&gt;% # filter(wtr_yr == wateryr) Run another set of simulations and compare the values of the best performing parameters across sites. # ######################################## # # nx sets the number of Monte Carlo runs # nx &lt;- 100 # number of runs # # # set up the parameter matrix # param &lt;- tibble( # Run = vector(mode = &quot;double&quot;, nx), # ddf = vector(mode = &quot;double&quot;, nx), # Tb = vector(mode = &quot;double&quot;, nx), # pack_threshold = vector(mode = &quot;double&quot;, nx), # nse = vector(mode = &quot;double&quot;, nx), # kge = vector(mode = &quot;double&quot;, nx), # SWEsim = vector(mode = &quot;list&quot;, nx) # ) # # tic(&quot;MC Simulation&quot;) # pb &lt;- progress_bar$new( # format = &quot; downloading [:bar] :percent eta: :eta&quot;, # total = nx, clear = FALSE, width = 60 # ) # initialize progress bar # # #### MONTE CARLO SETUP (use ii as index) # for (n in 1:nx) { # pb$tick() # needed for progress bar # # # write run number into the parameter matrix # param$Run[n] &lt;- n # # ##################### # # initiate parameters # param$ddf[n] &lt;- runif(1, 0, 5) # Threshold temperature for distinguishing rain from snow. # param$Tb[n] &lt;- runif(1, -5, 5) # Degree-day factor for snowmelt calculations, utilized in calculating the potential melt (Melt) based on the difference between the mean temperature (temp_mean.c) and Tb. # param$pack_threshold[n] &lt;- runif(1, 0, 100) #threshold value used in determining whether precipitation falls as snow or accumulates on existing snowpack # # ######################################## # # generate columns with snow, rain, melt # SWE.Modellfc &lt;- lfc_SINGLE_YEAR %&gt;% # # liquid precipitation # mutate(Rain = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &gt;= param$Tb[n], Pdiff.mm, 0)) %&gt;% # # snow accumulation # mutate(Snow = if_else(Pdiff.mm &gt; 0 &amp; temp_mean.c &lt; param$Tb[n], Pdiff.mm, 0)) %&gt;% # # potential melt # mutate(Melt = if_else(temp_mean.c &gt; param$Tb[n], param$ddf[n]*(temp_mean.c - param$Tb[n]), 0)) %&gt;% # # cumulative SWE without melt # mutate(SWE.cumul = cumsum(Snow)) %&gt;% # # no melt if swe &lt;= 0 (here only for first few steps) # mutate(Melt = if_else(SWE.cumul == 0, 0, Melt)) # # # ######################################## # # for loop to loop through each timestep # SWEsim &lt;- vector() # SWEsim[1] &lt;- 0 # set first value to 0 # for (ii in 2:dim(SWE.Modellfc)[1]) { # # # snow accumulation if snowfall happens # SWEsim[ii] &lt;- if_else(SWE.Modellfc$Snow[ii] &gt; 0, SWE.Modellfc$Snow[ii]+SWEsim[ii-1], # # # snow accumulation if liquid precip but snow on the ground # if_else(SWE.Modellfc$Rain[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; param$pack_threshold[n], SWE.Model$Rain[ii]+SWEsim[ii-1], # # # melt if SWEsim on the ground larger than the melt pulse # if_else(SWE.Modellfc$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &gt; SWE.Modellfc$Melt[ii], SWEsim[ii-1] - SWE.Modellfc$Melt[ii], # # # melt if SWEsim is present but smaller than melt pulse # if_else(SWE.Modellfc$Melt[ii] &gt; 0 &amp; SWEsim[ii-1] &lt; SWE.Modellfc$Melt[ii], 0, SWEsim[ii-1])))) # } # # # store simulated SWEsim in df # param$SWEsim[n] &lt;- list(SWEsim) # # # #################### # # We will store efficiencies comparing observed SWE with simulated SWE. # # NSE # param$nse[n] &lt;- 1 - ((sum((SWE.Model$swe.mm - SWEsim)^2)) / sum(((SWE.Model$swe.mm - mean(SWE.Model$swe.mm))^2))) # # # KGE # kge_r &lt;- cor(SWE.Model$swe.mm, SWEsim) # kge_beta &lt;- mean(SWEsim) / mean(SWE.Model$swe.mm) # kge_gamma &lt;- (sd(SWEsim) / mean(SWEsim)) / (sd(SWE.Model$swe.mm) / mean(SWE.Model$swe.mm)) # param$kge[n] &lt;- 1 - sqrt((kge_r - 1)^2 + (kge_beta - 1)^2 + (kge_gamma - 1)^2) # # # } # toc() # # ######################################### # # sort parameter matrix from highest to lowest NSE and save it # param_lfc &lt;- arrange(param, desc(nse)) # # Can save simulated data if desired # #save(param_lfc, file = paste(wateryr, &quot;SWE_lfc.Model.Runs.RData&quot;, sep = &quot;.&quot;)) # # # # take the best run, get the list of simulated values and unlist it in order to store it in a dataframe # SWE.Modellfc$SWEsim.mm &lt;- unlist(param_lfc$SWEsim[1]) Since we don’t have SWE measurement for Lower Fool Creek, let’s see how the simulated values compare to acutal Upper Fool SNOTEL SWE measurements. # SWE.Model$date &lt;- as.Date(SWE.Model$date) # # Plot best run # swe_plot &lt;- ggplot() + # geom_line(data = SWE.Modellfc, aes(x = date, y = SWEsim.mm, color = &quot;Simulated SWE&quot;)) + # geom_line(data = SWE.Model, aes(x = date, y = swe.mm, color = &quot;Actual SWE&quot;)) + # annotate(&quot;text&quot;, x = mean(SWE.Model$date, na.rm = TRUE), y = min(SWE.Model$swe.mm, na.rm = TRUE), # label = paste(&quot;NSE =&quot;, round(param$nse[1], 2)), hjust = 1, vjust = 1) # # ggplotly(swe_plot) # compare the best performing parameters for the Upper Fool Creek and Lower Fool Creek # print(param[1, ]) # print(param_lfc[1, ]) # # # You can change the indexing and wording in this statement if you want to explore all parameters. # print(paste(&#39;The estimated threshold temperature distinguishing rain from snow in Lower Fool is &#39;, round(param_lfc[1, 2], 2), &#39;degrees and in Upper Fool Creek is&#39;, round(param[1, 2],2), &#39;degrees&#39;)) Question 3: Based on the differences in precipitation and temperature totals between the two sites, are SWE simulation results as you expected? (1 sentence)(2pnts) ANSWER: Question 4: How does the estimated threshold temperature distinguishing rain from snow change with elevation? Is the relationship between your parameter estimations as expected? How does this influence your confidence in the model? (2pnts) ANSWER Question 4: Compare the precipitation and snow accumulation patterns between two sites. Discuss at least 3 potential natural and/or anthropogenic mechanisms driving the observed variations in precipitation and snow accumulation. Consider the factors you observed in question 1 (3 points)(2-3 sentences). ANSWER: Question 5: Let’s say you want to model runoff for the Fool Creek catchment identified above. To do that, you want to estimate the timing and volume of snowmelt for this watershed. Refer to at least 2 model types reviewed in the 06_model_classifications lecture or terms in the lecture above and qualitatively describe a model that you could use to capture the spatial heterogeneity of snow accumulation in this watershed (4 points) (2-4 sentences). ANSWER: "],["learning-module-10---gridded-and-tabular-data-retrieval.html", "Chapter 10 Learning Module 10 - Gridded and tabular data retrieval 10.1 Objectives 10.2 Background 10.3 Codework", " Chapter 10 Learning Module 10 - Gridded and tabular data retrieval 15 pnts The repo for this module can be found here 10.1 Objectives In this module you will: -Retrieve flow data using the dataretrieval package -Learn to retrieve and manage digital elevation models -Retrieve and import remotely sensed data for the watershed extent 10.2 Background 10.2.1 A refresher in GIS concepts: This module assumes familiarity with basic Geographic Information System (GIS) such as coordinate systems (latitude and lognitude, UTM). Some terms to recall: Gridded data refers to spatial data in raster format. If you have performed any GIS work, you are likely using gridded data. Rasters organize geographical areas into a grid where each regularly spaced cell represents a specific location and contains some attribute representing that location. For example, digital elevation models (DEM) are a form of gridded data. Imagine that we are working in a watershed and we can drop a grid or square celled net over the entire catchment. We can then model the elevation data of that catchment by assigning each cell in the net a mean elevation value for the area represented by the cell. Similarly, we can assign any value we wish to model to the cells. If we were to drop another net and assign a maximum tree height to every cell, we would be adding another ‘layer’ to our model. The spatial resolution of gridded data refers to the length and width of each cell represents. If we want to observe gridded data over time, we can generate a raster stack, where each raster layer represents a point in time. With a raster stack, we can increase the temporal resolution of our gridded data. Vector data describes spatial features using points, lines, and polygons. These spatial features are represented a geometrical objects with associated attributes, like lines to represent linear features such as rivers, or points to represent single locations. We will work with a .shp file in this module that is a collection of points representing the boundary of a watershed. Tabular data refers to data organized in a row-column format. In this class, each of our .csv datasets are a form of tabular data. 10.2.2 A refresher in remote sensing: Remote sensing is the science of obtaining information about objects or areas from a distance, typically from aircraft or satellites. It involves the collection and interpretation of data gathered by sensors that detect electromagnetic radiation reflected or emitted from the Earth’s surface and atmosphere. As ecologists, many of us utilize remote sensing to observe and analyze the Earth’s surface and its dynamics, providing valuable insights into environmental processes, land use and land cover changes, natural disasters, and more. Some helpful terms to recall: spatial resolution: Remotely sensed data can vary in its spatial resolution, which refers to the scale of the smallest unit, or level of visual detail. temporal resolution: When used to describe remotely sensed data, we are often talking about how frequently a remote sensing system revisits the same area on the Earth surface. Systems with high temporal resolution revisit the same location frequently, allowing for monitoring rapid changes in phenomena, where a lower resolution may limit a system’s ability to capture short-term events. We often find that data collection requires a balance of spatial and temporal resolution to select the most appropriate data for the coverage and detail that ecological work requires. 10.2.3 Ethical conduct When utilizing publicly available datasets, it is important for us as users to: -Be aware of any copyright or licensing agreements. There may be restrictions or usage terms imposed by data providers. Read provided agreements or terms to be sure we are in compliance with these when using datasets in our research. This may also include requirements for registration or subscription established by providers or agencies like USGS or NASA. -Cite and acknowledge the contributions of data providers and researchers whose datasets we utilize. This also facilitates transparency and reproducibility in our research. - Assess the quality and accuracy of geospatial data before using it for decision making purposes. It is important that we consider spatial resolution, temporal resolution and metadata documentation to critically evaluate the reliability of the datasets we use. -Promote Open Data Principles: Try to explore and use publicly available datasets that support transparency and collaboration within our scientific and academic communities. 10.3 Codework In this module we will import streamflow (tablular) data, using USGS’s DataRetrieval package, then visualize vector data to delineate our area of interest and learn how to find spatial gridded data for the area. 10.3.1 Setup: When importing packages and setting parameters, think about how the local working environment can vary among users and computers. A function that examines and installs packages before importing them enhances the script’s ability to be easily used by others. 10.3.2 Selecting a watershed: Here, we are using USGS’ dataRetrieval package to search and filter hydrologic datasets. There are many ways data can be searched; more can be found in this dataRetrieval vingette. In this example, we will fetch USGS site numbers for Colorado watersheds with streamflow (discharge) data for a specified time period. # Start by defining and storing your search parameters as variables at the beginning of the script. This practice enables you and other users to easily modify the search parameters from a single location within the script. # stateCd &lt;- &quot;CO&quot; # Colorado # parameterCd &lt;- &quot;00060&quot; # Discharge (cubic feet per second) # start_date &lt;- &quot;2023-01-01&quot; # end_date &lt;- &quot;2023-12-31&quot; # gage_no &lt;- &#39;401733105392404&#39; #&#39;05015500&#39; #&quot;05017500&quot; # # # sites &lt;- whatNWISsites(stateCd = stateCd, parameterCd = parameterCd) Q1 (2pnts). If you were creating this script for your work team’s shared collaboration, or for yourself to use again a year from now, what might be the disadvantages of embedding search parameters directly into the function parameters? ANSWER: Q2 (2pnts). What is a potential use for whatNWISsites? ANSWER: EXTRA (1pnt): Why might the above script return an error if dpylr:: is not placed before select()? ANSWER: # # Now we will request more data from these sites by looping through the list we just generated. This chunk will take several minutes. Feel free to comment this out if needed once you have executed it once. # # # Initialize an empty dataframe to store the results from each list item # all_site_data &lt;- data.frame() # # for (site in sites$site_no) { #for every value in site_no column of sites # siteINFO &lt;- readNWISsite(site) %&gt;% #read the site data # # filter by drainage area # filter(drain_area_va &lt; 300 &amp; drain_area_va &gt; 250) %&gt;% # # select a subset of potential columns # dplyr::select(&#39;agency_cd&#39;, &quot;site_no&quot;,&#39;station_nm&#39;, &quot;lat_va&quot;,&quot;long_va&quot;, &quot;drain_area_va&quot;) # # # Add the siteINFO to the all_site_data dataframe # all_site_data &lt;- bind_rows(all_site_data, siteINFO) # } # # # Note that the results of each iteration of the loop need to be stored by adding them to the empty dataframe before the &#39;for&#39; loop. If your result contains only one iteration worth of data, you may have forgotten this step. drain_area_va is a column value returned by readNWISsite. Other variables can be found here Q3 (1pnt). In a sentence, what does this dataframe (all_site_data) tell you? ANSWER: Note that while your research interests may not require streamflow data, it can serve as a valuable resource for a wide array of data types. For the remainder of this assignment, we’ll focus on a single watershed in central Colorado near the Experimental Forest that we explored in the Snowmelt module. There are several dataRetrieval functions could we utilize to see what data is available for a particular site number. Check dataRetrieval docs to learn package functions or tools. # Check data availability for the gage_no of interest # dataavailable &lt;- whatNWISdata(siteNumber = BLANK, service=&quot;all&quot;) Q4 (4pnts). Find the longest running dataset in DataAvailable (i.e. either by count or by start and end date). Use https://help.waterdata.usgs.gov to look up the meaning of some of column headers and the codes associated with them. a. What kind of data does this long running dataset hold? b. What column did you use to learn that? c. What does ‘dv’ mean in data_type_cd? ANSWER: 10.3.3 Flow data retrieval: # Can change filter() arguments to use as you need # CHECK UNITS of data imported: # parameterCdFile %&gt;% # filter(srsname == &#39;Precipitation&#39;, parameter_cd ==&#39;00045&#39;) # #import - determine years of burn # gage_no &lt;- &#39;09032100&#39; # # # Raw daily data: # rawQData &lt;- readNWISdv(gage_no, parameterCd, # &quot;2014-01-01&quot;, &quot;2024-01-01&quot;) Let’s rename our columns. For this you will need to determine the units of the date # rawQData &lt;- rawQData %&gt;% # rename (&#39;BLANK&#39; = &#39;X_00060_00003&#39;) %&gt;% # # the X_00060_00003_cd column provides information about the data quality. It is good to check this to anticipate or understand potential data anomalies. We have checked this and determined that we no longer need it for this exercise: # dplyr::select(-c(&#39;X_00060_00003_cd&#39;, &#39;agency_cd&#39;)) # It is good to visualize your data upon import: # # ggplot(rawQData, aes(x = Date, y = BLANK)) + # geom_point() + # scale_x_date(date_labels = &quot;%b %Y&quot;, date_breaks = &quot;3 month&quot;) + # theme(axis.text.x = element_text(angle = BLANK, hjust = 1)) Q5 (2pnts). Based on the hydrograph plot, what type of precipitation events appear to influence the observed intra-annual variations in flow rates? ANSWER: 10.3.4 Watershed boundary vector data: A boundary shapefile has been provided to save time and improve repeatability. Watershed boundaries for gauging stations can be found at USGS StreamStats by zooming into the water map and clicking ‘Delineate’ near your desired gage or point. Once your watershed is delineated, you can click on ‘Download Basin’ and select from a choice of file types. This can be a great to to delineate river basins if all you require is a shapefile. For smaller streams, other analyses are available. We will cover those in the next module. Let’s import a pre-created shapefile: # import_path &lt;- paste(getwd(), &#39;/cabin_creek_shp/globalwatershed.shp&#39;, sep=&#39;&#39;) # # # Read the shapefile # ws_boundary &lt;- st_read(import_path) # Check the coordinate reference system (CRS) of the shapefile # print(st_crs(ws_boundary)) Let’s transform this to a projected CRS (flat surface) rather than a geographic (spherical surface) CRS. A projected CRS will depict the data on a flat surface which is required for distance or area measurements. A more detailed explanation can be found at the University of Colorado’s Earth Lab if needed. # # Use the EPSG code for UTM Zone 12. Choose a UTM or other code appropriate for the area of interest. # new_crs &lt;- st_crs(32613) # Use the appropriate EPSG code for your CRS # ws_boundary &lt;- st_transform(ws_boundary, crs = BLANK) # # # We will export the transformed shapefile for use in the MODIS gui later # st_write(ws_boundary, &quot;cabin_creek_shp_projected/cabin_creek_projected.shp&quot;, append=FALSE) 10.3.4.1 Visualization - watershed boundary i.e., sanity check # # Let&#39;s look at the shapefile with a basic leaflet map. Leaflet can be a nice way to see your data with on a basemap for geographic context. # # # Transform the shapefile to long-lat for visualization # ws_boundary_longlat &lt;- st_transform(ws_boundary, crs = st_crs(&quot;EPSG:4326&quot;)) # # # Create the Leaflet map # map &lt;- leaflet() %&gt;% # setView(lng = -105.7, lat = 40.0, zoom = 13) %&gt;% # addProviderTiles(&quot;Esri.WorldImagery&quot;) %&gt;% # addPolygons(data = ws_boundary_longlat, fillOpacity = 0.7, layerId = &quot;my_layer&quot;) # # # Print or display the map # map 10.3.5 Digital Elevation Models (DEM) A DEM has been provided for consistency among users. However, if you are unfamiliar with the process of retrieving a DEM here are three methods that our team commonly uses: USDA’s Natural Resources Conservation Science (NRCS) website. Here you can order (free) DEMs by state, county or bounding box. It takes just minutes for them to be emailed to you. If you have a USGS account (fast and free) you can also find DEM data at https://earthexplorer.usgs.gov. See this video if interested. This is the method we used to provide this DEM and we will use this resource again to retrieve Landsat data. GIS or hydrological software like QGIS and SAGA offer tools for downloading DEMs directly into your project for analysis. This can be optimal when working over large geographic extents to streamline the data acquisition process. Both QGIS and SAGA are free and easy to learn if you have experience with ESRI products. Let’s check out our DEM: # # Import the pre-downloaded DEMs. Note that we need two files to obtain a complete DEM for our watershed. # dem1 &lt;- raster(file.path(&#39;cabin_creek_dems_raw/n39_w106_1arc_v3.tif&#39;)) # dem2 &lt;- raster(file.path(&#39;cabin_creek_dems_raw/n40_w106_1arc_v3.tif&#39;)) # # # Check that the DEMs align. If retrieving them from different sources, you can also check that the projections are the same with proj4string() before mosaicing # origin(dem2) &lt;- origin(dem1) # # # Mosaic the rasters # dem &lt;- mosaic(dem1, dem2, fun = mean) # fun indicates the treatment of overlapping cells # # # Project the raster to match the ws_boundary projection # dem_projected &lt;- projectRaster(dem, crs = crs(BLANK)) # # # Check # plot(dem_projected) # plot(ws_boundary, add = TRUE) If the watershed and raster don’t overlap as you expect, be sure the projection and extent is the same for all data files plotted. # # Clip the dem to the watershed. This will change all elevation values outside of the watershed to &#39;NA&#39; # dem_clipped &lt;- mask(dem_projected, ws_boundary) # # # Remove rows and columns with only &#39;NA&#39; values # dem_trimmed &lt;- trim(dem_clipped) # # # Save the trimmed raster to a new file or use it for further analysis # writeRaster(dem_trimmed, &quot;trimmed_dem.tif&quot;, format = &quot;GTiff&quot;, overwrite=TRUE) # # plot(dem_trimmed) 10.3.6 Other remotely sensed data: Now that we have site data spatial data, let’s cover a couple of techniques or resources you can use to access satellite data for your site. In the event that you only need a vegetative map layer from a specific point in time, it is not too difficult to download actual Landsat imagery from usgs.gov. In the case of Cabin Creek let’s look at a single summer image from 2020. Navigate to https://ers.cr.usgs.gov/, and if you don’t already have an account, you can create one quickly or sign in if you already have one. Once signed in, go to https://earthexplorer.usgs.gov you should see a surface map with tabs on the left. Start with the ‘Search Criteria’ tab. For this assignment the shape file we have is too large to upload, so it is easiest to scroll down and choose ‘polygon’ or ‘circle’ to define your area. Under the circle tab you can use Center Latitude 39.9861 Center Longitude -105.719 and a radius of about 3.5 kilometers to capture our study area. Let’s say we want an NDVI map layer from summer of 2020. In ‘date range’, select dates for the summer of 2020. In this high elevation area of Colorado, the greenest times of the year without snow cover are likely June and July with senescence beginning sometime in late July. First search from June 1 to July 15 2020. Then click on ‘Data Sets’ either at the bottom or top of the left-hand side menus. From the data sets we will select Landsat Collection 2 level 2, Landsat 8 - 9 OLI TIRS C2L2. We can skip ‘Additional Criteria’ for now and go to ‘Results’. You should receive a set of about 11 images. For each image you can check out the metadata. That is the ‘pen and notepad’ icon in the middle of image specific menu. Here you can look at data like cloud cover. For these images, the values you see are a cloud coverage percent. Cloud coverage measurements and the codes to filter for certain cloud cover thresholds may change across missions. If you are retrieving Landsat data for time series using code, it will be important to explore the different quality control parameters and what they represent. You don’t need to download anything for this assignment, we’ve provided it for you. However to gain familiarity with this process, click on the ‘download’ icon for July 7, 2020 and click ‘select files’ on the Reflectance Bands option. Q6 (1pnt): What 2 bands should we download to calculate NDVI? Answer: Check out other earthexplorer resources while you are at the site. You will likely find resources here for your MS projects, even if it is just for making maps of your study area for presentations or paper. 10.3.6.0.1 Landsat If you want to learn about Landsat missions or utilize Landsat data, USGS offers all of the reference information you need. The sensors on these missions can be a fantastic resource if characterizing land or water surface over large temporal scales. Landsat data provides (30m) spatial resolution so can capture spatial heterogeneity, even for smaller study areas. If calculating vegetation indices over different time periods, it is important to know which mission you are retrieving data from and check the band assignments for that mission. For example, on the Landsat 6 instrument, Band 4 is near-infrared, whereas Band 4 on Landsat 9’s instrument is red. 10.3.6.0.2 Moderate Resolution Imaging Spectroradiometer (MODIS): is a sensor aboard NASA’s Terra and Aqua satellites which orbit Earth and collect data on a daily basis. MODIS data provides moderate spatial resolution (250m to 1km) and high temporal resolution. Therefore, MODIS data can be an optimal way to monitor or characterize rapid changes to Earth’s surface like land cover changes resulting from wildfire. Data can be accessed and downloaded directly through NASA’s Earthdata tool. Let’s import our raster data. # # Import the two bands needed for NDVI calculation # red_band &lt;- raster(&quot;landsat/LC08_L2SP_034032_20200702_20200913_02_T1_SR_B4.TIF&quot;) # nir_band &lt;- raster(&quot;landsat/LC08_L2SP_034032_20200702_20200913_02_T1_SR_B5.TIF&quot;) # # # Note that the file name tells us about the data. # # # Calculate NDVI # ndvi &lt;- (BLANK - BLANK) / (BLANK + red_band) # # # Project the NDVI layer so that the coordinate reference system matches that of the watershed boundary shapefile. # ndvi_projected &lt;- projectRaster(ndvi, crs = crs(ws_boundary)) # # # Crop NDVI raster to the extent of ws_boundary. # ndvi_crop &lt;- crop(ndvi_projected, ws_boundary) # # # Plot NDVI # plot(ndvi_crop, col = rev(rainbow(255))) # # # Plot ws_boundary as an outline # plot(ws_boundary, add = TRUE, border = &quot;black&quot;, lwd = 2, col = &quot;transparent&quot;) Q7 (1pnt) What does ‘SR’ in the .tif names tell us about the data? ANSWER: Q8 (2pnts) What is the difference between ‘mask’ and ‘crop’ when filtering large files to fit the area of interest? ANSWER: 10.3.6.1 Earth surface changes over time If considering changes over time, comparing individual rasters from different time periods directly may not always provide meaningful insights because the absolute values of vegetation indices can vary due to factors such as differences in solar angle, atmospheric conditions, and land cover changes. Instead, it’s often more informative to analyze trends in vegetation over time. 10.3.6.1.1 LandTrendr One fun way to do this is with Oregon State University’s LandTendr. With a Google account and access to Earth Engine, you can easily interact with the UI Applications in section 8. You can jump to the pixel timeseries here. This will allow you to view changes in vegetation indices over a specified time period within a single Landsat pixel. 10.3.6.1.2 Google Earth Engine Another effective way to access evaluate large spatial datasets is through Google Earth Engine (GEE). GEE is a cloud-based platform developed by Google for large scale environmental data analysis. It provides a massive archive of satellite imagery and other geospatial datasets, as well as computational tools to view and analyze them. GEE uses Google’s cloud to provide processing of data so you don’t need powerful computing ability to run a potentially computationally expensive spatial analysis. 10.3.6.1.2.1 GEE Access If spatial data is something you may need for your MS project, we recommend exploring this resource Sign Up: Go to the Google Earth Engine website (https://earthengine.google.com/) and sign in with your Google account. If you don’t have a Google account, you can sign up for access by filling out a request form. Request Access: If you’re signing up for the first time, you’ll need to request access to Google Earth Engine. Provide information about your affiliation, research interests, and how you plan to use Earth Engine (learn and access data!). Approval: After submitting your request, you’ll need to wait for approval from the Google Earth Engine team. Approval times may vary, but you should receive an email notification once your request has been approved. Access Earth Engine: Once approved, you can access Google Earth Engine through the Code Editor interface in your web browser at code.earthengine.google.com to start exploring the available datasets, running analyses, and visualizing your results. We have provided a basic script for retrieving and viewing NDVI data. This is a code snapshot, so you can manually copy the code from the snapshot and paste it into you own Google Earth Engine Code editor. You are welcome to save this to your own GEE files and try changing the area of interest, vegetation indices, time periods or exploring other data. Colorado State University also has a great tutorial here that will start you from scratch. Note that JavaScript is the default language for the code editor interface in your web browser. If you are familiar with Python, GEE provides a Python API that can allow you to use Python syntax. If neither of these languages are familiar, don’t give up yet! You can still use this tool with your R coding background and trial and error. We recommend finding other scripts that suit your need (there are a lot of shared resources online) and adapting them to your area of interest. Once you understand coding concepts, like those in R, other languages can be picked up easily. You will find that you are learning JavaScript before you realize it. "],["dem-processing.html", "Chapter 11 DEM processing 11.1 THE LINK TO THE REPO IS HERE 11.2 Objective: 11.3 Background: 11.4 Codework", " Chapter 11 DEM processing 11.1 THE LINK TO THE REPO IS HERE 15pnts 11.2 Objective: Users will explore basic hydrological tools through the process of DEM preprocessing and defining stream networks for a watershed in the Fraser Experimental Forest in Colorado using Whitebox Tools for R. This exercise will also demonstrate methods for writing functions and efficiently handling multiple files simultaneously, including importing, processing, and exporting data within the context of Whitebox Tools for R. 11.3 Background: Hydrological analysis preprocessing involves the use of digital elevation model (DEM) raster data to establish a watershed model and a simulation of surface hydrological processes. These steps enable us to quantify key parameters such as flow accumulation, stream network characteristics, and hydrological connectivity, which are essential for studying the dynamics of water movement within a landscape. Overall, preprocessing is the set of foundational steps in hydrological modeling and analysis. Whitebox Tools is an advanced geospatial data analysis platform that can be used to perform common geographical information systems (GIS) analysis operations. This platform was developed with the Center for Hydrogeomatics in Guelph University so it is focused on hydrological analysis. With just a DEM, it allows us to produce a multitude of outputs that can be used for future analysis (Lindsay, 2016) doi:10.1016/j.cageo.2016.07.003). While we are demonstrating its use in R, these tools are also available in QGIS and Python platforms. 11.4 Codework 11.4.1 Installing libraries We are going to try installing the whitebox R package from CRAN as it should be the simplest method. However, if this does not work for you, you can install the development version from GitHub by putting this inside a code chunk: if (!require(“remotes”)) install.packages(‘remotes’) remotes::install_github(“opengeos/whiteboxR”, build = FALSE) More information on installation can be found at: https://cran.r-project.org/web/packages/whitebox/readme/README.html Helpful whitebox documentation can be found at https://jblindsay.github.io/wbt_book/preface.html. Essentially, we will be using input rasters via filepath and output filepaths as arguments for various whitebox functions. The script is designed to perform functions on all rasters in a given folder at once. When writing scripts, developers typically follow a standard workflow: 1. Import required libraries 2. Generate functions useful throughout the script 3. Establish working directories or paths to other directories if needed 4. Import data 5. Data Cleaning and Preprocessing - this may involve handling missing values, removing outlines, converting to preferred units. etc. 6. Exploratory Data Analysis - it is beneficial to explore data visually to help uunderstand the characteristics of the data. 7. Apply functions, or models or other analytical techniques 8. Evaluate results - If modeling, this may involve comparing model predictions with observed data or conducting sensitivity analysis 9. Visualize and report - plots, maps and tables can be effective ways to communicate findings. While you might find slight variations among collaborators, following this general workflow ensures that our scripts are structured in a way that facilitates easy sharing and reproducibility of results. 11.4.2 Generate functions Since we have imported the required libraries, let’s generate some functions. # # extractsitename will extract a site name from the filepaths we provide. You will likely need to change the indexing in this function based on the filepaths to your DEMs. # extractsitename &lt;- function(x) { # splitpath &lt;- strsplit(x,&#39;/&#39;) # basename &lt;- splitpath[[1]][3] # splitbase &lt;- strsplit(basename,&#39;_&#39;)[[1]][1] # return(splitbase) # } # # #One method to determine how to assign correct indexes is to run each of the lines in this function in your console and view the results. For example, let&#39;s first list all files in your current working directory: # # # List all files recursively in the current working directory # all_files &lt;- list.files(recursive = TRUE) Q1.(2 pnt) What does date: “16 April, 2024” in the header do? ANSWER: Q2.(2 pnt) What does ‘recursive = TRUE’ do? What would the ‘recursive = FALSE’ return? ANSWER: # # Now look at all_files printed in your console and select a filepath that represents a DEM. You can access that filepath alone by indexing all_files # # print(all_files[3]) # you will likely have to change &#39;3&#39; here to match your filepath of choice # # You can use this indexed file list to test each line of the extractsitename function to see what it does: # splitpath &lt;- strsplit(all_files[3], split = &#39;/&#39;) # again, change &#39;3&#39; to match your filepath # splitpath # # # Note that strsplit splits a character string by whatever symbol or character we provide in the &#39;split&#39; argument. Also note that splitpath is a list within a list, indicated by the double set of brackets around 1: [[1]]. We can &#39;unlist&#39; splitpath, or we can call the string we want by using both list indexes: # # basename &lt;- splitpath[[1]][2] # here, basename should return the .tif name. If it does not, change the indices (likely &#39;2&#39;) until your basename is the name of the tif. e.g., &#39;lexen_0.5m_DEM.tif&#39; # basename # # Now we can split the tif name to extract the site name: # splitbase &lt;- strsplit(basename,&#39;_&#39;)[[1]][1] # splitbase # Note that here we combined a couple of steps, splitting the file name and extracting the desired string in one line. # # Now let&#39;s save the function extractsitename again, but with the indices generate the sitenames using your current working directory. # # extractsitename &lt;- function(x) { # splitpath &lt;- strsplit(x,&#39;/&#39;) # basename &lt;- splitpath[[1]][2] # splitbase &lt;- strsplit(basename,&#39;_&#39;)[[1]][1] # return(splitbase) # } # You can test the function in your console with: extractsitename(all_files[2]). This should return a site name (in this exercise, fool, lexen or deadhorse). EXTRA (1pnt) : Rewrite this function to generate ‘splitbase’ with fewer lines. For example, what happens when you replace ‘basename’ in ‘splitbase &lt;- strsplit(basename,’_‘)[[1]][1]’ with ‘splitpath[[1]][2]’? # Here we will generate another function that we will use later in the script. # # # Create a function to resample and export a single raster # resample_and_export &lt;- function(file_path) { # sitename &lt;- extractsitename(file_path) # r &lt;- raster(file_path) # Read in the raster # r_resamp &lt;- aggregate(r, fact = 20) # Resample the raster # output_file_path &lt;- file.path(paste0(&#39;resampleddems/&#39;, sitename, &quot;_resampled.tif&quot;)) # Create the output file path # writeRaster(r_resamp, filename = output_file_path, format = &quot;GTiff&quot;, overwrite = TRUE) # Export the resampled raster # return(list(sitename = sitename, output_file_path = output_file_path)) # Return the sitename and output file path as a list # } Q3. (2pnt) What is the point of writing a function and why do you think it is advantageous to write functions early in your script? ANSWER: 11.4.3 Establish new directories(folders) in our working directory. We will use these to store the outputs of the Whitebox functions. # # Define the directory names # directory_names &lt;- c(&#39;breachsinglecellpits&#39;, &#39;breachdep&#39;, &#39;resampleddems&#39;, &#39;d8pointer&#39;, &#39;logd8flowaccum&#39;, &#39;d8flowaccum&#39;, &#39;streams_wws&#39;,&#39;streams&#39;) # # # Loop through each directory name # for (dir_name in directory_names) { # # Check if the directory already exists, and create it if not # if (!dir.exists(dir_name)) { # dir.create(dir_name) # } # } Check your working directory, you should see the new folders there. 11.4.4 Resample DEMs Here we will start with LiDAR data with 0.5m resolution. While this resolution has useful applications, a high resolution DEM can make hydrological models very computationally expensive with little or no improvement to the output. If you have the option of high resolution data in your work, you can test model outputs at different resolutions to determine what is the most efficient and effective resolution for your work. Here, we will resample our LiDAR data to a 10m resolution. # # Resample dem to 10m # # #List all of the dem files within the LiDARdem directory # files &lt;- list.files(path =&#39;LiDARdem&#39;, pattern=&quot;*.tif&quot;, full.names=TRUE, recursive=FALSE) # # # Use lapply to resample and export all resampled rasters # output_files &lt;- lapply(files, resample_and_export) Q4. (3pnts) Did we use the function extractsitenames in the above chunk? How? What did it do? ANSWER: Let’s quickly check our work by importing a resampled DEM and checking the resolution. # # Specify the path to the raster file # path &lt;- &#39;resampleddems/fool_resampled.tif&#39; # # # Import the raster # raster_obj &lt;- raster(path) # # # Get the resolution # resolution &lt;- res(raster_obj) # # # Print the resolution # print(resolution) Note: This can also be done without importing the raster to the workspace by installing the library gdalUtils. Q5.(2pnts) What is the resolution of the resampled DEM? Where and how could we change the resolution to 30m if desired? ANSWER: # #List all of the resampled dem files within the resampleddems directory # files &lt;- list.files(path = paste(&#39;resampleddems&#39;, sep = &quot;/&quot;), pattern=&quot;*.tif&quot;, full.names=TRUE, recursive=FALSE) 11.4.5 Filling and breaching When performing hydrological analysis on a DEM, the DEM usually needs to be pre-processed by ‘filling’ or ‘breaching’ any depressions or sinks to create a hydraulically connected and filled DEM. There are several depression or pit filling options available in whitebox. Breach depressions can be a better option that just pit filling according to whitebox documentation, however, some users argue that this can smooth too much, resulting in an altered watershed delineation. It is prudent to investigate different DEM pre-processing methods and their resulting DEM. You can fill depressions directly, breach depressions and then fill them, applying breach or fill single cell pit before breach/fill depressions, and use the one that generates the most reasonable watershed delineation results. Here we are going to make extra sure our flowpaths are uninhibited by first filling in single cell pits, and then breaching any larger depressions. # #Fill single cell pits for all rasters the folder &#39;resampleddems&#39; (for hydrologic correctness) # # lapply(files, function(x) { # sitename &lt;- extractsitename(x) # bscp &lt;- wbt_breach_single_cell_pits(x, output = paste0(getwd(), &#39;/breachsinglecellpits/&#39;, sitename, &quot;_breach1&quot;)) #first wb function input # }) # # #List all of the files from previous function # breach1 &lt;- list.files(path= paste(&#39;breachsinglecellpits&#39;, sep = &#39;/&#39;), pattern=&quot;*.tif&quot;, full.names=TRUE, recursive=FALSE) # # # apply breach depressions to every .tif in files list # lapply(breach1, function(x) { # sitename &lt;- extractsitename(x) # wbt_breach_depressions(x, output = paste0(getwd(),&#39;/breachdep/&#39;, sitename, &quot;_breachdep&quot;), flat_increment=.01) #first wb function input # }) 11.4.6 Flow direction and accumulation rasters # # List all of the filled files and use this list to make a flow direction grid and flow accumulation raster # breachdep &lt;- list.files(path = paste(&#39;breachdep&#39;, sep = &quot;/&quot;), pattern=&quot;*.tif&quot;, full.names=TRUE, recursive=FALSE) # # #Flow direction grid (d8_pntr) # lapply(breachdep, function(x) { # sitename &lt;- extractsitename(x) # wbt_d8_pointer(x, output = paste0(getwd(), &#39;/d8pointer/&#39;, sitename, &quot;_d8pointer&quot;)) # }) # # # D8 flow accumulation (raster cells fully drain in 1 of 8 directions) # # lapply(breachdep, function(x) { # sitename &lt;- extractsitename(x) # wbt_d8_flow_accumulation(x, output = paste0(getwd(), &#39;/logd8flowaccum/&#39;, sitename, &quot;_logd8flowaccum&quot;), # out_type=&#39;catchment area&#39;, # log=T) # }) # # # Not log-transformed so output is the upslope accumulated area to each pixel in m2 # lapply(breachdep, function(x) { # sitename &lt;- extractsitename(x) # wbt_d8_flow_accumulation(x, output = paste0(getwd(), &#39;/d8flowaccum/&#39;, sitename, &quot;_d8flowaccum&quot;), # out_type=&#39;catchment area&#39;, # log=F) # }) Q6 (2pnts) Check out the WhiteboxTools User Manual. What does a d8pointer raster tell us and what might we use it for? ANSWER: Let’s visualize some of our work so far: # #%%%%%%$$$$$$$$$ Visualize # path &lt;- paste0(getwd(), &#39;/d8pointer/fool_d8pointer.tif&#39;) # # checkwrk &lt;- raster(path) # mapview(checkwrk) # #%%%%%%$$$$$$$$$ Visualize # path &lt;- paste0(getwd(), &#39;/d8flowaccum/fool_d8flowaccum.tif&#39;) # # checkwrk &lt;- raster(path) # mapview(checkwrk) Q7. (2pnts) What are the units for the values that you see in each of these legends? It may be helpful to check out the Manual again. ANSWER: 11.4.7 Streams Define streams using the flow accumulation raster. Use of the wbt_extract_streams function will return a raster with stream cells indicated only. # #List all of the filled files # log_d8_flow_accum &lt;- list.files(path = paste(&#39;logd8flowaccum&#39;, sep = &quot;/&quot;), pattern=&quot;*.tif&quot;, full.names=TRUE, recursive=FALSE) # # lapply(log_d8_flow_accum, function(x) { # sitename &lt;- extractsitename(x) # stream_raster &lt;- wbt_extract_streams(x, output = paste0(getwd(), &#39;/streams/&#39;, sitename, &quot;_streams&quot;), threshold = 12) # }) # #%%%%%%$$$$$$$$$ Visualize # path &lt;- paste0(getwd(), &#39;/streams/fool_streams.tif&#39;) # # mapview(raster(path)) Sometimes we would prefer to see the stream within the watershed boundary. Here we are using the extent of the flow accumulation raster to generate a raster with ‘0’ to indicate watershed cells, along with stream cells indicated by the streams.tif. This is a demonstration for one watershed. # input_path &lt;- paste0(getwd(), &#39;/streams/fool_streams.tif&#39;) # # Read the output raster # stream_raster &lt;- raster(input_path) # # # Convert NA to 0 # stream_raster[is.na(stream_raster)] &lt;- 0 # # # Convert all non-zero values to 1 # stream_raster[stream_raster != 0] &lt;- 1 # # # Create a zero raster with the same extent and resolution as the dem # zero_raster &lt;- raster(log_d8_flow_accum[2]) # zero_raster[is.na(log_d8_flow_accum[2])] &lt;- NA # zero_raster[!is.na(zero_raster)] &lt;- 0 # # # Set the values in zero_raster to 1 where the stream_raster has 1 # zero_raster[stream_raster &gt; 0] &lt;- 1 # # # Write the final raster to file # output_file_path &lt;- file.path(getwd(), &quot;streams_wws&quot;, paste0(&quot;fool_streams_wws.tif&quot;)) # writeRaster(zero_raster, filename = output_file_path, format = &quot;GTiff&quot;, overwrite = TRUE) # #%%%%%%$$$$$$$$$ Visualize # path &lt;- paste0(getwd(), &#39;/streams_wws/fool_streams_wws.tif&#39;) # # mapview(raster(path)) 11.4.8 Final thoughts: There are many more hydrological preprocessing and analysis tools available through Whitebox for R. If you are interested in watershed delineation in R, there is a tutorial here that is fairly easy to follow. However, if you find that you use these tools frequently and do not use R much in other work, you may also consider these options for hydrological analysis: 1. SAGA SAGA tools offer a versatile suite of geospatial processing capabilities accessible through both QGIS and ArcGIS plugins as well their standalone GUI interface. Often I find the GUI easiest for preprocessing, then I will import SAGA’s output rasters to QGIS for formatting or map making, or into model scripts. SAGA has a robust online support community, so it can be a valuable resource for hydrological work. 2. Similarly, Whitebox GAT tools can be used as plugin to QGIS and ArcGIS, providing Whitebox functionality directly with in a GIS environment. When using these tools, the order of operations is similar to our work above: fill the DEM, generate a flow direction and flow accumulation raster, identify channels, delineate watersheds, then you can move forward according to the specificity of your project. Ultimately, the choice of workflow is yours, but I suggest documenting your process as you proceed, including links or file paths to projects and scripts within the written workflow. It’s also important to carefully consider the organization and storage of your projects and files. For instance, files generated by a GIS project should be readily accessible to any associated scripts. Returning to a preprocessing step can be challenging if there’s no clear way to trace back your workflow and regenerate a crucial layer. "],["evapotranspiration.html", "Chapter 12 Evapotranspiration 12.1 The repo can be found here 12.2 Background 12.3 Codework:", " Chapter 12 Evapotranspiration 12.1 The repo can be found here 15pnts 12.2 Background Evapotranspiration (ET) encompasses all processes through which water moves from the Earth’s surface to the atmosphere, comprising both evaporation and transpiration. This includes water vaporizing into the air from soil surfaces, the capillary fringe of groundwater, and water bodies on land. Much like snowmelt modeling, ET modeling and measurements are critical to many fields and could be a full course on its own. We will be focused on the basics of ET, modeling and data retrieval methods for water balance in hydrological modeling. Evapotranspiration is an important variable in hydrological models, as it accounts for much of the water loss in a system, outside of discharge. Transpiration, a significant component of ET, involves the movement of water from soil to atmosphere through plants. This occurs as plants absorb liquid water from the soil and release water vapor through their leaves. To gain a deeper understanding of ET, let’s review transpiration. 12.2.1 Transpiration Plant root systems to absorb water and nutrients from the soil, which they then distribute to their stems and leaves. As part of this process, plants regulate the loss of water vapor into the atmosphere through stomatal apertures, or transpiration. However, the volume of water transpired can vary widely due to factors like weather conditions and plant traits. Vegetation type: Plants transpire water at different rates. Some plants in arid regions have evolved mechanisms to conserve water by reducing transpiration. One mechanism involves regulating stomatal opening and closure. These plants can minimize water loss, especially during periods of high heat and low humidity. This closure of stomata can lead to diel and seasonal patterns in transpiration rates. Throughout the day, when environmental conditions are favorable for photosynthesis, stomata open to allow gas exchange, leading to increased transpiration. Conversely, during the night or under stressful conditions, stomata may close to conserve water, resulting in reduced transpiration rates. Humidity: As the relative humidity of the air surrounding the plant rises the transpiration rate falls. It is easier for water to evaporate into dryer air than into more saturated air. Soil type and saturation: Clay particles, being small, have a high capacity to retain water, while sand particles, being larger, readily release water. When there is a shortage of moisture, plants may enter a state of senescence and reduce their rate of transpiration. Precipitation: During dry periods, transpiration can contribute to the loss of moisture in the upper soil zone. Temperature: Transpiration rates go up as the temperature goes up, especially during the growing season, when the air is warmer due to stronger sunlight and warmer air masses. Higher temperatures cause the plant cells to open stomata, allowing for the exchange of CO2 and water with the atmosphere, whereas colder temperatures cause the openings to close. The availability and intensity of sunlight have a direct impact on transpiration rates. Likewise, the aspect of a location can influence transpiration since sunlight availability often depends on it. Wind &amp; air movement: Increased movement of the air around a plant will result in a higher transpiration rate. Wind will move the air around, with the result that the more saturated air close to the leaf is replaced by drier air. 12.2.2 Measurements In the realm of evapotranspiration (ET) modeling and data analysis, you’ll frequently encounter the terms potential ET and actual ET. These terms are important to consider when selecting data, as they offer very different insights into water loss processes from the land surface to the atmosphere. Potential Evapotranspiration (PET): Potential ET refers to the maximum possible rate at which water could evaporate and transpire under ideal conditions. These conditions typically assume an ample supply of water, unrestricted soil moisture availability, and sufficient energy to drive the evaporative processes. PET is often estimated based on meteorological variables such as temperature, humidity, wind speed, and solar radiation using empirical equations like the Penman-Monteith equation. Actual Evapotranspiration (AET): Actual ET, on the other hand, represents the observed or estimated rate at which water is actually evaporating and transpiring from the land surface under existing environmental conditions. Unlike PET, AET accounts for factors such as soil moisture availability, vegetation cover, stomatal conductance, and atmospheric demand. It reflects the true water loss from the ecosystem and is of greater interest in hydrological modeling, as it provides a more realistic depiction of water balance dynamics. The formula for converting PET to AET is: AET = PET * Kc Where: AET is the actual evapotranspiration, PET is the potential evapotranspiration, and Kc is the crop coefficient. The crop coefficient accounts for factors such as crop type, soil moisture levels, climate conditions, and management practices. It can vary throughout the growing season as well. 12.2.2.1 Direct measurements: There are several methods to measure ET directly like lysimeters and gravimetric analysis, but this data rarely available to the public. There has been a concerted effort to enhance the accessibility of Eddy Covariance data, so this dataset may expand in the years to come. knitr::include_url(&quot;https://www.youtube.com/embed/CR4Anc8Mkas&quot;) This video focuses on CO2 as an output of eddy covariance data, but among the ‘other gases’ mentioned, water vapor is included, offering measurements of actual ET. The video also provides a resource where you might find eddy covariance data for your region of interest. 12.2.2.2 Remote sensing: Remote sensing of evapotranspiration (ET) involves the use of satellite or airborne sensors to observe and quantify the exchange of water vapor between the Earth’s surface and the atmosphere over large spatial scales. This approach offers several advantages, including the ability to monitor ET across diverse landscapes, regardless of accessibility, and to capture variations in ET over time with high temporal resolution. Remote sensing data, coupled with energy balance models, can be used to estimate ET by quantifying the energy fluxes at the land surface. These models balance incoming solar radiation with outgoing energy fluxes, including sensible heat flux and latent heat flux (representing ET). Remote sensing-based ET estimates are often validated and calibrated using ground-based measurements, such as eddy covariance towers or lysimeters, to ensure accuracy and reliability. It can be helpful to validate these models yourself if you have a data source available in your ecoregion as a ‘sanity check’. Keep in mind that there are numerous models available, some of which may be tailored for specific ecoregions, resulting in significant variations in estimated evapotranspiration (ET) for your area among these models. If directly measured ET data is not available, you can check model output in a simple water balance. For example, inputs - outputs for your watershed (Ppt - Q - ET) should be approximately 0 (recall from our transfer function module that it is likely not exact). If the ET estimate matches precipitation, it’s likely that the selected model is overestimating ET for your region. Some resources for finding ET modeled from remote sensing data: ClimateEngine.org - This is a fun resource for all kinds of data. Actual evapotranspiration can be found in the TerraClimate dataset. OpenET - you need a Google account for this one. This site is great if you need timeseries data. You can select ‘gridded data’ and draw a polygon in your area of interest. You can select the year of interest at the top of the map, and once the timeseries generates, you can view and compare the output of seven different models. 12.2.2.3 Modeling: 12.3 Codework: For this assignment, we will work again in the Fraser Experimental Forest (same site as snowmelt module). Not only are there meteorological stations in our watershed of interest, but there is a Eddy Covariance tower nearby, in a forest with the same tree community as our watershed of interest. We can use this data to verify our model output for modeled data. 12.3.1 The Evapotranspiration package We will test a couple of simple methods that require few data inputs. However, it may be helpful to know that this package will allow calculations of PET, AET and Reference Crop Evapotranspiration from many different equations. Your selection of models may depend on the study region and the data available. 12.3.1.1 Import libraries 12.3.1.2 Import data 12.3.1.2.1 Meteorological data We’ll import three datasets in this workflow, one containing actual data from the eddy covariance tower for the years 2017-2018. Also imported is the discharge (Q) data from our watershed. Note that data is not collected during periods of deep snow. We will use this data to estimate the total discharge for 2022. Additionally, we’ll import the meteorological data from our watershed for the years of interest. We are going to estimate ET for our watershed during the 2022 water year, though met data is provided in calendar years, so we will need to extract the desired data from the met dataframe. # # We have two meteorological (met) files in the working directory, # # List all files in the directory # file_list &lt;- list.files(pattern = &quot;\\\\.csv$&quot;) # # # Filter filenames containing &#39;met&#39; # met_files &lt;- file_list[grep(&quot;met&quot;, file_list)] # # # Read in and concatenate the &#39;met&#39; files # data_list &lt;- lapply(met_files, read.csv) # usfs_met_full &lt;- do.call(rbind, data_list) # # # Read in daylight hours: # daylight&lt;- read.csv(&quot;daylight_hours.csv&quot;) 12.3.1.3 Data formatting We will model ET at daily timesteps. Note that all imported data is for different timesteps, units and measurements than what we need for analysis. It is common practice to manipulate the imported data and adjust it to align with our model functions. Ensuring the correct structure of the data can prevent potential issues later in the code # # Let&#39;s start with met data. We need RHmax and RHmin, so we will choose the max and min of each day from the 10 minute intervals. # # # Convert &#39;TIMESTAMP&#39; column to POSIXct type using lubridate # usfs_met_full$TIMESTAMP &lt;- mdy_hm(usfs_met_full$TIMESTAMP) # # # Filter data for the desired water year (Oct 1, 2021 - Sep 30, 2022) # water_year_start &lt;- ymd(&quot;2021-10-01&quot;) # water_year_end &lt;- ymd(&quot;2022-09-30&quot;) # # usfs_met &lt;- usfs_met_full %&gt;% # filter(TIMESTAMP &gt;= water_year_start &amp; TIMESTAMP &lt;= water_year_end) # # # Aggregate data to daily intervals # usfs_met_rh &lt;- usfs_met %&gt;% # group_by(Date = as.Date(TIMESTAMP)) %&gt;% # summarise(Tmean = mean(AirT..C.), # Tmax = max(AirT..C.), # Tmin = min(AirT..C.), # RHmean = mean(RH....), # RHmax = max(RH....), # RHmin = min(RH....)) %&gt;% # ungroup() # # Let&#39;s add daylight hours to the met dataframe # # daylight contains month and day, lets format month and day for usfs_met so we can merge the dataframes: # usfs_met_rh$month &lt;- format(usfs_met_rh$Date, &quot;%m&quot;) # usfs_met_rh$day &lt;- format(usfs_met_rh$Date, &quot;%d&quot;) # # Convert &quot;month&quot; and &quot;day&quot; columns in daylight to character # daylight$month &lt;- sprintf(&quot;%02d&quot;, daylight$month) # daylight$day &lt;- sprintf(&quot;%02d&quot;, daylight$day) # # # Now, merge the &quot;N&quot; column from daylight into usfs_met_rh based on the month and day columns # usfs_met_rh &lt;- merge(usfs_met_rh, daylight[, c(&quot;month&quot;, &quot;day&quot;, &quot;N&quot;)], by = c(&quot;month&quot;, &quot;day&quot;), all.x = TRUE) # usfs_met_rh$n &lt;- usfs_met_rh$N*12 # # # Reorder merged data based on the &quot;Date&quot; column # usfs_met_rh &lt;- usfs_met_rh %&gt;% arrange(Date) # # # Check the head of merged data to verify the reordering # head(usfs_met_rh) Now we have the RHmax and RHmin values that we will need for our ET models 12.3.1.3.1 Precipitation To add a layer of checks we will also check precipitation totals for the watershed from our met data. This dataframe also provides cumulative precipitation, though the accumulation begins on January 1. Therefore we will need to calculate daily precipitation for each day of the water year and generate a new cumulative value. # # Calculate daily precipitation by subtracting consecutive values # usfs_met &lt;- usfs_met %&gt;% # mutate(daily_ppt_mm = Cumulative_ppt..mm. - lag(Cumulative_ppt..mm., default = 0)) # # # Set the first row of daily_ppt_mm to 0 # usfs_met$daily_ppt_mm[1] &lt;- 0 # # # reset cumulative value calculated from calendar year on Jan 1 # usfs_met &lt;- usfs_met %&gt;% # mutate(daily_ppt_mm = if_else(TIMESTAMP == ymd_hms(&quot;2022-01-01 00:00:00&quot;), 0, daily_ppt_mm)) # # # Sum daily precipitation for the water year # cumulative_ppt_2022 &lt;- sum(usfs_met$daily_ppt_mm, na.rm = TRUE) This was a lot of work to save one value. 12.3.1.4 Pseudocode Often when writing code, it is necessary to first write ‘pseudocode’. This allows us to think about workflow before writing scripts involves planning and structuring the logic of a script. Psuedocode is usually mixture of natural language and code-like constructs (e.g., ‘I’ll use ’merge’ or ‘ggplot’ to…’) that serves as a blueprint for our script before we dive into specific programming language syntax. Then we can develop our code by filling in the programming language as needed. Q1. (2pnts) In you own words, what did we do at each step in the above chunk and what is cumulative_ppt_2022. Why do we want to know this? ANSWER: 12.3.1.4.1 Discharge data Our next step will be to import and format discharge data collected from the weir at Fool Creek. Again, data collection starts on April 20th this year. To estimate discharge between Sept 31 of the previous year and April 20th of 2022, we will assume daily discharge between these dates is the mean of the discharge values from each of these dates. Q2 (3pnts) Write pseudocode in a stepwise fashion to describe the workflow that will need to happen to estimate the total volume of water in mm/watershed area for 2022 lost to stream discharge. Consider that the units provided by USFS are in cubic feet/second (cfs). ANSWER: 1. Import dataframe 2. … # # Import weir data # lfc_weir&lt;- read.csv(&quot;LFC_weir_2022.csv&quot;) # # # Convert &#39;TIMESTAMP&#39; column to POSIXct type using lubridate # lfc_weir$TIMESTAMP &lt;- mdy_hm(lfc_weir$TIMESTAMP) # # # Conversion factor from cfs to cubic millimeters per second # cfs_to_mm3_per_sec &lt;- 28316846.6 # # # Convert cfs to cubic millimeters per day # lfc_weir &lt;- lfc_weir %&gt;% # group_by(Date = as.Date(TIMESTAMP)) %&gt;% # summarise( # Q_daily_cfs = mean(Q..cfs.), # Q_daily_mm3_per_day = mean(Q..cfs.) * cfs_to_mm3_per_sec * 86400 # 86400 seconds in a day # ) %&gt;% # ungroup() # # # Watershed area in square millimeters # watershed_area_mm2 &lt;- 2.69e6 * 1e6 # Convert km^2 to square millimeters # # # Convert flow rate from cubic millimeters per day to millimeters per day per unit area # lfc_weir &lt;- lfc_weir %&gt;% # mutate(Q_daily_mm_per_day_per_area = Q_daily_mm3_per_day / watershed_area_mm2) # # meanwinterQ &lt;- sum(lfc_weir$Q_daily_mm_per_day_per_area[lfc_weir$Date == as.Date(&quot;2022-09-30&quot;)],lfc_weir$Q_daily_mm_per_day_per_area[1])/2 # # # Calculate the difference in days from the end of the wateryear to the resumption of data collection # start_date &lt;- as.Date(&quot;2021-09-30&quot;) # end_date &lt;- as.Date(&quot;2022-04-20&quot;) # # days_difference &lt;- difftime(end_date, start_date, units = &quot;days&quot;) # # # Convert the result to numeric # days_difference &lt;- as.numeric(days_difference) # # Qsum2022 &lt;-sum(meanwinterQ *days_difference, sum(lfc_weir$Q_daily_mm_per_day_per_area)) 12.3.1.4.2 Evapotranspiration data Import and transform ET data. Note that ET data is in mmol/m2/s. We want to convert this to mm/day. Eddy covariance data collected from towers represents the exchange of gases, including water vapor, between the atmosphere and the land surface within a certain area known as the “footprint.” This footprint area is not fixed; it changes in size and shape depending on factors like wind direction, thermal stability, and measurement height. Typically, it has a gradual border, meaning that the influence of the tower measurements extends beyond its immediate vicinity. For our specific tower data, we’ve estimated the mean flux area, or footprint area, to be approximately 0.4 square kilometers. However, when estimating the total evapotranspiration (ET) for our entire watershed, we need to extrapolate the ET measured by the tower to cover the entire watershed area. This extrapolation involves scaling up the measurements from the tower footprint to the larger area of the watershed to get a more comprehensive understanding of water vapor exchange over the entire region. # # Import ET data # usfs_et &lt;- read.csv(&quot;US_FEF_ET.csv&quot;) # # # The ET data contains negative values. ET is not negative, we are going to convert these to NA. This may not always be an ideal solution. It is a good idea to talk to the tower maintainer or consult metadata to check the source of negative values: # # # Replace negative values in &#39;ET&#39; column with 0 (this is the recommended course of action by our USFS maintainer) # usfs_et$ET_mmol.m2.s[usfs_et$ET_mmol.m2.s &lt; 0] &lt;- 0.0 # # # Define function to convert mmol/m2/s to mm/30 min interval # mmol_to_mm &lt;- function(x) { # return((x * 1.5552) / 48) # } # # # Apply function to &#39;ET_mmol.m2.s&#39; column and create new column &#39;ET_mm&#39; # usfs_et$ET_mm &lt;- sapply(usfs_et$ET_mmol.m2.s, mmol_to_mm) # # # Now let&#39;s aggregate the 30min timesteps to daily means: # # # Convert &#39;datetime&#39; column to POSIXct type # usfs_et$datetime &lt;- mdy_hm(usfs_et$datetime) # # # Extract date from datetime # usfs_et$date &lt;- as.Date(usfs_et$datetime) # # # Group by date and calculate sum of &#39;ET_mm&#39; # usfs_et_daily &lt;- usfs_et %&gt;% # group_by(date) %&gt;% # summarise(ET_mm = sum(ET_mm)) # # # Account for covariance footprint and watershed area: # usfs_et_daily$ws_et &lt;- usfs_et_daily$ET_mm * 0.35 * 2.69 # # usfs_et_daily$date &lt;- as.Date(usfs_et_daily$date) # # subset_data &lt;- subset(usfs_et_daily, date &gt;= as.Date(&quot;2021-10-01&quot;) &amp; date &lt;= as.Date(&quot;2022-09-30&quot;)) # # # Finally, calculate the sum of &#39;ws_et&#39; for the subsetted data # ws_et_2022 &lt;- sum(subset_data$ws_et) # Hbalance &lt;- cumulative_ppt_2022 - ws_et_2022 - Qsum2022 # print(Hbalance) This appears to be a fairly well balanced water budget, especially consdering that we have made some estimates along the way. Let’s see how this ET data compares to modeled data. 12.3.2 The Priestly Taylor method (actual evapotranspiration) The Priestley-Taylor method is a semi-empirical model that estimates potential evapotranspiration as a function of net radiation. This method requires a list which contains: (climate variables) required by Priestley-Taylor formulation: Tmax, Tmin (degree Celcius), RHmax, RHmin (percent), Rs (Megajoules per sqm) or n (hour). We have measurements of relative humidity (RH) data from within our watershed meteorological station. However, RH data can be found through many sources providing local weather data. n refers to the number of sunshine hours. # # Transform data to a list for the function # data_list &lt;- list( # Date.daily = usfs_met_rh$Date, # Assuming this is your &quot;Date&quot; column # Date.monthly = as.yearmon(unique(usfs_met_rh$Date)), # J = zoo::zoo(c(272:365,1:271), usfs_met_rh$Date), # Tmax = zoo::zoo(usfs_met_rh$Tmax, usfs_met_rh$Date), # Tmin = zoo::zoo(usfs_met_rh$Tmin, usfs_met_rh$Date), # RHmax = zoo::zoo(usfs_met_rh$RHmax, usfs_met_rh$Date), # RHmin = zoo::zoo(usfs_met_rh$RHmin, usfs_met_rh$Date), # n = zoo::zoo(usfs_met_rh$n, usfs_met_rh$Date) # ) Evapotranspiration comes with a list physical ‘constants’, but we want to replace important constants with data specific to our cite. # data(&quot;constants&quot;) # constants$Elev &lt;- 2900 # constants$lat &lt;- 39.89 # constants$lat_rad &lt;- 0.6962 # # Lets plot the modeled output: # plot(outputPT$ET.Daily, main = &quot;ET Daily Data&quot;, xlab = &quot;Date&quot;, ylab = &quot;ET&quot;) # # Lets plot the modeled output: # # Plot the first zoo object # plot(outputH$ET.Daily, main = &quot;ET Daily Data&quot;, xlab = &quot;Date&quot;, ylab = &quot;ET&quot;, col = &quot;blue&quot;) # # # Add the second zoo object to the same plot # lines(outputPT$ET.Daily, col = &quot;red&quot;) # # # Add a legend # legend(&quot;topright&quot;, legend = c(&quot;H Method&quot;, &quot;PT Method&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lty = 1) Recall that each of these are estimates of potential ET. Crop coefficients (Kc values) for specific tree species like those found in the Fraser Experimental Forest (lodgepole pine or Englemann spruce) may not be as readily available as they are for agricultural crops. However, you may be able to find some guidance in scientific literature or forestry publications. From what we have found, Kc estimates for lodgepole pine forests can be between 0.4 and 0.8. These values may vary depending on factors such as climate, soil type, elevation, and other site-specific factors. Our own estimates using water balance data from dates that correspond with the eddy flux tower data suggest seasonal fluctuations, with a mean of 0.55. AET = PET * Kc # kc = 0.55 # # # Convert zoo series to dataframe # outputPT_df &lt;- broom::tidy(outputPT$ET.Daily) # outputH_df &lt;- broom::tidy(outputH$ET.Daily) # # outputPT_df$aet_pt &lt;- outputPT_df$value * kc # outputH_df$aet_pt &lt;- outputH_df$value * kc Let’s plot the two modeled ET timeseries with the eddy covaraince tower data. # # Plot # ggplot() + # geom_line(data = subset_data, aes(x = date, y = ET_mm, color = &quot;ET_mm&quot;)) + # geom_line(data = outputH_df, aes(x = index, y = aet_pt, color = &quot;H Method&quot;)) + # geom_line(data = outputPT_df, aes(x = index, y = aet_pt, color = &quot;PT Method&quot;)) + # # labs(title = &quot;Comparison of AET_PT from Different Methods and ET_mm&quot;, # x = &quot;Date&quot;, # y = &quot;AET_PT / ET_mm&quot;, # color = &quot;Method&quot;) + # scale_color_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)) + # theme_minimal() Q3. (4pnts) Consider the two evapotranspiration (ET) models we have generated, one consistently underestimates ET, while the other consistently overestimates summer values and underestimates winter values. Consider cosystem specificity in modeling. Why do you think these two methods generate such different estimates? What ecosystem-specific factors might contribute to the discrepancies between models? ANSWER: Q4 (3pnts) Let’s assume we do not have eddy covariance tower data for this time period, but you have the provided discharge and precipitation measurements. Which model would you choose to estimate an annual ET sum and why? ANSWER: Q5 (3pnts) In our modeling script, while our main objective was to estimate evapotranspiration (ET), which specific aspect required the most labor and attention to detail? Reflect on the tasks involved in data preparation, cleaning, and formatting. How did these preliminary steps impact the overall modeling process? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
